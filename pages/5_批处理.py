"""
åˆ†å­åº“ä»£è¡¨æ€§å­é›†é€‰æ‹©ç³»ç»Ÿ - æ‰¹å¤„ç†é¡µé¢
"""
import os
import sys
import time
import pandas as pd
import numpy as np
import streamlit as st
from rdkit import Chem
import concurrent.futures
import io
import json
import tempfile

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼Œç¡®ä¿èƒ½å¯¼å…¥utilsæ¨¡å—
script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(script_dir)

# å¯¼å…¥å·¥å…·æ¨¡å—
from utils.molecular_utils import MoleculeProcessor
from utils.state_utils import initialize_session_state, display_state_sidebar
from utils.config_utils import create_config_from_parameters
from utils.clustering_utils import (
    butina_clustering, kmeans_clustering, maxmin_selection, 
    select_cluster_representatives, select_representatives_from_kmeans
)

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œé…ç½®
st.set_page_config(
    page_title="åˆ†å­åº“ä»£è¡¨æ€§å­é›†é€‰æ‹©ç³»ç»Ÿ - æ‰¹å¤„ç†",
    page_icon="ğŸ§ª",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
initialize_session_state()

# åœ¨ä¾§è¾¹æ æ˜¾ç¤ºå½“å‰çŠ¶æ€
display_state_sidebar()

# é¡µé¢æ ‡é¢˜
st.title("æ‰¹å¤„ç†")
st.markdown("æœ¬é¡µé¢æä¾›æ‰¹é‡å¤„ç†å¤šä¸ªæ•°æ®é›†çš„åŠŸèƒ½ï¼Œå¯ä»¥ä¸€æ¬¡æ€§å¤„ç†å¤šä¸ªæ–‡ä»¶å¹¶å¯¼å‡ºç»“æœã€‚")

# é…ç½®åŒºåŸŸ
st.subheader("æ‰¹å¤„ç†é…ç½®")

col1, col2 = st.columns(2)
with col1:
    # ä¸Šä¼ å¤šä¸ªæ–‡ä»¶
    uploaded_files = st.file_uploader("ä¸Šä¼ å¤šä¸ªSMILESæ–‡ä»¶", type=["csv"], accept_multiple_files=True)
    
    # SMILESåˆ—å
    smiles_col = st.text_input("SMILESåˆ—å", "SMILES")
    
    # å­é›†æ¯”ä¾‹
    subset_ratio = st.slider("å­é›†æ¯”ä¾‹ (%)", 0.1, 10.0, 1.0, 0.1)

with col2:
    # èšç±»æ–¹æ³•
    clustering_method = st.selectbox(
        "èšç±»/é€‰æ‹©ç®—æ³•",
        ["butina", "kmeans", "maxmin"]
    )
    
    # æ ¹æ®èšç±»æ–¹æ³•æ˜¾ç¤ºä¸åŒå‚æ•°
    if clustering_method == "butina":
        cutoff = st.slider("ç›¸ä¼¼åº¦é˜ˆå€¼", 0.0, 1.0, 0.6, 0.05)
        clustering_params = {"cutoff": cutoff}
    elif clustering_method == "kmeans":
        use_fixed_clusters = st.checkbox("ä½¿ç”¨å›ºå®šç°‡æ•°é‡", value=False)
        if use_fixed_clusters:
            n_clusters = st.number_input("ç°‡æ•°é‡", 10, 10000, 100)
            clustering_params = {"n_clusters": n_clusters, "use_fixed_clusters": True}
        else:
            st.info(f"å°†ä½¿ç”¨å­é›†æ¯”ä¾‹ ({subset_ratio}%) è®¡ç®—ç°‡æ•°é‡")
            clustering_params = {"use_fixed_clusters": False}
        
        kmeans_iterations = st.slider("æœ€å¤§è¿­ä»£æ¬¡æ•°", 10, 1000, 100)
        clustering_params["kmeans_iterations"] = kmeans_iterations
    elif clustering_method == "maxmin":
        init_method = st.selectbox("åˆå§‹ç‚¹é€‰æ‹©", ["random", "first"])
        clustering_params = {"init_method": init_method}

# æŒ‡çº¹é…ç½®
st.subheader("æŒ‡çº¹ä¸ç‰¹å¾é…ç½®")
fps_type = st.selectbox("æŒ‡çº¹ç±»å‹", ["morgan", "rdkit", "maccs"])
if fps_type == "morgan":
    morgan_radius = st.slider("MorganåŠå¾„", 1, 4, 2)
    morgan_bits = st.selectbox("Morganä½æ•°", [512, 1024, 2048])
    fps_params = {"morgan_radius": morgan_radius, "morgan_bits": morgan_bits}
else:
    fps_params = {}

include_properties = st.checkbox("åŒ…å«ç†åŒ–æ€§è´¨", value=True)
validation_stats = st.checkbox("è®¡ç®—éªŒè¯ç»Ÿè®¡", value=True)

# å¹¶è¡Œé…ç½®
n_jobs = st.slider("å¹¶è¡Œå¤„ç†ä»»åŠ¡æ•°", 1, 8, 4)

# ç”Ÿæˆé…ç½®
def create_batch_config():
    config = {
        'smiles_col': smiles_col,
        'subset_ratio': subset_ratio,
        'clustering_method': clustering_method,
        'fps_type': fps_type,
        'include_3d': False,
        'include_charges': False,
        'batch_size': 1000,
        'n_jobs': n_jobs,
        **clustering_params,
        **fps_params
    }
    return config

# æ‰¹å¤„ç†å‡½æ•°
def process_file(file, config):
    """å¤„ç†å•ä¸ªæ–‡ä»¶
    
    å‚æ•°:
        file: ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
        config: é…ç½®å­—å…¸
        
    è¿”å›:
        å¤„ç†ç»“æœå­—å…¸
    """
    start_time = time.time()
    
    # è¯»å–æ–‡ä»¶
    try:
        df = pd.read_csv(file)
    except Exception as e:
        return {
            'file_name': file.name,
            'status': 'error',
            'message': f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}",
            'duration': 0,
            'subset_size': 0,
            'total_size': 0,
            'subset_data': None
        }
    
    # æ£€æŸ¥SMILESåˆ—
    smiles_col = config.get('smiles_col', 'SMILES')
    if smiles_col not in df.columns:
        return {
            'file_name': file.name,
            'status': 'error',
            'message': f"æ‰¾ä¸åˆ°SMILESåˆ—: {smiles_col}",
            'duration': 0,
            'subset_size': 0,
            'total_size': 0,
            'subset_data': None
        }
    
    # åˆ›å»ºå®Œæ•´é…ç½®
    full_config = create_config_from_parameters(**config)
    
    # åˆå§‹åŒ–åˆ†å­å¤„ç†å™¨
    processor = MoleculeProcessor(full_config)
    
    # å¤„ç†åˆ†å­
    results = {
        'mols': [],
        'fps': [],
        'fps_binary': [],
        'basic_desc': [],
        'shape_desc': [],
        'charges': []
    }
    
    # ç®€åŒ–ç‰ˆæœ¬çš„å¤„ç†é€»è¾‘
    smiles_list = df[smiles_col].tolist()
    
    # ç®€åŒ–æ‰¹å¤„ç†
    for smiles in smiles_list:
        mol = processor.prepare_molecule(smiles)
        
        if mol is None:
            results['mols'].append(None)
            results['fps'].append(None)
            results['fps_binary'].append(None)
            results['basic_desc'].append({})
            results['shape_desc'].append({})
            results['charges'].append(None)
            continue
        
        # è®¡ç®—æŒ‡çº¹
        fp_config = full_config.get('features', {}).get('fingerprints', {})
        fp_type = fp_config.get('types', ['morgan'])[0]
        radius = fp_config.get('morgan_radius', 2)
        n_bits = fp_config.get('morgan_bits', 1024)
        
        fp = processor.compute_fingerprint(mol, radius=radius, nBits=n_bits, fp_type=fp_type)
        fp_array = processor.fp_to_numpy(fp)
        
        # è®¡ç®—åŸºæœ¬æè¿°ç¬¦
        basic_desc = processor.compute_basic_descriptors(mol) if include_properties else {}
        
        # æ·»åŠ ç»“æœ
        results['mols'].append(mol)
        results['fps'].append(fp_array)
        results['fps_binary'].append(fp)
        results['basic_desc'].append(basic_desc)
        results['shape_desc'].append({})
        results['charges'].append(None)
    
    # èšç±»å’Œé€‰æ‹©
    method = full_config.get('clustering', {}).get('method', 'butina')
    subset_ratio = full_config.get('subset_ratio', 1.0)
    
    # è¿‡æ»¤æœ‰æ•ˆåˆ†å­
    valid_idx = [i for i, m in enumerate(results['mols']) if m is not None]
    
    if not valid_idx:
        return {
            'file_name': file.name,
            'status': 'error',
            'message': "æ²¡æœ‰æœ‰æ•ˆåˆ†å­å¯ä¾›èšç±»",
            'duration': 0,
            'subset_size': 0,
            'total_size': len(df),
            'subset_data': None
        }
    
    # è®¡ç®—ç›®æ ‡é€‰æ‹©æ•°é‡
    total_valid = len(valid_idx)
    target_count = max(1, int(total_valid * subset_ratio / 100.0))
    
    # æ‰§è¡Œèšç±»å’Œé€‰æ‹©
    representatives = []
    
    try:
        if method == 'butina':
            # Butinaèšç±»
            valid_fps = [results['fps_binary'][i] for i in valid_idx]
            cutoff = full_config.get('clustering', {}).get('butina', {}).get('cutoff', 0.4)
            
            clusters = butina_clustering(valid_fps, cutoff)
            
            # é€‰æ‹©ä»£è¡¨åˆ†å­
            local_indices = []
            for cluster in clusters:
                if cluster:  # ç¡®ä¿ç°‡éç©º
                    local_indices.append(cluster[0])
                    
            representatives = [valid_idx[i] for i in local_indices]
            
        elif method == 'kmeans':
            # K-meansèšç±»
            valid_features = [results['fps'][i] for i in valid_idx]
            valid_features = np.array([f for f in valid_features if f is not None])
            
            use_ratio = full_config.get('clustering', {}).get('kmeans', {}).get('use_ratio', True)
            if use_ratio:
                n_clusters = target_count
            else:
                n_clusters = full_config.get('clustering', {}).get('kmeans', {}).get('n_clusters', target_count)
                
            max_iter = full_config.get('clustering', {}).get('kmeans', {}).get('max_iter', 100)
            
            # ä½¿ç”¨scikit-learn
            from sklearn.cluster import KMeans
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, max_iter=max_iter)
            labels = kmeans.fit_predict(valid_features)
            centers = kmeans.cluster_centers_
            
            # é€‰æ‹©æœ€æ¥è¿‘ç°‡ä¸­å¿ƒçš„ç‚¹
            for i in range(n_clusters):
                # æ‰¾å‡ºå±äºå½“å‰ç°‡çš„æ‰€æœ‰ç‚¹
                cluster_indices = np.where(labels == i)[0]
                
                if len(cluster_indices) == 0:
                    continue  # è·³è¿‡ç©ºç°‡
                
                # è®¡ç®—åˆ°ä¸­å¿ƒçš„è·ç¦»
                cluster_points = valid_features[cluster_indices]
                distances = np.linalg.norm(cluster_points - centers[i], axis=1)
                
                # é€‰æ‹©æœ€è¿‘çš„ç‚¹
                closest_point_idx = cluster_indices[np.argmin(distances)]
                
                # æ˜ å°„å›åŸå§‹ç´¢å¼•
                representatives.append(valid_idx[closest_point_idx])
                
        elif method == 'maxmin':
            # MaxMiné€‰æ‹©
            valid_features = [results['fps'][i] for i in valid_idx]
            valid_features = np.array([f for f in valid_features if f is not None])
            
            num_to_select = min(target_count, len(valid_features))
            
            # åŸºæœ¬å®ç°
            from rdkit.SimDivFilters.rdSimDivPickers import MaxMinPicker
            
            # å®šä¹‰è·ç¦»å‡½æ•°
            def distance_fn(i, j):
                return np.linalg.norm(valid_features[i] - valid_features[j])
            
            # åˆå§‹ç§å­ç‚¹
            init_method = full_config.get('clustering', {}).get('maxmin', {}).get('init_method', 'random')
            if init_method == 'random':
                seed_idx = np.random.randint(0, len(valid_features))
            else:
                seed_idx = 0
                
            # MaxMiné€‰æ‹©
            picker = MaxMinPicker()
            indices = list(picker.LazyBitVectorPick(
                distance_fn, len(valid_features), num_to_select, seed_idx
            ))
            
            # æ˜ å°„å›åŸå§‹ç´¢å¼•
            representatives = [valid_idx[i] for i in indices]
            
    except Exception as e:
        return {
            'file_name': file.name,
            'status': 'error',
            'message': f"èšç±»å¤±è´¥: {str(e)}",
            'duration': time.time() - start_time,
            'subset_size': 0,
            'total_size': len(df),
            'subset_data': None
        }
    
    # æå–å­é›†æ•°æ®
    subset_df = df.iloc[representatives].copy().reset_index(drop=True)
    
    # æ·»åŠ ç†åŒ–æ€§è´¨
    if include_properties:
        for prop_name in ['mw', 'logp', 'tpsa', 'hba', 'hbd', 'rotatable_bonds']:
            subset_df[prop_name] = [
                results['basic_desc'][i].get(prop_name, 0) 
                for i in representatives
            ]
    
    # è®¡ç®—éªŒè¯ç»Ÿè®¡
    validation_data = None
    if validation_stats:
        # åŸºæœ¬éªŒè¯ç»Ÿè®¡
        valid_fps = [results['fps_binary'][i] for i in valid_idx]
        subset_fps = [results['fps_binary'][i] for i in representatives]
        
        from utils.validation_utils import calculate_coverage_metrics, calculate_nearest_neighbor_distance
        
        coverage_metrics = calculate_coverage_metrics(valid_fps, subset_fps)
        nn_distances, mean_dist, max_dist, median_dist = calculate_nearest_neighbor_distance(
            valid_fps, subset_fps
        )
        
        validation_data = {
            'coverage_metrics': coverage_metrics,
            'nn_stats': {
                'mean_dist': mean_dist,
                'max_dist': max_dist,
                'median_dist': median_dist
            }
        }
    
    # å‡†å¤‡è¿”å›ç»“æœ
    duration = time.time() - start_time
    return {
        'file_name': file.name,
        'status': 'success',
        'message': "å¤„ç†æˆåŠŸ",
        'duration': duration,
        'subset_size': len(representatives),
        'total_size': len(df),
        'subset_data': subset_df,
        'validation_data': validation_data
    }

# å¯åŠ¨æ‰¹å¤„ç†
if uploaded_files:
    if st.button("å¼€å§‹æ‰¹å¤„ç†"):
        # ç”Ÿæˆé…ç½®
        config = create_batch_config()
        
        # æ˜¾ç¤ºæ‰¹å¤„ç†è¿›åº¦
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # å¤„ç†ç»“æœå®¹å™¨
        all_results = []
        
        with st.spinner("å¤„ç†æ–‡ä»¶ä¸­..."):
            total_files = len(uploaded_files)
            
            # å¹¶è¡Œå¤„ç†æ–‡ä»¶
            if n_jobs > 1 and total_files > 1:
                with concurrent.futures.ProcessPoolExecutor(max_workers=min(n_jobs, total_files)) as executor:
                    futures = [
                        executor.submit(process_file, file, config)
                        for file in uploaded_files
                    ]
                    
                    for i, future in enumerate(concurrent.futures.as_completed(futures)):
                        try:
                            result = future.result()
                            all_results.append(result)
                        except Exception as e:
                            all_results.append({
                                'file_name': uploaded_files[i].name,
                                'status': 'error',
                                'message': f"å¤„ç†å¤±è´¥: {str(e)}",
                                'duration': 0,
                                'subset_size': 0,
                                'total_size': 0,
                                'subset_data': None
                            })
                        
                        # æ›´æ–°è¿›åº¦
                        progress = (i + 1) / total_files
                        progress_bar.progress(progress)
                        status_text.text(f"å·²å¤„ç† {i+1}/{total_files} ä¸ªæ–‡ä»¶")
            else:
                # ä¸²è¡Œå¤„ç†
                for i, file in enumerate(uploaded_files):
                    try:
                        result = process_file(file, config)
                        all_results.append(result)
                    except Exception as e:
                        all_results.append({
                            'file_name': file.name,
                            'status': 'error',
                            'message': f"å¤„ç†å¤±è´¥: {str(e)}",
                            'duration': 0,
                            'subset_size': 0,
                            'total_size': 0,
                            'subset_data': None
                        })
                    
                    # æ›´æ–°è¿›åº¦
                    progress = (i + 1) / total_files
                    progress_bar.progress(progress)
                    status_text.text(f"å·²å¤„ç† {i+1}/{total_files} ä¸ªæ–‡ä»¶")
        
        # æ˜¾ç¤ºç»“æœ
        st.success("æ‰¹å¤„ç†å®Œæˆ!")
        
        # ç»“æœæ‘˜è¦è¡¨æ ¼
        summary_data = []
        for result in all_results:
            summary_data.append({
                "æ–‡ä»¶å": result['file_name'],
                "çŠ¶æ€": result['status'],
                "æ¶ˆæ¯": result['message'],
                "æ€»åˆ†å­æ•°": result['total_size'],
                "å­é›†å¤§å°": result['subset_size'],
                "å­é›†æ¯”ä¾‹": f"{result['subset_size']/result['total_size']:.2%}" if result['total_size'] > 0 else "N/A",
                "å¤„ç†æ—¶é—´": f"{result['duration']:.2f}ç§’"
            })
        
        st.subheader("å¤„ç†ç»“æœæ‘˜è¦")
        summary_df = pd.DataFrame(summary_data)
        st.dataframe(summary_df)
        
        # ä¸‹è½½é€‰é¡¹
        st.subheader("ä¸‹è½½ç»“æœ")
        
        # åˆå¹¶æ‰€æœ‰æˆåŠŸçš„å­é›†ä¸ºä¸€ä¸ªZIPæ–‡ä»¶
        if any(r['status'] == 'success' for r in all_results):
            # å‡†å¤‡ZIPæ–‡ä»¶
            import zipfile
            
            with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as tmp:
                with zipfile.ZipFile(tmp.name, 'w') as zipf:
                    # æ·»åŠ CSVæ–‡ä»¶
                    for result in all_results:
                        if result['status'] == 'success' and result['subset_data'] is not None:
                            # åˆ›å»ºCSV
                            csv_buffer = io.StringIO()
                            result['subset_data'].to_csv(csv_buffer, index=False)
                            
                            # ç”Ÿæˆæ–‡ä»¶å
                            file_base = os.path.splitext(result['file_name'])[0]
                            csv_name = f"{file_base}_subset.csv"
                            
                            # æ·»åŠ åˆ°ZIP
                            zipf.writestr(csv_name, csv_buffer.getvalue())
                            
                            # å¦‚æœæœ‰éªŒè¯ç»Ÿè®¡ï¼Œæ·»åŠ JSON
                            if result['validation_data'] is not None:
                                json_name = f"{file_base}_validation.json"
                                zipf.writestr(json_name, json.dumps(result['validation_data']))
                    
                    # æ·»åŠ æ‰¹å¤„ç†é…ç½®
                    zipf.writestr('batch_config.json', json.dumps(config))
                
                # è¯»å–ZIPæ–‡ä»¶
                with open(tmp.name, 'rb') as f:
                    zip_data = f.read()
                
                # æä¾›ä¸‹è½½
                st.download_button(
                    label="ä¸‹è½½æ‰€æœ‰å­é›†å’ŒéªŒè¯æ•°æ® (ZIP)",
                    data=zip_data,
                    file_name="molecular_subsets.zip",
                    mime="application/zip"
                )
                
                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                os.unlink(tmp.name)
        
        # æä¾›å•ä¸ªå­é›†çš„ä¸‹è½½
        for i, result in enumerate(all_results):
            if result['status'] == 'success' and result['subset_data'] is not None:
                with st.expander(f"ä¸‹è½½ {result['file_name']} çš„å­é›†"):
                    # CSVä¸‹è½½
                    csv_buffer = io.StringIO()
                    result['subset_data'].to_csv(csv_buffer, index=False)
                    
                    st.download_button(
                        label=f"ä¸‹è½½CSVæ ¼å¼å­é›†",
                        data=csv_buffer.getvalue(),
                        file_name=f"{os.path.splitext(result['file_name'])[0]}_subset.csv",
                        mime="text/csv",
                        key=f"csv_download_{i}"
                    )
                    
                    # æ˜¾ç¤ºéªŒè¯ç»Ÿè®¡
                    if result['validation_data'] is not None:
                        metrics = result['validation_data']['coverage_metrics']
                        nn_stats = result['validation_data']['nn_stats']
                        
                        st.write("éªŒè¯ç»Ÿè®¡:")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("è¦†ç›–ç‡", f"{metrics['coverage_ratio']:.2%}")
                        with col2:
                            st.metric("å¹³å‡è·ç¦»", f"{nn_stats['mean_dist']:.3f}")
                        with col3:
                            st.metric("ä¸­ä½æ•°è·ç¦»", f"{nn_stats['median_dist']:.3f}")
else:
    st.info("è¯·ä¸Šä¼ åŒ…å«SMILESæ•°æ®çš„CSVæ–‡ä»¶ä»¥å¼€å§‹æ‰¹å¤„ç†")

# é¡µé¢åº•éƒ¨ä¿¡æ¯
st.markdown("---")
st.info("""
**æ‰¹å¤„ç†åŠŸèƒ½è¯´æ˜**

1. å¯ä»¥åŒæ—¶ä¸Šä¼ å¤šä¸ªCSVæ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶åŒ…å«SMILESåˆ—
2. æ‰€æœ‰æ–‡ä»¶å°†ä½¿ç”¨ç›¸åŒçš„å‚æ•°é…ç½®è¿›è¡Œå¤„ç†
3. ç»“æœå°†åŒ…å«æ‰€æœ‰å­é›†å’ŒéªŒè¯ç»Ÿè®¡
4. æ”¯æŒå¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡ä»¶ï¼Œæé«˜æ•ˆç‡
""") 