"""
åˆ†å­åº“ä»£è¡¨æ€§å­é›†é€‰æ‹©ç³»ç»Ÿ - 3Då½¢çŠ¶å¯¹æ¯”é¡µé¢
"""
import os
import concurrent.futures # æ·»åŠ å¯¼å…¥
import multiprocessing # æ·»åŠ å¯¼å…¥

# æŠ‘åˆ¶ TensorFlow å’Œ CUDA è­¦å‘Š
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 3 = ERRORï¼Œä»…æ˜¾ç¤ºé”™è¯¯
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
# æŠ‘åˆ¶CUDAç›¸å…³é‡å¤æ³¨å†Œè­¦å‘Š
os.environ['TF_CUDNN_DETERMINISTIC'] = '1'
# è®¾ç½®TFæ—¥å¿—çº§åˆ«
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
# ç¦ç”¨è­¦å‘Š
os.environ['TF_ENABLE_DEPRECATION_WARNINGS'] = 'false'

# æŠ‘åˆ¶ PyTorch è­¦å‘Š (å¯é€‰)
import warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

import sys
import random
import numpy as np
import pandas as pd
import streamlit as st
from rdkit import Chem
from rdkit.Chem import AllChem, rdMolDescriptors
import matplotlib.pyplot as plt
import torch
import seaborn as sns
from sklearn.manifold import TSNE
import umap
import time
from scipy.stats import wasserstein_distance
from sklearn.neighbors import KernelDensity

# å°è¯•å¯¼å…¥GPUåŠ é€Ÿç›¸å…³çš„åº“
try:
    import cupy as cp
    from cuml.manifold import TSNE as cuTSNE
    HAS_CUML = True
except ImportError:
    HAS_CUML = False
    warnings.warn("cuMLæœªå®‰è£…ï¼Œå°†ä½¿ç”¨CPUç‰ˆæœ¬çš„t-SNE")

try:
    import torchani
    HAS_TORCHANI = True
    
    # å°è¯•æ£€æµ‹ CUDA æ”¯æŒï¼Œä½¿ç”¨æ›´å®‰å…¨çš„æ–¹æ³•
    try:
        # ä»…æ£€æŸ¥æ˜¯å¦èƒ½åˆ›å»ºæ¨¡å‹å¹¶ç§»è‡³ GPU
        if torch.cuda.is_available():
            # å°è¯•åŠ è½½æ¨¡å‹åˆ° GPUï¼Œè¿™æ¯”æ£€æŸ¥ torch.classes.torchani æ›´å¯é 
            model = torchani.models.ANI1x(model_index=0)
            device = torch.device('cuda')
            model = model.to(device)
            # å¦‚æœèƒ½èµ°åˆ°è¿™æ­¥ï¼Œè¯´æ˜æ”¯æŒ CUDA
            HAS_TORCHANI_CUDA = True
            del model  # æ¸…ç†
            torch.cuda.empty_cache()
        else:
            HAS_TORCHANI_CUDA = False
    except Exception:
        HAS_TORCHANI_CUDA = False
        warnings.warn("TorchANI CUDA åŠ é€Ÿæ£€æµ‹å¤±è´¥ï¼Œå°†ä½¿ç”¨ CPU ç‰ˆæœ¬")
except ImportError:
    HAS_TORCHANI = False
    HAS_TORCHANI_CUDA = False
    warnings.warn("TorchANI æœªå®‰è£…ï¼Œå°†ä¸èƒ½ä½¿ç”¨ TorchANI åç«¯")
    
# DeepChemå’ŒTensorFlow
HAS_DEEPCHEM = False
HAS_DEEPCHEM_GPU = False
try:
    # é˜²æ­¢TensorFlowäº§ç”Ÿä¸å¿…è¦çš„æ—¥å¿—
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # ä»…æ˜¾ç¤ºERROR
    try:
        import tensorflow as tf
        # å®‰å…¨åœ°æ£€æŸ¥GPU
        physical_devices = tf.config.list_physical_devices('GPU')
        tf_gpu_available = len(physical_devices) > 0
        # è®¾ç½®å†…å­˜å¢é•¿
        if tf_gpu_available:
            for device in physical_devices:
                try:
                    tf.config.experimental.set_memory_growth(device, True)
                except:
                    pass
    except ImportError:
        tf = None
        tf_gpu_available = False
    
    # å°è¯•å¯¼å…¥DeepChem
    try:
        import deepchem as dc
        HAS_DEEPCHEM = True
        HAS_DEEPCHEM_GPU = tf_gpu_available
    except ImportError:
        warnings.warn("DeepChemæœªå®‰è£…ï¼Œå°†ä¸èƒ½ä½¿ç”¨DeepChemåç«¯")
except Exception as e:
    warnings.warn(f"åˆå§‹åŒ–TensorFlow/DeepChemæ—¶å‡ºé”™: {str(e)}")

try:
    import clara.conformer as clara_conf
    import clara.molecule as clara_mol
    HAS_CLARA = True
except ImportError:
    HAS_CLARA = False
    warnings.warn("NVIDIA Claraæœªå®‰è£…ï¼Œå°†ä¸èƒ½ä½¿ç”¨Claraåç«¯")

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(script_dir)

# è®¾ç½®Streamlité…ç½®
st.set_page_config(
    page_title="åˆ†å­åº“ä»£è¡¨æ€§å­é›†é€‰æ‹©ç³»ç»Ÿ - 3Då½¢çŠ¶å¯¹æ¯”",
    page_icon="ğŸ§¬",
    layout="wide"
)

# ç¦ç”¨æ–‡ä»¶ç›‘è§†å™¨ä»¥é¿å…PyTorchç›¸å…³é”™è¯¯
if hasattr(st, 'server'):
    st.server.server.server_options["watcher_type"] = "none"

# æ¸…ç†ç¼“å­˜ï¼ˆä½¿ç”¨æ–°çš„æ–¹æ³•ï¼‰
if st.session_state.get('clear_cache', False):
    st.cache_data.clear()
    st.cache_resource.clear()
    st.session_state.clear_cache = False

st.title("3Då½¢çŠ¶å¯¹æ¯”")

# æœ€æ–°æ›´æ–°è¯´æ˜
with st.expander("ğŸš€ æœ€æ–°æ›´æ–°ï¼šTorchANIæ··åˆç²¾åº¦ä¼˜åŒ–", expanded=False):
    st.markdown("""
    ### ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–æ›´æ–°
    **åŸºäºPyTorchå®˜æ–¹æ–‡æ¡£çš„æ··åˆç²¾åº¦æœ€ä½³å®è·µï¼š**
    
    âœ… **APIæ›´æ–°**ï¼š
    - ä½¿ç”¨ `torch.amp.GradScaler("cuda", enabled=use_amp)` æ›¿ä»£æ—§API
    - ä½¿ç”¨ `torch.autocast(device_type, dtype=torch.float16, enabled=use_amp)` 
    - æ”¯æŒ `enabled` å‚æ•°å®ç°æ— ç¼åˆ‡æ¢
    
    âœ… **è‡ªåŠ¨å›é€€æœºåˆ¶**ï¼š
    - æ··åˆç²¾åº¦å¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°FP32
    - æ™ºèƒ½é”™è¯¯å¤„ç†å’Œç”¨æˆ·å‹å¥½æç¤º
    - ä¿æŒè®¡ç®—è¿ç»­æ€§
    
    âœ… **æ•°æ®ç±»å‹ä¸€è‡´æ€§**ï¼š
    - ä¿®å¤ `masked_scatter_` æ•°æ®ç±»å‹ä¸åŒ¹é…é”™è¯¯
    - ä½¿ç”¨ `.to(energies.dtype)` ç¡®ä¿ç±»å‹å…¼å®¹
    - æ­£ç¡®çš„æ¢¯åº¦è£å‰ªé¡ºåºï¼šunscale â†’ clip â†’ step â†’ update
    
    ### ğŸ¯ é¢„æœŸæ•ˆæœ
    - **æ˜¾å­˜èŠ‚çœ**: 50%ï¼ˆFP16 vs FP32ï¼‰
    - **é€Ÿåº¦æå‡**: 2-3å€ï¼ˆåœ¨æ”¯æŒTensor Coreçš„GPUä¸Šï¼‰
    - **ç¨³å®šæ€§**: è‡ªåŠ¨å›é€€ç¡®ä¿è®¡ç®—ä¸ä¸­æ–­
    - **å…¼å®¹æ€§**: æ”¯æŒå„ç§GPUæ¶æ„
    """)

col1, col2 = st.columns(2)

with col1:
    st.subheader("æ•°æ®é›†A")
    fileA = st.file_uploader("ä¸Šä¼ ç¬¬ä¸€ä¸ªCSVæ–‡ä»¶", type="csv")
    
with col2:
    st.subheader("æ•°æ®é›†B")
    fileB = st.file_uploader("ä¸Šä¼ ç¬¬äºŒä¸ªCSVæ–‡ä»¶", type="csv")

# ç»Ÿä¸€å‚æ•°è®¾ç½®
st.subheader("å‚æ•°è®¾ç½®")
# åˆ›å»ºä¸»è¦çš„è®¾ç½®é€‰é¡¹å¡
main_tabs = st.tabs(["æ•°æ®è®¾ç½®", "æ„è±¡ç”Ÿæˆè®¾ç½®", "åˆ†æè®¾ç½®", "GPUè®¾ç½®"])

# æ•°æ®è®¾ç½®é€‰é¡¹å¡
with main_tabs[0]:
    col1, col2 = st.columns(2)
    with col1:
        smiles_col = st.text_input("SMILESåˆ—å", value="SMILES")
        max_samples = st.number_input("æ¯ä¸ªæ•°æ®é›†çš„æœ€å¤§æ ·æœ¬æ•°", 100, 5000, 500)
    
    with col2:
        shape_desc = st.selectbox("å½¢çŠ¶æè¿°ç¬¦ç±»å‹", ["USR", "USRCAT"], 
                                help="USR: è¶…å¿«å½¢çŠ¶è¯†åˆ«ï¼›USRCAT: åŒ…å«åŸå­ç±»å‹ä¿¡æ¯çš„USR")
        normalize_desc = st.checkbox("æ ‡å‡†åŒ–æè¿°ç¬¦", value=True, 
                                   help="åº”ç”¨æ ‡å‡†åŒ–ä»¥å¹³è¡¡ä¸åŒå°ºåº¦çš„ç‰¹å¾")

# æ„è±¡ç”Ÿæˆè®¾ç½®é€‰é¡¹å¡
with main_tabs[1]:
    # æ„è±¡ç”Ÿæˆå¼•æ“é€‰æ‹© - å§‹ç»ˆæ˜¾ç¤ºæ‰€æœ‰åç«¯é€‰é¡¹
    available_backends = [
        "auto",
        "rdkit", 
        "torchani", 
        "deepchem", 
        "clara"
    ]
    
    conformer_backend = st.selectbox(
        "3Dæ„è±¡ç”Ÿæˆåç«¯",
        available_backends,
        help="é€‰æ‹©ç”¨äºç”Ÿæˆ3Dæ„è±¡çš„è®¡ç®—åç«¯"
    )
    
    # æ˜¾ç¤ºåç«¯å¯ç”¨æ€§çŠ¶æ€
    with st.expander("åç«¯å¯ç”¨æ€§çŠ¶æ€", expanded=False):
        st.write("**åç«¯å®‰è£…çŠ¶æ€ï¼š**")
        col1, col2 = st.columns(2)
        with col1:
            st.write(f"âœ… RDKit: å§‹ç»ˆå¯ç”¨" if True else "âŒ RDKit: ä¸å¯ç”¨")
            st.write(f"âœ… TorchANI: å¯ç”¨" if HAS_TORCHANI else "âŒ TorchANI: ä¸å¯ç”¨")
        with col2:
            st.write(f"âœ… DeepChem: å¯ç”¨" if HAS_DEEPCHEM else "âŒ DeepChem: ä¸å¯ç”¨")
            st.write(f"âœ… NVIDIA Clara: å¯ç”¨" if HAS_CLARA else "âŒ NVIDIA Clara: ä¸å¯ç”¨")
    
    # æ£€æŸ¥æ‰€é€‰åç«¯çš„å¯ç”¨æ€§
    backend_available = True
    if conformer_backend == "torchani" and not HAS_TORCHANI:
        st.error("âŒ TorchANI æœªå®‰è£…ï¼Œè¯·å®‰è£… TorchANI æˆ–é€‰æ‹©å…¶ä»–åç«¯")
        backend_available = False
    elif conformer_backend == "deepchem" and not HAS_DEEPCHEM:
        st.error("âŒ DeepChem æœªå®‰è£…ï¼Œè¯·å®‰è£… DeepChem æˆ–é€‰æ‹©å…¶ä»–åç«¯")
        backend_available = False
    elif conformer_backend == "clara" and not HAS_CLARA:
        st.error("âŒ NVIDIA Clara æœªå®‰è£…ï¼Œè¯·å®‰è£… Clara æˆ–é€‰æ‹©å…¶ä»–åç«¯")
        backend_available = False
    
    # åŸºç¡€è®¾ç½®
    col1, col2, col3 = st.columns(3)
    with col1:
        max_attempts = st.slider("æ„è±¡ç”Ÿæˆæœ€å¤§å°è¯•æ¬¡æ•°", 10, 100, 50)
        add_hydrogens = st.checkbox("æ·»åŠ æ°¢åŸå­", value=True, 
                                  help="åœ¨æ„è±¡ç”Ÿæˆå‰æ·»åŠ æ°¢åŸå­")
    
    with col2:
        use_mmff = st.checkbox("ä½¿ç”¨åŠ›åœºä¼˜åŒ–", value=True, 
                              help="ä½¿ç”¨åˆ†å­åŠ›åœºä¼˜åŒ–æ„è±¡")
        energy_iter = st.slider("èƒ½é‡ä¼˜åŒ–è¿­ä»£æ¬¡æ•°", 0, 500, 200, 
                              disabled=not use_mmff)
    
    with col3:
        if conformer_backend == "auto":
            st.info("è‡ªåŠ¨æ¨¡å¼ï¼šç³»ç»Ÿå°†æ ¹æ®åˆ†å­å¤§å°å’Œå¯ç”¨èµ„æºé€‰æ‹©æœ€ä½³åç«¯")
            auto_select_info = """
            â€¢ å°åˆ†å­ (<50åŸå­)ï¼šä¼˜å…ˆä½¿ç”¨TorchANI
            â€¢ ä¸­ç­‰åˆ†å­ (50-100åŸå­)ï¼šä¼˜å…ˆä½¿ç”¨DeepChem
            â€¢ å¤§åˆ†å­ (>100åŸå­)ï¼šä¼˜å…ˆä½¿ç”¨Clara
            â€¢ å¦‚æœæ²¡æœ‰å¯ç”¨GPUï¼šä½¿ç”¨RDKit
            """
            st.markdown(auto_select_info)
    
    # åˆ›å»ºç‰¹å®šåç«¯è®¾ç½®é€‰é¡¹å¡
    backend_tabs = st.tabs(["TorchANI", "DeepChem", "Clara"])
    
    # TorchANIè®¾ç½®
    with backend_tabs[0]:
        if conformer_backend in ["torchani", "auto"]:
            if not HAS_TORCHANI:
                st.warning("âš ï¸ TorchANI æœªå®‰è£…ï¼Œä»¥ä¸‹è®¾ç½®ä»…ä¾›å‚è€ƒ")
            
            st.write("TorchANIè®¾ç½®")
            
            # åŸºç¡€è®¾ç½®
            col1, col2 = st.columns(2)
            with col1:
                torchani_model = st.selectbox(
                    "ç¥ç»ç½‘ç»œæ¨¡å‹",
                    ["ANI2x", "ANI1x", "ANI1ccx"],
                    help="é€‰æ‹©TorchANIçš„ç¥ç»ç½‘ç»œæ¨¡å‹",
                    disabled=not HAS_TORCHANI
                )
                optimization_steps = st.slider(
                    "ä¼˜åŒ–æ­¥æ•°", 
                    50, 500, 100,
                    disabled=not HAS_TORCHANI,
                    help="ä¼˜åŒ–è¿­ä»£æ¬¡æ•°ï¼Œå½±å“æ„è±¡è´¨é‡å’Œè®¡ç®—æ—¶é—´"
                )
            
            with col2:
                torchani_batch_size = st.slider(
                    "TorchANIæ‰¹å¤„ç†å¤§å°",
                    8, 1024, 32, # å°†æœ€å¤§å€¼ä»64ä¿®æ”¹ä¸º1024
                    disabled=not HAS_TORCHANI,
                    help="æ‰¹é‡å¤„ç†çš„åˆ†å­æ•°ï¼Œè¶Šå¤§GPUåˆ©ç”¨ç‡è¶Šé«˜ä½†å†…å­˜æ¶ˆè€—ä¹Ÿè¶Šå¤§"
                )
                use_torchani_optimization = st.checkbox(
                    "å¯ç”¨æ‰¹é‡ä¼˜åŒ–æ¨¡å¼",
                    value=True,
                    disabled=not HAS_TORCHANI,
                    help="å¯ç”¨ä¼˜åŒ–çš„æ‰¹å¤„ç†æ¨¡å¼ï¼Œå¯æ˜¾è‘—æé«˜GPUåˆ©ç”¨ç‡å’Œå¤„ç†é€Ÿåº¦"
                )
            
            # é«˜çº§è®¾ç½®
            with st.expander("ğŸ”§ TorchANIé«˜çº§è®¾ç½®", expanded=False):
                col3, col4 = st.columns(2)
                with col3:
                    learning_rate = st.slider(
                        "å­¦ä¹ ç‡",
                        0.001, 0.1, 0.01,
                        disabled=not HAS_TORCHANI,
                        help="Adamä¼˜åŒ–å™¨å­¦ä¹ ç‡ï¼Œå½±å“æ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§"
                    )
                    use_mixed_precision_torchani = st.checkbox(
                        "ä½¿ç”¨æ··åˆç²¾åº¦",
                        value=True,
                        disabled=not HAS_TORCHANI,
                        help="ä½¿ç”¨FP16æ··åˆç²¾åº¦è®¡ç®—ä»¥èŠ‚çœGPUå†…å­˜å¹¶æé«˜é€Ÿåº¦ã€‚å¦‚é‡åˆ°æ•°æ®ç±»å‹é”™è¯¯ï¼Œè¯·ç¦ç”¨æ­¤é€‰é¡¹"
                    )
                    
                    if use_mixed_precision_torchani and HAS_TORCHANI:
                        st.info("ğŸ’¡ **æ··åˆç²¾åº¦è¯´æ˜**")
                        st.markdown("""
                        - **ä¼˜åŠ¿**: èŠ‚çœ50%æ˜¾å­˜ï¼Œæé«˜2-3å€è®¡ç®—é€Ÿåº¦ï¼ˆéœ€Volta/Turing/Ampereæ¶æ„ï¼‰
                        - **è‡ªåŠ¨å›é€€**: å¦‚é‡æ•°æ®ç±»å‹é”™è¯¯ä¼šè‡ªåŠ¨åˆ‡æ¢åˆ°FP32
                        - **æœ€ä½³å®è·µ**: é‡‡ç”¨PyTorchå®˜æ–¹æ¨èçš„autocast + GradScaleræ¨¡å¼
                        """)
                        
                        if torch.cuda.is_available():
                            gpu_name = torch.cuda.get_device_name(0)
                            if any(arch in gpu_name.upper() for arch in ['V100', 'A100', 'RTX', 'TITAN RTX', 'QUADRO RTX']):
                                st.success("âœ… æ£€æµ‹åˆ°æ”¯æŒTensor Coreçš„GPUï¼Œæ··åˆç²¾åº¦æ•ˆæœæœ€ä½³")
                            elif any(arch in gpu_name.upper() for arch in ['GTX 16', 'GTX 20', 'GTX 30', 'GTX 40']):
                                st.info("â„¹ï¸ å½“å‰GPUæ”¯æŒæ··åˆç²¾åº¦ï¼Œé¢„æœŸæœ‰é€‚åº¦åŠ é€Ÿ")
                            else: # This else corresponds to the inner if torch.cuda.is_available()
                                st.warning("âš ï¸ å½“å‰GPUå¯èƒ½ä¸æ”¯æŒTensor Coreï¼Œæ··åˆç²¾åº¦åŠ é€Ÿæ•ˆæœæœ‰é™")
                    # Linter Error: Unindent amount does not match previous indent (Line 331 for elif)
                    # This elif should align with the `if use_mixed_precision_torchani and HAS_TORCHANI:`
                    elif not use_mixed_precision_torchani and HAS_TORCHANI: 
                        st.warning("âš ï¸ **æ··åˆç²¾åº¦å·²ç¦ç”¨**") # Linter Error: Unexpected indentation (Line 332)
                        st.markdown("""
                        - GPUå†…å­˜ä½¿ç”¨å°†å¢åŠ çº¦2å€
                        - è®¡ç®—é€Ÿåº¦å¯èƒ½é™ä½2-3å€
                        - ä½†æ•°å€¼ç²¾åº¦æ›´é«˜ï¼Œæ›´ç¨³å®š
                        """)
                        
                        st.info("ğŸ’¡ å¦‚æœé‡åˆ°dtypeé”™è¯¯ï¼Œå¯ä»¥å°è¯•ï¼š")
                        st.markdown("""
                        1. å‡å°æ‰¹å¤„ç†å¤§å°
                        2. é™ä½å­¦ä¹ ç‡
                        3. ç¦ç”¨æ¢¯åº¦è£å‰ª
                        4. æ›´æ–°PyTorchåˆ°æœ€æ–°ç‰ˆæœ¬
                        """)
                
                # Linter Error: Unindent amount does not match previous indent (Line 348 for with col4:)
                # This with col4: should align with `with col3:`
                with col4:
                    gradient_clipping = st.checkbox(
                        "æ¢¯åº¦è£å‰ª",
                        value=True,
                        disabled=not HAS_TORCHANI,
                        help="é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œæé«˜ä¼˜åŒ–ç¨³å®šæ€§"
                    )
            
            # Linter Error: Unindent amount does not match previous indent (Line 363 for if HAS_TORCHANI:)
            # This if HAS_TORCHANI: should align with the `with st.expander(...)`
            if HAS_TORCHANI:
                if use_torchani_optimization:
                    st.success("âœ… æ‰¹é‡ä¼˜åŒ–æ¨¡å¼å·²å¯ç”¨ï¼Œé¢„æœŸæ€§èƒ½æå‡ 5-20x")
                    if torch.cuda.is_available():
                        gpu_name = torch.cuda.get_device_name(0)
                        if "RTX" in gpu_name or "Tesla" in gpu_name or "A100" in gpu_name:
                            st.info("ğŸš€ æ£€æµ‹åˆ°é«˜æ€§èƒ½GPUï¼Œå»ºè®®å¢å¤§æ‰¹å¤„ç†å¤§å°ä»¥è·å¾—æœ€ä½³æ€§èƒ½")
                    else:
                        st.warning("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
                else:
                    st.warning("âš ï¸ æ‰¹é‡ä¼˜åŒ–æœªå¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿçš„é€ä¸ªå¤„ç†æ¨¡å¼")
                    
                with st.expander("ğŸ”§ æ··åˆç²¾åº¦æœ€ä½³å®è·µå’Œæ•…éšœæ’é™¤", expanded=False):
                    st.markdown("### ğŸ” ç‰ˆæœ¬å…¼å®¹æ€§æ£€æŸ¥")
                    import torch # Local import, consider moving to top if not already there
                    import sys # Local import
                    
                    torch_version = torch.__version__
                    python_version = sys.version.split()[0]
                    
                    st.info(f"**å½“å‰ç¯å¢ƒ:**")
                    st.write(f"- Python: {python_version}")
                    st.write(f"- PyTorch: {torch_version}")
                    
                    if HAS_TORCHANI:
                        try:
                            import torchani # Local import
                            torchani_version = torchani.__version__
                            st.write(f"- TorchANI: {torchani_version}")
                            
                            if torch_version >= "1.12.0" and torchani_version <= "2.2.0":
                                st.warning("âš ï¸ **å·²çŸ¥å…¼å®¹æ€§é—®é¢˜**: TorchANI â‰¤ 2.2.0 ä¸ PyTorch â‰¥ 1.12.0 åœ¨æ··åˆç²¾åº¦ä¸‹å¯èƒ½ä¸å…¼å®¹")
                                st.info("å»ºè®®å‡çº§TorchANIåˆ°æœ€æ–°ç‰ˆæœ¬: `pip install --upgrade torchani`")
                        except:
                            st.write("- TorchANI: æ— æ³•è·å–ç‰ˆæœ¬ä¿¡æ¯")
                    
                    if torch.cuda.is_available():
                        cuda_version = torch.version.cuda
                        cudnn_version = torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else "N/A"
                        st.write(f"- CUDA: {cuda_version}")
                        st.write(f"- cuDNN: {cudnn_version}")
                        
                        gpu_name = torch.cuda.get_device_name(0)
                        gpu_capability = torch.cuda.get_device_capability(0)
                        st.write(f"- GPU: {gpu_name}")
                        st.write(f"- è®¡ç®—èƒ½åŠ›: {gpu_capability[0]}.{gpu_capability[1]}")
                        
                        if gpu_capability[0] >= 7:
                            st.success("âœ… GPUæ”¯æŒTensor Coreï¼Œæ··åˆç²¾åº¦æ•ˆæœæœ€ä½³")
                        elif gpu_capability[0] >= 6:
                            st.info("â„¹ï¸ GPUéƒ¨åˆ†æ”¯æŒæ··åˆç²¾åº¦ï¼Œæ•ˆæœæœ‰é™")
                        else:
                            st.warning("âš ï¸ GPUä¸æ”¯æŒæ··åˆç²¾åº¦åŠ é€Ÿ")
                    
                    st.markdown("### ğŸ§ª æ··åˆç²¾åº¦å…¼å®¹æ€§æµ‹è¯•")
                    if st.button("è¿è¡ŒTorchANIæ··åˆç²¾åº¦å…¼å®¹æ€§æµ‹è¯•"):
                        if HAS_TORCHANI and torch.cuda.is_available():
                            try:
                                st.info("æ­£åœ¨æµ‹è¯•TorchANIæ··åˆç²¾åº¦å…¼å®¹æ€§...")
                                from rdkit import Chem # Local import
                                from rdkit.Chem import AllChem # Local import
                                test_mol = Chem.MolFromSmiles("CCO")
                                test_mol = Chem.AddHs(test_mol)
                                AllChem.EmbedMolecule(test_mol)
                                
                                coords = []
                                species_atomic_nums = []
                                for i in range(test_mol.GetNumAtoms()):
                                    atom = test_mol.GetAtomWithIdx(i)
                                    pos = test_mol.GetConformer().GetAtomPosition(i)
                                    coords.append([pos.x, pos.y, pos.z])
                                    species_atomic_nums.append(atom.GetAtomicNum())
                                
                                coords_tensor = torch.tensor([coords], dtype=torch.float32).cuda()
                                
                                model_test = torchani.models.ANI2x(periodic_table_index=False).cuda().eval()
                                # Assuming SUPPORTED_SPECIES_PREPROC is globally available from previous edits
                                symbol_to_int_test = torchani.utils.ChemicalSymbolsToInts(list(SUPPORTED_SPECIES_PREPROC.values()))
                                symbols_test = [SUPPORTED_SPECIES_PREPROC.get(s_num, 'X') for s_num in species_atomic_nums]
                                species_idx_test = symbol_to_int_test(symbols_test).unsqueeze(0).cuda()
                                
                                test_results = {}
                                try:
                                    with torch.no_grad():
                                        model_test((species_idx_test, coords_tensor)).energies
                                    test_results["FP32"] = "âœ… æˆåŠŸ"
                                except Exception as e_fp32:
                                    test_results["FP32"] = f"âŒ å¤±è´¥: {str(e_fp32)}"
                                
                                try:
                                    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16):
                                        model_test((species_idx_test, coords_tensor)).energies
                                    test_results["AMP (autocast)"] = "âœ… æˆåŠŸ"
                                except Exception as e_amp:
                                    test_results["AMP (autocast)"] = f"âŒ å¤±è´¥: {str(e_amp)[:100]}..."
                                
                                st.write("**æµ‹è¯•ç»“æœ:**")
                                for test_name, result in test_results.items():
                                    st.write(f"- {test_name}: {result}")
                                
                                if "âœ… æˆåŠŸ" in test_results.get("AMP (autocast)", ""):
                                    st.success("ğŸ‰ TorchANIæ··åˆç²¾åº¦å…¼å®¹æ€§æµ‹è¯•é€šè¿‡ï¼")
                                else:
                                    st.error("âŒ TorchANIæ··åˆç²¾åº¦å…¼å®¹æ€§æµ‹è¯•å¤±è´¥ã€‚")
                            except Exception as e_test:
                                st.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {str(e_test)}")
                        else:
                            st.warning("éœ€è¦TorchANIå’ŒCUDAæ”¯æŒæ‰èƒ½è¿›è¡Œæµ‹è¯•")
                    
                    st.markdown("""
                    ### ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®
                    **æœ€å¤§åŒ–æ··åˆç²¾åº¦æ•ˆæœ:**
                    - ç¡®ä¿æ‰¹å¤„ç†å¤§å°æ˜¯8çš„å€æ•°ï¼ˆåˆ©ç”¨Tensor Coreï¼‰
                    - ä½¿ç”¨æ”¯æŒTensor Coreçš„GPUï¼ˆVolta/Turing/Ampereæ¶æ„ï¼‰
                    - ä¿æŒç½‘ç»œè¶³å¤Ÿå¤æ‚ä»¥å……åˆ†åˆ©ç”¨GPU
                    
                    ### âš ï¸ å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
                    **1. 'masked_scatter_: expected self and source to have same dtypes but got Float and Half'**
                    - âœ… å·²ä¿®å¤ï¼šç°åœ¨ä½¿ç”¨ `.to(energies.dtype)` ç¡®ä¿ç±»å‹ä¸€è‡´
                    - è‡ªåŠ¨å›é€€ï¼šå¦‚æœæ··åˆç²¾åº¦å¤±è´¥ä¼šè‡ªåŠ¨åˆ‡æ¢åˆ°FP32
                    
                    **2. 'CUDNN_STATUS_BAD_PARAM' æˆ–ç±»å‹ä¸åŒ¹é…é”™è¯¯**
                    - å‡å°æ‰¹å¤„ç†å¤§å°åˆ°16æˆ–8
                    - é™ä½å­¦ä¹ ç‡åˆ°0.001
                    - ç¦ç”¨æ¢¯åº¦è£å‰ª
                    
                    **3. å†…å­˜ä¸è¶³ (OOM)**
                    - å‡å° `max_atoms_per_batch` å‚æ•°
                    - é™ä½æ‰¹å¤„ç†å¤§å°
                    - å¯ç”¨æ¢¯åº¦ç´¯ç§¯æ¨¡å¼
                    
                    **4. æ€§èƒ½æå‡ä¸æ˜æ˜¾**
                    - æ£€æŸ¥GPUæ˜¯å¦æ”¯æŒTensor Core
                    - å¢å¤§æ‰¹å¤„ç†å¤§å°ä»¥å……åˆ†åˆ©ç”¨GPU
                    - ç¡®ä¿åˆ†å­å¤æ‚åº¦è¶³å¤Ÿï¼ˆ>20ä¸ªåŸå­ï¼‰
                    
                    ### ğŸš€ ç°ä»£PyTorchæœ€ä½³å®è·µ
                    **æˆ‘ä»¬å·²é‡‡ç”¨çš„å®˜æ–¹æ¨èåšæ³•:**
                    ```python
                    # æ–°çš„APIï¼ˆæ¨èï¼‰
                    scaler = torch.amp.GradScaler("cuda", enabled=use_amp)
                    with torch.autocast(device_type=device.type, dtype=torch.float16, enabled=use_amp):
                        # å‰å‘ä¼ æ’­
                    
                    # æ­£ç¡®çš„æ¢¯åº¦å¤„ç†é¡ºåº
                    scaler.scale(loss).backward()
                    scaler.unscale_(optimizer)  # æ¢¯åº¦è£å‰ªå‰å¿…é¡»unscale
                    torch.nn.utils.clip_grad_norm_(...)
                    scaler.step(optimizer)
                    scaler.update()
                    ```
                    """)
                    
                    # PyTorchç‰ˆæœ¬æ£€æŸ¥
                    if torch_version < "1.10.0":
                        st.warning("âš ï¸ å»ºè®®å‡çº§åˆ°PyTorch 1.10+ä»¥è·å¾—æœ€ä½³æ··åˆç²¾åº¦æ”¯æŒ")
                    else:
                        st.success("âœ… PyTorchç‰ˆæœ¬æ”¯æŒæ–°çš„æ··åˆç²¾åº¦API")
                
                # GPUå†…å­˜ä¼°ç®—
                if torch.cuda.is_available() and use_torchani_optimization:
                    estimated_mem = torchani_batch_size * 50  # æ¯ä¸ªåˆ†å­å¤§çº¦50MB
                    gpu_total_mem = torch.cuda.get_device_properties(0).total_memory / 1024**2
                    mem_usage = (estimated_mem / gpu_total_mem) * 100
                    
                    if mem_usage > 80:
                        st.error(f"âš ï¸ ä¼°ç®—GPUå†…å­˜ä½¿ç”¨: {mem_usage:.1f}%ï¼Œå»ºè®®å‡å°æ‰¹å¤„ç†å¤§å°")
                    elif mem_usage > 60:
                        st.warning(f"âš ï¸ ä¼°ç®—GPUå†…å­˜ä½¿ç”¨: {mem_usage:.1f}%ï¼Œæ³¨æ„ç›‘æ§å†…å­˜")
                    else:
                        st.info(f"âœ… ä¼°ç®—GPUå†…å­˜ä½¿ç”¨: {mem_usage:.1f}%ï¼Œè®¾ç½®åˆç†")
            
            if not HAS_TORCHANI:
                st.info("ğŸ’¡ å®‰è£… TorchANI: `pip install torchani`")
        else:
            st.info("å½“å‰æœªé€‰æ‹© TorchANI åç«¯")
    
    # DeepChemè®¾ç½®
    with backend_tabs[1]:
        if conformer_backend in ["deepchem", "auto"]:
            if not HAS_DEEPCHEM:
                st.warning("âš ï¸ DeepChem æœªå®‰è£…ï¼Œä»¥ä¸‹è®¾ç½®ä»…ä¾›å‚è€ƒ")
            
            st.write("DeepChemè®¾ç½®")
            deepchem_model = st.selectbox(
                "æ¨¡å‹ç±»å‹",
                ["mpnn", "schnet", "cgcnn"],
                help="é€‰æ‹©DeepChemçš„åˆ†å­è¡¨ç¤ºæ¨¡å‹",
                disabled=not HAS_DEEPCHEM
            )
            use_mixed_precision = st.checkbox(
                "ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ",
                value=True,
                help="å¯ç”¨FP16æ··åˆç²¾åº¦ä»¥æé«˜æ€§èƒ½",
                disabled=not HAS_DEEPCHEM
            )
            batch_size_dc = st.slider(
                "æ‰¹å¤„ç†å¤§å°",
                16, 256, 64,
                help="DeepChemçš„æ‰¹å¤„ç†å¤§å°",
                disabled=not HAS_DEEPCHEM
            )
            dc_force_field = st.selectbox(
                "åŠ›åœºç±»å‹",
                ["mmff94s", "uff", "gaff"],
                help="DeepChemä½¿ç”¨çš„åŠ›åœº",
                disabled=not HAS_DEEPCHEM
            )
            
            if not HAS_DEEPCHEM:
                st.info("ğŸ’¡ å®‰è£… DeepChem: `pip install deepchem`")
        else:
            st.info("å½“å‰æœªé€‰æ‹© DeepChem åç«¯")
    
    # Claraè®¾ç½®
    with backend_tabs[2]:
        if conformer_backend in ["clara", "auto"]:
            if not HAS_CLARA:
                st.warning("âš ï¸ NVIDIA Clara æœªå®‰è£…ï¼Œä»¥ä¸‹è®¾ç½®ä»…ä¾›å‚è€ƒ")
            
            st.write("NVIDIA Claraè®¾ç½®")
            clara_force_field = st.selectbox(
                "åŠ›åœº",
                ["MMFF94s", "UFF", "GAFF"],
                help="é€‰æ‹©Claraçš„åŠ›åœº",
                disabled=not HAS_CLARA
            )
            clara_precision = st.selectbox(
                "è®¡ç®—ç²¾åº¦",
                ["mixed", "fp32", "fp16"],
                help="é€‰æ‹©è®¡ç®—ç²¾åº¦",
                disabled=not HAS_CLARA
            )
            clara_num_conformers = st.slider(
                "æ„è±¡æ•°é‡",
                1, 10, 1,
                help="ç”Ÿæˆçš„æ„è±¡æ•°é‡",
                disabled=not HAS_CLARA
            )
            clara_energy_threshold = st.slider(
                "èƒ½é‡é˜ˆå€¼(kcal/mol)",
                0.1, 10.0, 1.0,
                help="èƒ½é‡ç­›é€‰é˜ˆå€¼",
                disabled=not HAS_CLARA
            )
            clara_optimization_steps = st.slider(
                "ä¼˜åŒ–æ­¥æ•°", 
                100, 1000, 500,
                help="Claraä¼˜åŒ–è¿­ä»£æ¬¡æ•°",
                disabled=not HAS_CLARA
            )
            
            if not HAS_CLARA:
                st.info("ğŸ’¡ å®‰è£… NVIDIA Clara: å‚è€ƒ NVIDIA Clara å®˜æ–¹æ–‡æ¡£")
        else:
            st.info("å½“å‰æœªé€‰æ‹© NVIDIA Clara åç«¯")

# åˆ†æè®¾ç½®é€‰é¡¹å¡
with main_tabs[2]:
    col1, col2 = st.columns(2)
    with col1:
        dim_reduction = st.selectbox("é™ç»´æ–¹æ³•", ["t-SNE", "UMAP"])
        if dim_reduction == "t-SNE":
            perplexity = st.slider("t-SNEå›°æƒ‘åº¦", 5, 50, 30)
        else:
            n_neighbors = st.slider("UMAPé‚»å±…æ•°", 5, 50, 15)
            min_dist = st.slider("UMAPæœ€å°è·ç¦»", 0.01, 0.99, 0.1, 0.01)
    
    with col2:
        st.write("å¯è§†åŒ–è®¾ç½®")
        plot_height = st.slider("å›¾è¡¨é«˜åº¦", 400, 1000, 600)
        plot_width = st.slider("å›¾è¡¨å®½åº¦", 400, 1000, 800)
        color_scheme = st.selectbox("é…è‰²æ–¹æ¡ˆ", 
                           ["viridis", "plasma", "inferno", "magma", "cividis"])

# GPUè®¾ç½®é€‰é¡¹å¡
with main_tabs[3]:
    col1, col2 = st.columns(2)
    with col1:
        enable_gpu = st.checkbox("å¯ç”¨GPUåŠ é€Ÿ", value=True, 
                                help="ä½¿ç”¨GPUåŠ é€Ÿè®¡ç®—")
        auto_batch = st.checkbox("è‡ªåŠ¨æ‰¹å¤„ç†å¤§å°", value=True, 
                                 help="æ ¹æ®GPUå†…å­˜è‡ªåŠ¨è°ƒæ•´æ‰¹å¤„ç†å¤§å°")
        batch_size = st.slider("æ‰¹å¤„ç†å¤§å°", 10, 500, 50, 
                             disabled=auto_batch)
    
    with col2:
        if enable_gpu:
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                gpu_mem_total = torch.cuda.get_device_properties(0).total_memory / 1024**2
                
                st.success(f"âœ… GPUå¯ç”¨: {gpu_name}")
                st.info(f"æ€»æ˜¾å­˜: {gpu_mem_total:.1f} MB")
                
                # GPUä½¿ç”¨ç­–ç•¥
                gpu_strategy = st.selectbox(
                    "GPUä½¿ç”¨ç­–ç•¥",
                    ["å¹³è¡¡", "æ€§èƒ½ä¼˜å…ˆ", "å†…å­˜ä¼˜å…ˆ"],
                    help="å¹³è¡¡ï¼šå¹³è¡¡é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨ï¼›æ€§èƒ½ä¼˜å…ˆï¼šæ›´å¿«ä½†ä½¿ç”¨æ›´å¤šå†…å­˜ï¼›å†…å­˜ä¼˜å…ˆï¼šèŠ‚çœå†…å­˜ä½†è¾ƒæ…¢"
                )
                
                # GPUå†…å­˜é™åˆ¶
                gpu_mem_limit = st.slider(
                    "GPUå†…å­˜ä½¿ç”¨é™åˆ¶ (%)",
                    10, 95, 80,
                    help="é™åˆ¶GPUå†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯”ä»¥é˜²æ­¢å´©æºƒ"
                )
            else:
                st.error("âŒ æœªæ£€æµ‹åˆ°å¯ç”¨çš„GPU")
                st.info("å°†ä½¿ç”¨CPUè¿›è¡Œè®¡ç®—ï¼Œé€Ÿåº¦å¯èƒ½è¾ƒæ…¢")
        else:
            st.info("GPUåŠ é€Ÿå·²ç¦ç”¨ï¼Œå°†ä½¿ç”¨CPUè¿›è¡Œè®¡ç®—")

def initialize_cuda():
    """åˆå§‹åŒ–CUDAè®¾å¤‡å¹¶è¿”å›è®¾å¤‡ä¿¡æ¯"""
    try:
        cuda_available = torch.cuda.is_available()
        device = torch.device("cuda" if cuda_available else "cpu")
        
        if cuda_available:
            torch.cuda.empty_cache()
            gpu_name = torch.cuda.get_device_name(0)
            gpu_mem_total = torch.cuda.get_device_properties(0).total_memory / 1024**2
            gpu_mem_alloc = torch.cuda.memory_allocated(0) / 1024**2
            gpu_mem_cached = torch.cuda.memory_reserved(0) / 1024**2
            
            st.sidebar.success("âœ… CUDAå¯ç”¨ï¼Œå°†ä½¿ç”¨GPUåŠ é€Ÿ")
            st.sidebar.info(
                f"GPUä¿¡æ¯:\n"
                f"- è®¾å¤‡: {gpu_name}\n"
                f"- æ€»æ˜¾å­˜: {gpu_mem_total:.1f}MB\n"
                f"- å·²åˆ†é…: {gpu_mem_alloc:.1f}MB\n"
                f"- å·²ç¼“å­˜: {gpu_mem_cached:.1f}MB"
            )
        else:
            st.sidebar.info("â„¹ï¸ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®¡ç®—")
        
        return cuda_available, device
    except Exception as e:
        st.sidebar.error(f"GPUåˆå§‹åŒ–é”™è¯¯: {str(e)}")
        return False, torch.device("cpu")

def load_smiles(file, smiles_col='SMILES'):
    """ä»CSVè¯»å–SMILESå¹¶è½¬æ¢ä¸ºRDKit Molå¯¹è±¡"""
    try:
        df = pd.read_csv(file)
        if smiles_col not in df.columns:
            st.error(f"æœªæ‰¾åˆ°SMILESåˆ—: {smiles_col}")
            return None, None
        
        mols = []
        valid_indices = []
        for i, row in df.iterrows():
            smi = str(row[smiles_col]).strip()
            mol = Chem.MolFromSmiles(smi)
            if mol:
                mols.append(mol)
                valid_indices.append(i)
        
        return mols, df.iloc[valid_indices]
    except Exception as e:
        st.error(f"è¯»å–CSVæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return None, None

def generate_3d_conformer(mol, max_attempts=50, use_mmff=True, energy_iter=200, add_hydrogens=True):
    """ç”Ÿæˆ3Dæ„è±¡ï¼Œæ”¯æŒæ›´å¤šé…ç½®é€‰é¡¹"""
    if mol is None:
        return None
    
    try:
        # æ ¹æ®éœ€è¦æ·»åŠ æ°¢åŸå­
        mol_3d = Chem.AddHs(mol) if add_hydrogens else Chem.Mol(mol)
        
        # è®¾ç½®ETKDGå‚æ•°
        ps = AllChem.ETKDGv3()
        # æ³¨æ„ï¼šRDKit ä¸­æ­£ç¡®çš„å‚æ•°åæ˜¯ maxAttemptsï¼Œä¸æ˜¯ maxAttempts
        # æ£€æŸ¥ RDKit ç‰ˆæœ¬å…¼å®¹æ€§
        try:
            ps.maxAttempts = max_attempts
        except AttributeError:
            # å¦‚æœä¸æ”¯æŒ maxAttemptsï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®
            st.warning("å½“å‰RDKitç‰ˆæœ¬ä¸æ”¯æŒmaxAttemptså‚æ•°ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®")
        
        ps.randomSeed = 42  # è®¾ç½®éšæœºç§å­ä»¥æé«˜å¯é‡å¤æ€§
        ps.numThreads = 0  # ä½¿ç”¨æ‰€æœ‰å¯ç”¨çº¿ç¨‹
        ps.useRandomCoords = True  # ä½¿ç”¨éšæœºåˆå§‹åæ ‡
        
        # åµŒå…¥åˆ†å­
        cid = AllChem.EmbedMolecule(mol_3d, ps)
        if cid < 0:
            # å¦‚æœ ETKDG å¤±è´¥ï¼Œå°è¯•æ›´ç®€å•çš„æ–¹æ³•
            st.warning("ETKDGåµŒå…¥å¤±è´¥ï¼Œå°è¯•åŸºæœ¬åµŒå…¥æ–¹æ³•")
            try:
                # å°è¯•å¤šæ¬¡åµŒå…¥
                for attempt in range(max_attempts):
                    cid = AllChem.EmbedMolecule(mol_3d, randomSeed=42 + attempt)
                    if cid >= 0:
                        break
                    if cid < 0:
                        return None
            except Exception:
                return None
        
        # åº”ç”¨åŠ›åœºä¼˜åŒ–
        if use_mmff:
            try:
                # å°è¯•MMFFä¼˜åŒ–
                result = AllChem.MMFFOptimizeMolecule(mol_3d, maxIters=energy_iter)
                if result != 0:
                # å¦‚æœMMFFå¤±è´¥ï¼Œå°è¯•UFF
                    st.warning("MMFFä¼˜åŒ–å¤±è´¥ï¼Œå°è¯•UFFä¼˜åŒ–")
                AllChem.UFFOptimizeMolecule(mol_3d, maxIters=energy_iter)
            except Exception as opt_error:
                st.warning(f"åŠ›åœºä¼˜åŒ–å¤±è´¥: {str(opt_error)}ï¼Œè·³è¿‡ä¼˜åŒ–æ­¥éª¤")
        
        # å¦‚æœæ·»åŠ äº†æ°¢åŸå­ï¼Œç°åœ¨å»é™¤å®ƒä»¬
        if add_hydrogens:
            mol_3d = Chem.RemoveHs(mol_3d)
        
        return mol_3d
    except Exception as e:
        st.warning(f"3Dæ„è±¡ç”Ÿæˆå¤±è´¥: {str(e)}")
        return None

# å…¨å±€æˆ–æ¨¡å—çº§å˜é‡ï¼Œç¡®ä¿ preprocess_single_mol_for_torchani å¯ä»¥è®¿é—®
# è¿™äº›é€šå¸¸åœ¨ Streamlit åº”ç”¨çš„é¡¶éƒ¨å®šä¹‰
# ç¡®ä¿ HAS_TORCHANI, atomic_numbers_to_symbols ç­‰å·²å®šä¹‰ä¸”åœ¨æ­¤ä½œç”¨åŸŸå¯è§
# å¦‚æœå®ƒä»¬åªåœ¨ Streamlit ä¸»å‡½æ•°æµä¸­å®šä¹‰ï¼Œéœ€è¦è°ƒæ•´æˆ–ä¼ é€’å®ƒä»¬

# å‡è®¾ atomic_numbers_to_symbols å·²ç»åœ¨å…¨å±€æˆ–æ¨¡å—çº§åˆ«å®šä¹‰ï¼Œä¾‹å¦‚ï¼š
# atomic_numbers_to_symbols = {1: 'H', 6: 'C', 7: 'N', 8: 'O', 9: 'F', 16: 'S', 17: 'Cl'}
# å¦‚æœå®ƒæ˜¯åœ¨æŸä¸ªå‡½æ•°å†…éƒ¨å®šä¹‰çš„ï¼Œä½ éœ€è¦æŠŠå®ƒç§»åˆ°å…¨å±€æˆ–è€…ä½œä¸ºå‚æ•°ä¼ é€’ç»™é¢„å¤„ç†å‡½æ•°

SUPPORTED_SPECIES_PREPROC = {
    1: 'H', 6: 'C', 7: 'N', 8: 'O', 9: 'F', 16: 'S', 17: 'Cl'
}

def preprocess_single_mol_for_torchani(args):
    """ä¸ºTorchANIé¢„å¤„ç†å•ä¸ªåˆ†å­ï¼Œç”¨äºå¤šè¿›ç¨‹å¤„ç†ã€‚"""
    original_idx, mol_smiles_or_rdkit_mol = args # å‡è®¾molæ˜¯RDKit Molå¯¹è±¡æˆ–SMILESå­—ç¬¦ä¸²

    # å¦‚æœä¼ å…¥çš„æ˜¯SMILESï¼Œå…ˆè½¬æ¢ä¸ºMolå¯¹è±¡ (è¿™å–å†³äºmolsåˆ—è¡¨çš„å†…å®¹)
    # ä¸ºç®€åŒ–ï¼Œå‡è®¾molsåˆ—è¡¨å·²ç»æ˜¯RDKit Molå¯¹è±¡
    # if isinstance(mol_smiles_or_rdkit_mol, str):
    #     mol = Chem.MolFromSmiles(mol_smiles_or_rdkit_mol)
    # else:
    mol = mol_smiles_or_rdkit_mol

    if mol is None:
        return {'original_idx': original_idx, 'mol_h': None, 'original_species_len': 0, 'error': 'Input mol is None'}

    try:
        mol_h = Chem.AddHs(mol)
        num_atoms = mol_h.GetNumAtoms()

        supported = True
        for atom in mol_h.GetAtoms():
            if atom.GetAtomicNum() not in SUPPORTED_SPECIES_PREPROC:
                supported = False
                break
        
        if not supported:
            return {'original_idx': original_idx, 'mol_h': None, 'original_species_len': 0, 'error': 'Unsupported atom types'}
        
        if mol_h.GetNumConformers() == 0:
            # ä½¿ç”¨æ›´é²æ£’çš„åµŒå…¥å‚æ•°
            ps = AllChem.ETKDGv3()
            ps.randomSeed = original_idx # Vary seed per molecule for better diversity if needed
            ps.numThreads = 0 # Use all available cores for embedding this single molecule by RDKit if it supports it
            embed_result = AllChem.EmbedMolecule(mol_h, ps)
            if embed_result < 0: # ETKDGå¤±è´¥
                # å°è¯•å¤‡ç”¨æ–¹æ³•
                embed_result = AllChem.EmbedMolecule(mol_h, useRandomCoords=True, forceBasicKnowledge=True, randomSeed=original_idx + 1000)
                if embed_result < 0:
                     return {'original_idx': original_idx, 'mol_h': None, 'original_species_len': 0, 'error': 'Initial conformer embedding failed after multiple attempts'}
        
        return {'original_idx': original_idx, 'mol_h': mol_h, 'original_species_len': num_atoms, 'error': None}
    except Exception as e:
        return {'original_idx': original_idx, 'mol_h': None, 'original_species_len': 0, 'error': f'Preprocessing exception: {str(e)}'}

def generate_3d_conformer_torchani_optimized(mols, model_name='ANI2x', optimization_steps=100, device=None, 
                                           batch_size=32, learning_rate=0.01, use_mixed_precision_torchani=True, 
                                           max_atoms_per_batch=5000, gradient_clipping=True,
                                           progress_bar_ui=None, 
                                           progress_text_ui=None,
                                           status_container_ui=None):
    """ä½¿ç”¨TorchANIæ‰¹é‡ç”Ÿæˆ3Dæ„è±¡ - ä¼˜åŒ–ç‰ˆæœ¬
    
    é‡‡ç”¨PyTorchå®˜æ–¹æ¨èçš„è‡ªåŠ¨æ··åˆç²¾åº¦æœ€ä½³å®è·µ:
    - ä½¿ç”¨ torch.autocast(device_type, dtype=torch.float16, enabled=use_amp) 
    - ä½¿ç”¨ torch.amp.GradScaler("cuda", enabled=use_amp)
    - è‡ªåŠ¨å›é€€æœºåˆ¶ï¼šæ··åˆç²¾åº¦å¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°FP32
    - æ­£ç¡®çš„æ¢¯åº¦è£å‰ªé¡ºåºï¼šunscale -> clip -> step -> update
    
    Args:
        mols: åˆ†å­åˆ—è¡¨
        model_name: TorchANIæ¨¡å‹åç§° ('ANI1x', 'ANI1ccx', 'ANI2x')
        optimization_steps: ä¼˜åŒ–è¿­ä»£æ¬¡æ•°
        device: è®¡ç®—è®¾å¤‡
        batch_size: æ‰¹å¤„ç†å¤§å°
        learning_rate: Adamä¼˜åŒ–å™¨å­¦ä¹ ç‡
        use_mixed_precision_torchani: æ˜¯å¦å¯ç”¨æ··åˆç²¾åº¦ï¼ˆFP16ï¼‰
        max_atoms_per_batch: æ¯æ‰¹æ¬¡æœ€å¤§åŸå­æ•°é™åˆ¶
        gradient_clipping: æ˜¯å¦å¯ç”¨æ¢¯åº¦è£å‰ª
    
    Returns:
        list: ä¼˜åŒ–åçš„åˆ†å­åˆ—è¡¨
    """
    if not HAS_TORCHANI or not mols:
        return [None] * len(mols)
        
    try:
        if device is None:
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
            st.info(f"ğŸš€ TorchANIä¼˜åŒ–æ‰¹å¤„ç† - æ‰¹å¤§å°: {batch_size}, è®¾å¤‡: {device}, æ··åˆç²¾åº¦: {use_mixed_precision_torchani}")
        
        # é¢„åŠ è½½æ¨¡å‹ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
        if model_name == 'ANI1x':
            model = torchani.models.ANI1x(periodic_table_index=False).to(device)
        elif model_name == 'ANI1ccx':
            model = torchani.models.ANI1ccx(periodic_table_index=False).to(device)
        elif model_name == 'ANI2x':
            model = torchani.models.ANI2x(periodic_table_index=False).to(device)
        else:
            model = torchani.models.ANI2x(periodic_table_index=False).to(device)
        
        model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        
        # æ”¯æŒçš„å…ƒç´ 
        supported_species = ['H', 'C', 'N', 'O', 'F', 'S', 'Cl']
        atomic_numbers_to_symbols = {1: 'H', 6: 'C', 7: 'N', 8: 'O', 9: 'F', 16: 'S', 17: 'Cl'}
        symbol_to_int = torchani.utils.ChemicalSymbolsToInts(supported_species)
        
        results = []
        start_time = time.time()
        
        initial_mol_count = len(mols)
        if progress_text_ui:
            progress_text_ui.text(f"TorchANI: é¢„å¤„ç† {initial_mol_count} ä¸ªåˆ†å­...")

        # ä½¿ç”¨ ThreadPoolExecutor å¹¶è¡ŒåŒ–é¢„å¤„ç†
        processed_mols_info_list = [None] * initial_mol_count # ä¿æŒé¡ºåº
        
        # ä»kwargsè·å–çº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸º36ï¼Œæˆ–CPUæ ¸å¿ƒæ•°
        default_threads = 36
        try:
            num_threads = kwargs.get('num_preprocessing_threads', default_threads)
            if not isinstance(num_threads, int) or num_threads <= 0:
                num_threads = default_threads
        except:
            num_threads = default_threads
            
        # ç¡®ä¿ä¸è¶…è¿‡CPUæ ¸å¿ƒæ•°å¤ªå¤šï¼Œæˆ–è€…å¯ä»¥è®¾ç½®ä¸€ä¸ªåˆç†çš„ä¸Šé™
        max_threads = multiprocessing.cpu_count() * 2 # ä¾‹å¦‚ï¼Œä¸è¶…è¿‡CPUæ ¸å¿ƒæ•°çš„ä¸¤å€
        num_threads = min(num_threads, max_threads, initial_mol_count if initial_mol_count > 0 else 1)


        if status_container_ui:
            status_container_ui.info(f"TorchANI: å¼€å§‹å¹¶è¡Œé¢„å¤„ç† {initial_mol_count} ä¸ªåˆ†å­ï¼Œä½¿ç”¨ {num_threads} ä¸ªçº¿ç¨‹...")

        # å‡†å¤‡å‚æ•°åˆ—è¡¨
        args_list = [(idx, mol) for idx, mol in enumerate(mols)]
        
        completed_tasks = 0
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
            # ä½¿ç”¨ submit å’Œ as_completed æ¥æ›´å¥½åœ°å¤„ç†è¿›åº¦æ›´æ–°
            future_to_idx = {executor.submit(preprocess_single_mol_for_torchani, arg): arg[0] for arg in args_list}
            
            for future in concurrent.futures.as_completed(future_to_idx):
                original_idx = future_to_idx[future]
                try:
                    result_dict = future.result()
                    # preprocess_single_mol_for_torchani è¿”å›çš„å­—å…¸åŒ…å« original_idx, mol_h, original_species_len, error
                    # æˆ‘ä»¬éœ€è¦å°†å…¶è½¬æ¢ä¸º processed_mols_info æœŸæœ›çš„æ ¼å¼å’Œå†…å®¹
                    processed_mols_info_list[original_idx] = {
                        'mol_h': result_dict.get('mol_h'),
                        'original_species_len': result_dict.get('original_species_len', 0),
                        'error': result_dict.get('error')
                    }
                except Exception as exc:
                    processed_mols_info_list[original_idx] = {
                        'mol_h': None, 
                        'original_species_len': 0, 
                        'error': f'Exception during parallel preprocessing: {str(exc)}'
                    }
                
                completed_tasks += 1
                if progress_bar_ui and initial_mol_count > 0:
                    progress_bar_ui.progress(completed_tasks / initial_mol_count, text=f"TorchANI: é¢„å¤„ç†åˆ†å­ {completed_tasks}/{initial_mol_count}")
        
        if progress_text_ui:
            progress_text_ui.text(f"TorchANI: é¢„å¤„ç†å®Œæˆ {completed_tasks}/{initial_mol_count} ä¸ªåˆ†å­ã€‚")

        # processed_mols_info ç°åœ¨æ˜¯ processed_mols_info_list
        processed_mols_info = processed_mols_info_list

        # Filter out mols that failed pre-processing for the actual processing list
        # ç¡®ä¿è¿™é‡Œçš„ 'mol_h' å’Œ 'original_species_len' é”®ä¸ preprocess_single_mol_for_torchani è¿”å›çš„ä¸€è‡´
        processed_mols_for_optimization = [info['mol_h'] for info in processed_mols_info if info and info.get('mol_h') is not None]
        
        if not processed_mols_for_optimization:
            if status_container_ui:
                status_container_ui.warning("TorchANI: æ‰€æœ‰åˆ†å­é¢„å¤„ç†å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œä¼˜åŒ–ã€‚")
            elif progress_text_ui:
                progress_text_ui.text("TorchANI: æ‰€æœ‰åˆ†å­é¢„å¤„ç†å¤±è´¥ã€‚")
            if progress_bar_ui:
                progress_bar_ui.progress(1.0, text="TorchANI: é¢„å¤„ç†å¤±è´¥")
            return [None] * len(mols) # Return list of Nones matching original input size

        # Dynamic batch size adjustment based on successfully pre-processed mols
        current_total_atoms = sum(info['original_species_len'] for info in processed_mols_info if info['mol_h'] is not None)
        num_valid_mols = len(processed_mols_for_optimization)
        effective_batch_size = batch_size # Directly use the user-provided batch_size
        if status_container_ui:
            status_container_ui.info(f"âš™ï¸ TorchANI: ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ‰¹å¤„ç†å¤§å°: {effective_batch_size} (å…± {num_valid_mols} ä¸ªæœ‰æ•ˆåˆ†å­)")

        num_batches = (num_valid_mols - 1) // effective_batch_size + 1
        results_for_optimized_mols = [None] * num_valid_mols # Results for successfully preprocessed mols

        for i in range(0, num_valid_mols, effective_batch_size):
            batch_mols_h = processed_mols_for_optimization[i : i + effective_batch_size]
            current_batch_num = i // effective_batch_size + 1

            if progress_text_ui:
                progress_text_ui.text(f"TorchANI: å¼€å§‹ä¼˜åŒ–æ‰¹æ¬¡ {current_batch_num}/{num_batches} (å…± {len(batch_mols_h)} åˆ†å­)")
            if progress_bar_ui:
                 # Progress based on batches being submitted to optimization
                 progress_bar_ui.progress( (i + 0.1) / num_valid_mols , text=f"TorchANI: ä¼˜åŒ–æ‰¹æ¬¡ {current_batch_num}/{num_batches}")

            batch_results = []
            
            # å‡†å¤‡æ‰¹é‡æ•°æ®
            batch_species = []
            batch_coordinates = []
            valid_indices = []
            
            for j, mol_h in enumerate(batch_mols_h):
                if mol_h is None:
                    continue
                    
                try:
                    # è·å–åŸå­ä¿¡æ¯
                    species = [atom.GetAtomicNum() for atom in mol_h.GetAtoms()]
                    symbols = [atomic_numbers_to_symbols[num] for num in species]
                    
                    # è·å–åæ ‡
                    conf = mol_h.GetConformer()
                    coordinates = []
                    for k in range(mol_h.GetNumAtoms()):
                        pos = conf.GetAtomPosition(k)
                        coordinates.append([pos.x, pos.y, pos.z])
        
                    batch_species.append(species)
                    batch_coordinates.append(coordinates)
                    valid_indices.append(j)
                except:
                    continue
            
            if not batch_species:
                batch_results = [None] * len(batch_mols_h)
                results.extend(batch_results)
                continue # continue to next batch in the main batch loop
            
            try:
                # ä½¿ç”¨å¡«å……å¤„ç†ä¸åŒå¤§å°çš„åˆ†å­
                max_atoms = max(len(species) for species in batch_species)
                
                # åˆ›å»ºæ‰¹é‡å¼ é‡
                padded_species = []
                padded_coords = []
                
                for species, coords in zip(batch_species, batch_coordinates):
                    # å¡«å……åˆ°æœ€å¤§åŸå­æ•°
                    padded_species_row = species + [0] * (max_atoms - len(species))
                    padded_coords_row = coords + [[0.0, 0.0, 0.0]] * (max_atoms - len(coords))
                    
                    padded_species.append(padded_species_row)
                    padded_coords.append(padded_coords_row)
                
                # è½¬æ¢ä¸ºå¼ é‡
                species_tensor = torch.tensor(padded_species, device=device)
                coordinates_tensor = torch.tensor(padded_coords, device=device, 
                                                dtype=torch.float32, requires_grad=True)
                
                # åˆ›å»ºmaskä»¥å¿½ç•¥å¡«å……éƒ¨åˆ†
                mask = torch.zeros_like(species_tensor, dtype=torch.bool, device=device)
                for idx, original_species in enumerate(batch_species):
                    mask[idx, :len(original_species)] = True
                
                # å°†speciesè½¬æ¢ä¸ºsymbol indices
                batch_species_idx = []
                for species in batch_species:
                    symbols = [atomic_numbers_to_symbols[num] for num in species]
                    species_idx = symbol_to_int(symbols)
                    # å¡«å……åˆ°max_atoms
                    padded_idx = torch.cat([
                        species_idx, 
                        torch.zeros(max_atoms - len(species_idx), dtype=species_idx.dtype)
                    ])
                    batch_species_idx.append(padded_idx)
                
                species_idx_tensor = torch.stack(batch_species_idx).to(device)
                
                # ä¼˜åŒ–å™¨ - ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„å­¦ä¹ ç‡
                optimizer = torch.optim.Adam([coordinates_tensor], lr=learning_rate)
                
                # åˆ›å»ºæ··åˆç²¾åº¦scaler - ä½¿ç”¨å®˜æ–¹æ¨èçš„æ–¹å¼
                scaler = torch.amp.GradScaler("cuda", enabled=use_mixed_precision_torchani and device.type == 'cuda')
                
                # æ‰¹é‡ä¼˜åŒ–
                best_energy = float('inf')
                energy_history = []
                mixed_precision_failed = False
                
                for step in range(optimization_steps):
                    optimizer.zero_grad()
                    
                    # ä½¿ç”¨æ··åˆç²¾åº¦æˆ–å¸¸è§„è®¡ç®— - æŒ‰ç…§PyTorchå®˜æ–¹æ–‡æ¡£çš„æœ€ä½³å®è·µ
                    try:
                        is_amp_really_active = use_mixed_precision_torchani and device.type == 'cuda' and not mixed_precision_failed
                        
                        with torch.autocast(device_type=device.type, dtype=torch.float16, enabled=is_amp_really_active):
                            energies = model((species_idx_tensor, coordinates_tensor)).energies

                            # å¦‚æœæ··åˆç²¾åº¦è¢«ç¦ç”¨ä½†energiesä»æ˜¯float64ï¼Œå¼ºåˆ¶è½¬æ¢ä¸ºfloat32
                            if not is_amp_really_active and energies.dtype == torch.float64:
                                energies = energies.float()
                            
                            mask_any_dim1 = mask.any(dim=1)
                            target_device_for_mask = energies.device
                            mask_float = mask_any_dim1.to(device=target_device_for_mask, dtype=energies.dtype)
                            masked_energies = energies * mask_float
                            total_energy = masked_energies.sum()
                        
                        # æ··åˆç²¾åº¦åå‘ä¼ æ’­
                        scaler.scale(total_energy).backward()
                        
                        # æ¢¯åº¦è£å‰ª - æŒ‰ç…§æ–‡æ¡£å…ˆunscaleå†è£å‰ª
                        if gradient_clipping:
                            scaler.unscale_(optimizer)
                            torch.nn.utils.clip_grad_norm_([coordinates_tensor], max_norm=1.0)
                        
                        scaler.step(optimizer)
                        scaler.update()
                        
                        current_energy = total_energy.float().item()
                        
                    except RuntimeError as mp_error:
                        if "autocast" in str(mp_error).lower() or "half" in str(mp_error).lower() or "dtype" in str(mp_error).lower() or "masked_scatter" in str(mp_error).lower():
                            if use_mixed_precision_torchani and not mixed_precision_failed:
                                # Use status_container_ui for warnings if available
                                warning_msg = f"TorchANI: æ··åˆç²¾åº¦è®¡ç®—å¤±è´¥ (æ‰¹æ¬¡ {current_batch_num}, æ­¥éª¤ {step})ï¼Œå›é€€åˆ°FP32: {str(mp_error)[:100]}..."
                                if status_container_ui:
                                    status_container_ui.warning(warning_msg)
                                else:
                                    st.warning(warning_msg) # Original warning as fallback
                                mixed_precision_failed = True
                                scaler = torch.amp.GradScaler("cuda", enabled=False)
                                optimizer.zero_grad()
                                energies_fp32 = model((species_idx_tensor, coordinates_tensor)).energies.float()
                                mask_val_fp32 = mask.any(dim=1).float().to(device=energies_fp32.device)
                                masked_energies = energies_fp32 * mask_val_fp32
                                total_energy = masked_energies.sum()
                                scaler.scale(total_energy).backward()
                                if gradient_clipping:
                                    scaler.unscale_(optimizer)
                                    torch.nn.utils.clip_grad_norm_([coordinates_tensor], max_norm=1.0)
                                scaler.step(optimizer)
                                scaler.update()
                                current_energy = total_energy.item()
                            else:
                                raise mp_error
                        else:
                            raise mp_error
                    
                    # è®°å½•èƒ½é‡å†å²
                    energy_history.append(current_energy)
                    
                    # è·Ÿè¸ªæœ€ä½³èƒ½é‡
                    if current_energy < best_energy:
                        best_energy = current_energy
                    
                    # Update progress more frequently, e.g., every 10 steps or if it's the last step
                    if step % 10 == 0 or step == optimization_steps - 1:
                        avg_energy = current_energy / len(valid_indices) if valid_indices else 0.0
                        precision_mode = "FP32" if mixed_precision_failed else ("FP16" if use_mixed_precision_torchani and device.type == 'cuda' else "FP32")
                        
                        # Progress text for overall batch step
                        if progress_text_ui:
                            progress_text_ui.text(f"TorchANI: æ‰¹æ¬¡ {current_batch_num}/{num_batches} - ä¼˜åŒ–æ­¥éª¤ {step+1}/{optimization_steps} [E: {avg_energy:.3f} kcal/mol, {precision_mode}]")
                        
                        # Detailed log via status_container if a separate UI element for logs
                        if status_container_ui and (step % 50 == 0 or step == optimization_steps -1): # Less frequent for detailed log line
                             status_container_ui.info(f"TorchANI æ‰¹æ¬¡ {current_batch_num} è¯¦ç»†: æ­¥éª¤ {step+1}, E:{avg_energy:.3f}, {precision_mode}")

                    # æ—©åœæœºåˆ¶ï¼šå¦‚æœèƒ½é‡ä¸å†æ˜¾è‘—æ”¹å–„
                    if step > 50 and len(energy_history) >= 10:
                        recent_improvement = energy_history[-10] - energy_history[-1]
                        if recent_improvement < 1e-6:
                            st.info(f"æ­¥éª¤ {step}: èƒ½é‡æ”¶æ•›ï¼Œæå‰åœæ­¢ä¼˜åŒ–")
                            break
                
                # æå–ä¼˜åŒ–åçš„åæ ‡å¹¶æ›´æ–°åˆ†å­
                # ç¡®ä¿åæ ‡å¼ é‡è½¬æ¢ä¸ºæ­£ç¡®çš„æ•°æ®ç±»å‹
                with torch.no_grad():
                    optimized_coords = coordinates_tensor.detach().float().cpu().numpy()
                
                batch_results = [None] * len(batch_mols_h)
                for idx, (mol_h, original_species) in enumerate(zip([batch_mols_h[vi] for vi in valid_indices], batch_species)):
                    if mol_h is None:
                        continue
                        
                    try:
                        # æ›´æ–°åæ ‡
                        conf = mol_h.GetConformer()
                        coords = optimized_coords[idx][:len(original_species)]  # åªå–çœŸå®åŸå­çš„åæ ‡
                        
                        for atom_idx, pos in enumerate(coords):
                            conf.SetAtomPosition(atom_idx, (float(pos[0]), float(pos[1]), float(pos[2])))
        
        # ç§»é™¤æ°¢åŸå­
                        mol_final = Chem.RemoveHs(mol_h)
                        batch_results[valid_indices[idx]] = mol_final
                    except Exception as e:
                        st.warning(f"æ›´æ–°åˆ†å­åæ ‡å¤±è´¥: {str(e)}")
                        batch_results[valid_indices[idx]] = None
                
                results.extend(batch_results)
                
                # æ˜¾ç¤ºæ‰¹æ¬¡ä¼˜åŒ–ç»“æœ
                final_avg_energy = best_energy / len(valid_indices) if valid_indices else 0
                # Use status_container_ui for batch completion
                if status_container_ui:
                    status_container_ui.info(f"âœ… TorchANI: æ‰¹æ¬¡ {current_batch_num}/{num_batches} å®Œæˆ, æœ€ä½³å¹³å‡èƒ½é‡: {final_avg_energy:.4f}")
                elif progress_text_ui: # Fallback
                    progress_text_ui.text(f"TorchANI: æ‰¹æ¬¡ {current_batch_num}/{num_batches} å®Œæˆ.")
                
                # Update progress bar after each batch is fully processed.
                if progress_bar_ui:
                    progress_bar_ui.progress( min(1.0, (i + len(batch_mols_h)) / num_valid_mols) , text=f"TorchANI: æ‰¹æ¬¡ {current_batch_num} å®Œæˆ")
                
            except Exception as e: # Catch exception for this specific batch processing
                if status_container_ui:
                    status_container_ui.warning(f"TorchANI: æ‰¹æ¬¡ {current_batch_num} ä¼˜åŒ–å¤±è´¥: {str(e)}")
                elif progress_text_ui:
                    progress_text_ui.text(f"TorchANI: æ‰¹æ¬¡ {current_batch_num} å¤±è´¥.")
                # Fill results for this batch with None
                for k_idx in range(len(batch_mols_h)):
                    if (i + k_idx) < len(results_for_optimized_mols):
                         results_for_optimized_mols[i + k_idx] = None
                
                # æ¸…ç†GPUå†…å­˜
                if device.type == 'cuda':
                    torch.cuda.empty_cache()
            
        # Reconstruct the final results list to match the original mols input size and order
        final_results_ordered = [None] * len(mols)
        opt_mol_idx = 0
        for original_idx in range(len(mols)):
            if processed_mols_info[original_idx]['mol_h'] is not None and opt_mol_idx < len(results_for_optimized_mols):
                final_results_ordered[original_idx] = results_for_optimized_mols[opt_mol_idx]
                opt_mol_idx += 1
            # else: it remains None (due to pre-processing failure or if something went wrong with indexing)

        total_time = time.time() - start_time
        success_count = sum(1 for r in final_results_ordered if r is not None)
        
        # The calling function batch_generate_3d_conformers will print the final success message.
        # Here, we just ensure the progress UI is finalized for this specific function's scope.
        if progress_bar_ui:
            progress_bar_ui.progress(1.0, text=f"TorchANI ä¼˜åŒ–å¤„ç†å®Œæ¯• ({success_count}/{len(mols)} æˆåŠŸ)")
        if progress_text_ui:
            progress_text_ui.text(f"TorchANI ä¼˜åŒ–å¤„ç†å®Œæ¯•: {success_count}/{len(mols)} æˆåŠŸ, ç”¨æ—¶ {total_time:.2f}s")
        if status_container_ui: # Clear or set a final message for the dedicated status line
            status_container_ui.info(f"TorchANI ä¼˜åŒ–æµç¨‹ç»“æŸ. {success_count} åˆ†å­æˆåŠŸä¼˜åŒ–ã€‚")

        return final_results_ordered
        
    except Exception as e:
        if status_container_ui:
            status_container_ui.error(f"TorchANI æ‰¹é‡ä¼˜åŒ–ä¸»ç¨‹åºå¤±è´¥: {str(e)}")
        elif progress_text_ui:
            progress_text_ui.text(f"TorchANI ä¼˜åŒ–ä¸¥é‡é”™è¯¯: {str(e)}")
        if progress_bar_ui:
            progress_bar_ui.progress(1.0, text="TorchANI ä¼˜åŒ–å‡ºé”™!")
        return [None] * len(mols)

def generate_3d_conformer_torchani(mol, model_name='ANI2x', optimization_steps=100, device=None):
    """ä½¿ç”¨TorchANIç”Ÿæˆ3Dæ„è±¡ - å•åˆ†å­ç‰ˆæœ¬ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    if not HAS_TORCHANI:
        return None

    # ä½¿ç”¨æ‰¹é‡ç‰ˆæœ¬å¤„ç†å•ä¸ªåˆ†å­ä»¥è·å¾—ä¼˜åŒ–æ•ˆæœ
    result = generate_3d_conformer_torchani_optimized([mol], model_name, optimization_steps, device, batch_size=1)
    return result[0] if result else None

def generate_3d_conformer_deepchem(mol, use_gpu=True, model_type='mpnn', force_field='mmff94s'):
    """ä½¿ç”¨DeepChemç”Ÿæˆ3Dæ„è±¡ï¼Œæ”¯æŒGPUåŠ é€Ÿ"""
    if not HAS_DEEPCHEM:
        return None
        
    try:
        # æ·»åŠ æ°¢åŸå­
        mol_with_h = Chem.AddHs(mol)
        
        # ä½¿ç”¨ETKDGç”Ÿæˆåˆå§‹æ„è±¡
        embed_result = AllChem.EmbedMolecule(mol_with_h, AllChem.ETKDGv3())
        if embed_result != 0:
            st.warning("ETKDGåµŒå…¥å¤±è´¥ï¼Œå°è¯•åŸºæœ¬åµŒå…¥")
            embed_result = AllChem.EmbedMolecule(mol_with_h)
            if embed_result != 0:
                return None
        
        # è®¾ç½®GPU/CPUè®¾å¤‡
        if use_gpu and tf.config.list_physical_devices('GPU'):
            with tf.device('/GPU:0'):
                # Initialize conformer generator with better parameters
                try:
                    conf_gen = dc.utils.conformers.ConformerGenerator(
                        max_conformers=1,
                            force_field=force_field,
                        pool_multiplier=1,
                            optimization_steps=200
                    )
                
                # Generate conformers
                    mol_optimized = conf_gen.generate_conformers(mol_with_h)
                    
                except Exception as gpu_error:
                    st.warning(f"GPUä¼˜åŒ–å¤±è´¥: {str(gpu_error)}ï¼Œå°è¯•CPUç‰ˆæœ¬")
                    # Fallback to CPU version
                    conf_gen = dc.utils.conformers.ConformerGenerator(
                        max_conformers=1,
                        force_field=force_field,
                        pool_multiplier=1
                    )
                    mol_optimized = conf_gen.generate_conformers(mol_with_h)
        else:
            # Fallback to CPU version
            conf_gen = dc.utils.conformers.ConformerGenerator(
                max_conformers=1,
                force_field=force_field,
                pool_multiplier=1
            )
            mol_optimized = conf_gen.generate_conformers(mol_with_h)
        
        # ç§»é™¤æ°¢åŸå­
        if mol_optimized is not None:
            mol_optimized = Chem.RemoveHs(mol_optimized)
            
        return mol_optimized
        
    except Exception as e:
        # Add the original model_type to the warning for context
        st.warning(f"DeepChemæ„è±¡ç”Ÿæˆå¤±è´¥(model_type='{model_type}', force_field='{force_field}'): {str(e)}")
        return None

def generate_3d_conformer_clara(mol, force_field='MMFF94s', precision='mixed', num_conformers=1, energy_threshold=1.0, optimization_steps=500):
    """ä½¿ç”¨NVIDIA Claraç”Ÿæˆ3Dæ„è±¡"""
    if not HAS_CLARA:
        return None
        
    try:
        # æ£€æŸ¥åˆ†å­å¤§å°ï¼ŒClaraé€‚åˆå„ç§å¤§å°çš„åˆ†å­
        num_atoms = mol.GetNumAtoms()
        if num_atoms > 200:
            st.warning(f"åˆ†å­è¾ƒå¤§ï¼ˆ{num_atoms}åŸå­ï¼‰ï¼ŒClaraå¤„ç†å¯èƒ½è¾ƒæ…¢")
        
        # æ·»åŠ æ°¢åŸå­
        mol_with_h = Chem.AddHs(mol)
        
        # ä½¿ç”¨ETKDGç”Ÿæˆåˆå§‹æ„è±¡
        embed_result = AllChem.EmbedMolecule(mol_with_h, AllChem.ETKDGv3())
        if embed_result != 0:
            st.warning("ETKDGåµŒå…¥å¤±è´¥ï¼Œå°è¯•åŸºæœ¬åµŒå…¥")
            embed_result = AllChem.EmbedMolecule(mol_with_h)
            if embed_result != 0:
                return None
        
        # è½¬æ¢ä¸ºClaraåˆ†å­æ ¼å¼
        clara_molecule = clara_mol.Molecule.from_rdkit(mol_with_h)
        
        # åˆ›å»ºæ„è±¡ç”Ÿæˆå™¨
        conf_gen = clara_conf.ConformerGenerator(
            num_conformers=num_conformers,
            use_gpu=True,
            energy_minimization=True,
            force_field=force_field,
            precision=precision,
            energy_threshold=energy_threshold,
            max_iterations=optimization_steps
        )
        
        # ç”Ÿæˆæ„è±¡
        conformers = conf_gen.generate(clara_molecule)
        
        if not conformers:
            st.warning("Claraæœªç”Ÿæˆæœ‰æ•ˆæ„è±¡")
            return None
        
        # è·å–æœ€ä½èƒ½é‡æ„è±¡
        best_conf = min(conformers, key=lambda x: x.energy)
        
        # è½¬æ¢å›RDKitåˆ†å­
        mol_with_conf = best_conf.to_rdkit()
        
        # ç§»é™¤æ°¢åŸå­
        mol_optimized = Chem.RemoveHs(mol_with_conf)
        
        st.success(f"Claraæ„è±¡ç”ŸæˆæˆåŠŸï¼Œèƒ½é‡: {best_conf.energy:.3f} kcal/mol")
        return mol_optimized
        
    except Exception as e:
        st.warning(f"NVIDIA Claraæ„è±¡ç”Ÿæˆå¤±è´¥: {str(e)}")
        return None

def generate_3d_conformer_multi(mol, backend='auto', **kwargs):
    """å¤šåç«¯3Dæ„è±¡ç”Ÿæˆå™¨"""
    if mol is None:
        return None
        
    if backend == 'auto':
        # æ ¹æ®åˆ†å­å¤§å°å’Œå¯ç”¨èµ„æºè‡ªåŠ¨é€‰æ‹©åç«¯
        num_atoms = mol.GetNumAtoms()
        gpu_available = torch.cuda.is_available()
        
        # æ£€æŸ¥åç«¯å¯ç”¨æ€§å¹¶è€ƒè™‘GPUæ”¯æŒ
        if HAS_CLARA and gpu_available:
            backend = 'clara'  # ä¼˜å…ˆä½¿ç”¨Claraï¼ˆNVIDIAé«˜æ€§èƒ½ï¼‰
        elif num_atoms <= 50 and HAS_TORCHANI and (gpu_available or HAS_TORCHANI_CUDA):
            backend = 'torchani'  # å°åˆ†å­ä½¿ç”¨TorchANI
        elif num_atoms <= 100 and HAS_DEEPCHEM and HAS_DEEPCHEM_GPU:
            backend = 'deepchem'  # ä¸­ç­‰åˆ†å­ä½¿ç”¨DeepChem
        elif HAS_CLARA and gpu_available:
            backend = 'clara'  # å¤§åˆ†å­ä½¿ç”¨Clara
        else:
            backend = 'rdkit'  # é»˜è®¤å›é€€åˆ°RDKit
            
        st.info(f"è‡ªåŠ¨é€‰æ‹©æ„è±¡ç”Ÿæˆåç«¯: {backend}")
    
    try:
        if backend == 'clara' and HAS_CLARA:
            st.info("ä½¿ç”¨NVIDIA Claraç”Ÿæˆæ„è±¡...")
            try:
                return generate_3d_conformer_clara(
                    mol,
                    force_field=kwargs.get('force_field', 'MMFF94s'),
                    precision=kwargs.get('precision', 'mixed'),
                    num_conformers=kwargs.get('num_conformers', 1),
                    energy_threshold=kwargs.get('energy_threshold', 1.0),
                    optimization_steps=kwargs.get('optimization_steps', 500)
                )
            except Exception as e:
                st.warning(f"NVIDIA Claraæ„è±¡ç”Ÿæˆå¤±è´¥: {str(e)}ï¼Œå›é€€åˆ°RDKit")
                return generate_3d_conformer(mol, **kwargs)
                
        elif backend == 'torchani' and HAS_TORCHANI:
            st.info("ä½¿ç”¨TorchANIç”Ÿæˆæ„è±¡...")
            try:
                # ç¡®å®šè®¾å¤‡
                device = None
                if torch.cuda.is_available() and HAS_TORCHANI_CUDA:
                    device = torch.device('cuda')
                else:
                    device = torch.device('cpu')
                    st.info("TorchANIä½¿ç”¨CPUè®¡ç®—")
                
                return generate_3d_conformer_torchani(
                    mol,
                    model_name=kwargs.get('torchani_model', 'ANI2x'),
                    optimization_steps=kwargs.get('optimization_steps', 100),
                    device=device
                )
            except Exception as e:
                st.warning(f"TorchANIæ„è±¡ç”Ÿæˆå¤±è´¥: {str(e)}ï¼Œå›é€€åˆ°RDKit")
                return generate_3d_conformer(mol, **kwargs)
                
        elif backend == 'deepchem' and HAS_DEEPCHEM:
            st.info("ä½¿ç”¨DeepChemç”Ÿæˆæ„è±¡...")
            try:
                return generate_3d_conformer_deepchem(
                    mol,
                    use_gpu=HAS_DEEPCHEM_GPU and kwargs.get('use_gpu', True),
                    model_type=kwargs.get('model_type', 'mpnn'),
                    force_field=kwargs.get('force_field', 'mmff94s')
                )
            except Exception as e:
                st.warning(f"DeepChemæ„è±¡ç”Ÿæˆå¤±è´¥: {str(e)}ï¼Œå›é€€åˆ°RDKit")
                return generate_3d_conformer(mol, **kwargs)
                
        else:
            # ä½¿ç”¨RDKitä½œä¸ºé»˜è®¤å’Œå›é€€é€‰é¡¹
            st.info("ä½¿ç”¨RDKitç”Ÿæˆæ„è±¡...")
            return generate_3d_conformer(
                mol,
                max_attempts=kwargs.get('max_attempts', 50),
                use_mmff=kwargs.get('use_mmff', True),
                energy_iter=kwargs.get('energy_iter', 200),
                add_hydrogens=kwargs.get('add_hydrogens', True)
            )
            
    except Exception as e:
        st.error(f"æ„è±¡ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {str(e)}")
        # å°è¯•æœ€åŸºæœ¬çš„æ„è±¡ç”Ÿæˆä½œä¸ºæœ€åçš„å›é€€é€‰é¡¹
        try:
            st.warning("å°è¯•ä½¿ç”¨æœ€åŸºæœ¬çš„RDKitæ„è±¡ç”Ÿæˆ...")
            mol_copy = Chem.Mol(mol)
            Chem.AllChem.EmbedMolecule(mol_copy)
            return mol_copy
        except:
            st.error("æ„è±¡ç”Ÿæˆå®Œå…¨å¤±è´¥")
            return None

# Modified function definition to accept progress_bar
def batch_generate_3d_conformers(mols, progress_bar, status_container, progress_text, backend='auto', batch_size=None, **kwargs):
    """æ‰¹é‡ç”Ÿæˆ3Dæ„è±¡ - ä¼˜åŒ–ç‰ˆæœ¬"""
    if not mols:
        return []
    
    # è‡ªåŠ¨é€‰æ‹©æ‰¹å¤„ç†å¤§å°
    if batch_size is None:
        if backend == 'clara':
            batch_size = min(10, len(mols))  # è¾ƒå°æ‰¹æ¬¡ä»¥å‡å°‘GPUå†…å­˜å‹åŠ›
        elif backend == 'torchani':
            batch_size = min(32, len(mols))  # TorchANIä¼˜åŒ–æ‰¹å¤„ç†
        elif backend == 'deepchem':
            batch_size = min(64, len(mols))
        else:
            batch_size = min(100, len(mols))
    
    start_time = time.time()
    status_container.info(f"å¼€å§‹ç”Ÿæˆæ„è±¡ï¼Œå…± {len(mols)} ä¸ªåˆ†å­ï¼Œä½¿ç”¨ {backend} åç«¯")
    
    # TorchANIç‰¹æ®Šæ‰¹å¤„ç†ä¼˜åŒ–
    if backend == 'torchani' and HAS_TORCHANI:
        status_container.info("ğŸš€ ä½¿ç”¨TorchANIä¼˜åŒ–æ‰¹å¤„ç†æ¨¡å¼")
        progress_text.text("æ­£åœ¨è¿›è¡ŒTorchANIæ‰¹é‡ä¼˜åŒ–...")
        
        try:
            # ç¡®å®šè®¾å¤‡å’Œå‚æ•°
            device = None
            if torch.cuda.is_available() and HAS_TORCHANI_CUDA:
                device = torch.device('cuda')
            else:
                device = torch.device('cpu')
                st.info("TorchANIä½¿ç”¨CPUè®¡ç®—")
            
            # è·å–ç”¨æˆ·è®¾ç½®çš„å‚æ•°
            optimization_params = {
                'model_name': kwargs.get('torchani_model', 'ANI2x'),
                'optimization_steps': kwargs.get('optimization_steps', 100),
                'device': device,
                'batch_size': kwargs.get('torchani_batch_size', batch_size),
                'learning_rate': kwargs.get('learning_rate', 0.01),
                'use_mixed_precision_torchani': kwargs.get('use_mixed_precision_torchani', True),
                'max_atoms_per_batch': kwargs.get('max_atoms_per_batch', 5000),
                'gradient_clipping': kwargs.get('gradient_clipping', True)
            }
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨ä¼˜åŒ–æ¨¡å¼
            if not kwargs.get('use_torchani_optimization', True):
                st.warning("âš ï¸ æ‰¹é‡ä¼˜åŒ–æ¨¡å¼å·²ç¦ç”¨ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ¨¡å¼")
                # è¿™é‡Œåº”è¯¥æœ‰ä¸€ä¸ªæ¸…æ™°çš„å›é€€è·¯å¾„åˆ°é€ä¸ªå¤„ç†é€»è¾‘ï¼Œç›®å‰å®ƒä¼šç›´æ¥è·³åˆ°å‡½æ•°æœ«å°¾çš„é€ä¸ªå¤„ç†
            else:
                # è°ƒç”¨ä¼˜åŒ–çš„æ‰¹é‡å¤„ç†å‡½æ•°
                # å°†Streamlit UIå…ƒç´ ä¼ é€’ç»™ä¼˜åŒ–å‡½æ•°
                optimization_params['progress_bar_ui'] = progress_bar
                optimization_params['progress_text_ui'] = progress_text
                optimization_params['status_container_ui'] = status_container

                results = generate_3d_conformer_torchani_optimized(mols, **optimization_params)
                
                # generate_3d_conformer_torchani_optimized å†…éƒ¨ä¼šå¤„ç†å…¶ä½œç”¨åŸŸå†…çš„æœ€ç»ˆè¿›åº¦æ›´æ–°
                # è¿™é‡Œä¸»è¦å¤„ç†è°ƒç”¨ä¼˜åŒ–å‡½æ•°åçš„æ€»ä½“çŠ¶æ€å’Œç»Ÿè®¡ä¿¡æ¯

                total_time = time.time() - start_time # Recalculate total time based on this function's scope
                success_count = sum(1 for r in results if r is not None)
                success_rate = (success_count / len(mols)) * 100 if len(mols) > 0 else 0
                
                status_container.success(
                    f"ğŸ¯ TorchANIæ‰¹é‡æ„è±¡ç”Ÿæˆå®Œæˆ! æˆåŠŸç‡: {success_rate:.1f}% ({success_count}/{len(mols)})"
                    f"ï¼Œæ€»è€—æ—¶: {total_time:.1f}ç§’"
                    f"ï¼Œå¹³å‡æ¯åˆ†å­: {total_time/len(mols) if len(mols) > 0 else 0:.2f}ç§’"
                )
                
                if progress_bar: # Final confirmation of progress bar
                    progress_bar.progress(1.0, text=f"TorchANIå¤„ç†å®Œæˆ ({success_count}/{len(mols)})")
                if progress_text:
                    progress_text.text(f"TorchANIå¤„ç†å®Œæˆ. {success_count} ä¸ªåˆ†å­æˆåŠŸã€‚")

                # æ˜¾ç¤ºæ€§èƒ½æå‡ä¿¡æ¯
                if success_count > 0:
                    molecules_per_second = success_count / total_time
                    st.info(f"âš¡ å¤„ç†é€Ÿåº¦: {molecules_per_second:.2f} åˆ†å­/ç§’")
                    
                    # ä¼°ç®—ç›¸æ¯”åŸç‰ˆæœ¬çš„æ€§èƒ½æå‡
                    estimated_old_time = success_count * 10  # å‡è®¾åŸç‰ˆæœ¬æ¯ä¸ªåˆ†å­10ç§’
                    speedup = estimated_old_time / total_time if total_time > 0 else 1
                    if speedup > 2:
                        st.success(f"ğŸ¯ ç›¸æ¯”é€ä¸ªå¤„ç†ä¼°è®¡åŠ é€Ÿ: {speedup:.1f}x")
                    
                    # æ˜¾ç¤ºä½¿ç”¨çš„ä¼˜åŒ–å‚æ•°
                    with st.expander("ğŸ”§ ä½¿ç”¨çš„ä¼˜åŒ–å‚æ•°", expanded=False):
                        st.json({
                            "æ¨¡å‹": optimization_params['model_name'],
                            "æ‰¹å¤„ç†å¤§å°": optimization_params['batch_size'],
                            "ä¼˜åŒ–æ­¥æ•°": optimization_params['optimization_steps'],
                            "å­¦ä¹ ç‡": optimization_params['learning_rate'],
                            "æ··åˆç²¾åº¦": optimization_params['use_mixed_precision_torchani'],
                            "æ¢¯åº¦è£å‰ª": optimization_params['gradient_clipping'],
                            "è®¾å¤‡": str(optimization_params['device'])
                        })
                
                return results
                
        except Exception as e:
            st.error(f"TorchANIæ‰¹é‡ä¼˜åŒ–å¤±è´¥: {str(e)}")
            st.warning("å›é€€åˆ°åŸæœ‰çš„é€ä¸ªå¤„ç†æ¨¡å¼...")
            # ç»§ç»­ä½¿ç”¨åŸæœ‰é€»è¾‘ä½œä¸ºå›é€€
    
    # åŸæœ‰çš„é€ä¸ªå¤„ç†é€»è¾‘ï¼ˆå…¶ä»–åç«¯æˆ–TorchANIå¤±è´¥æ—¶çš„å›é€€ï¼‰
    results = []
    failures = 0
    
    for i in range(0, len(mols), batch_size):
        batch = mols[i:min(i+batch_size, len(mols))]
        batch_size_actual = len(batch)
        
        # æ›´æ–°çŠ¶æ€
        batch_start_time = time.time()
        progress_text.text(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(mols)-1)//batch_size + 1}ï¼Œåˆ†å­ {i+1}-{min(i+batch_size_actual, len(mols))}/{len(mols)}")
        
        # ä½¿ç”¨å¤šåç«¯ç”Ÿæˆæ„è±¡
        batch_results = []
        batch_failures = 0
        
        for j, mol in enumerate(batch):
            # æ›´æ–°è¿›åº¦ (using the passed progress_bar)
            progress = (i + j) / len(mols)
            if progress_bar: # Check if progress_bar is not None
                progress_bar.progress(progress, text=f"å¤„ç†åˆ†å­ {i+j+1}/{len(mols)}")
            
            # ç”Ÿæˆæ„è±¡
            mol_3d = generate_3d_conformer_multi(mol, backend=backend, **kwargs)
            
            if mol_3d is None:
                batch_failures += 1
                failures += 1
            
            batch_results.append(mol_3d)
            
            # æ˜¾ç¤ºå½“å‰å¤„ç†é€Ÿåº¦
            elapsed = time.time() - start_time
            molecules_per_second = (i + j + 1) / elapsed if elapsed > 0 else 0
            remaining = (len(mols) - (i + j + 1)) / molecules_per_second if molecules_per_second > 0 else 0
            
            if (j + 1) % max(1, batch_size_actual // 5) == 0 or j == batch_size_actual - 1:
                progress_text.text(
                    f"æ‰¹æ¬¡ {i//batch_size + 1}/{(len(mols)-1)//batch_size + 1}"
                    f"ï¼Œåˆ†å­ {i+j+1}/{len(mols)}"
                    f"ï¼Œå¤„ç†é€Ÿåº¦: {molecules_per_second:.2f} åˆ†å­/ç§’"
                    f"ï¼Œå‰©ä½™æ—¶é—´: {int(remaining//60)}åˆ†{int(remaining%60)}ç§’"
                )
        
        results.extend(batch_results)
        
        # æ˜¾ç¤ºæ‰¹æ¬¡çŠ¶æ€
        batch_time = time.time() - batch_start_time
        status_container.info(
            f"å®Œæˆæ‰¹æ¬¡ {i//batch_size + 1}/{(len(mols)-1)//batch_size + 1}"
            f"ï¼Œæ„è±¡ç”ŸæˆæˆåŠŸç‡: {(batch_size_actual-batch_failures)/batch_size_actual*100:.1f}%"
            f"ï¼Œæ‰¹æ¬¡è€—æ—¶: {batch_time:.1f}ç§’"
            f"ï¼Œæ¯åˆ†å­: {batch_time/batch_size_actual:.2f}ç§’"
        )
        
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    total_time = time.time() - start_time
    success_rate = (len(mols) - failures) / len(mols) * 100 if len(mols) > 0 else 0
    
    status_container.success(
        f"æ„è±¡ç”Ÿæˆå®Œæˆ! æˆåŠŸç‡: {success_rate:.1f}% ({len(mols)-failures}/{len(mols)})"
        f"ï¼Œæ€»è€—æ—¶: {total_time:.1f}ç§’"
        f"ï¼Œå¹³å‡æ¯åˆ†å­: {total_time/len(mols):.2f}ç§’"
    )
    
    if failures > 0:
        st.warning(f"è­¦å‘Š: {failures}ä¸ªåˆ†å­çš„æ„è±¡ç”Ÿæˆå¤±è´¥")
    
    # Reset progress bar to 0 after completion
    if progress_bar:
        progress_bar.progress(1.0) # Set to 100%

    return results

def compute_usr_descriptor(mol):
    """è®¡ç®—USR (Ultrafast Shape Recognition) æè¿°ç¬¦"""
    if mol is None or mol.GetNumConformers() == 0:
        return None
    
    conf = mol.GetConformer()
    if not conf.Is3D():
        return None
    
    try:
        # æå–æ‰€æœ‰åŸå­åæ ‡
        coords = []
        for i in range(mol.GetNumAtoms()):
            pos = conf.GetAtomPosition(i)
            coords.append(np.array([pos.x, pos.y, pos.z]))
        coords = np.array(coords)
        
        # 1) è´¨å¿ƒ
        centroid = coords.mean(axis=0)
        # 2) ä¸è´¨å¿ƒæœ€è¿œçš„ç‚¹P1
        dists_centroid = np.linalg.norm(coords - centroid, axis=1)
        idx_p1 = np.argmax(dists_centroid)
        p1 = coords[idx_p1]
        # 3) ä¸p1æœ€è¿œçš„ç‚¹p2
        dists_p1 = np.linalg.norm(coords - p1, axis=1)
        idx_p2 = np.argmax(dists_p1)
        p2 = coords[idx_p2]
        # 4) ä¸p2æœ€è¿œçš„ç‚¹p3
        dists_p2 = np.linalg.norm(coords - p2, axis=1)
        idx_p3 = np.argmax(dists_p2)
        p3 = coords[idx_p3]
        
        # å››ä¸ªå‚è€ƒç‚¹
        P = [centroid, p1, p2, p3]
        
        # è®¡ç®—åˆ°è¿™4ç‚¹çš„è·ç¦»åˆ†å¸ƒ
        descriptor = []
        for ref_pt in P:
            dists = np.linalg.norm(coords - ref_pt, axis=1)
            d_mean = dists.mean()
            d_std = dists.std()
            d_min = dists.min()
            d_max = dists.max()
            descriptor.extend([d_mean, d_std, d_min, d_max])
        
        return np.array(descriptor)
    except:
        return None

def compute_usrcat_descriptor(mol):
    """è®¡ç®—USRCAT (USR-CAT) æè¿°ç¬¦"""
    if mol is None or mol.GetNumConformers() == 0:
        return None
    
    conf = mol.GetConformer()
    if not conf.Is3D():
        return None
    
    try:
        # å®šä¹‰åŸå­ç±»å‹
        atom_types = {
            'all': lambda a: True,
            'hydrophobic': lambda a: a.GetSymbol() in ('C', 'Cl', 'Br', 'I'),
            'aromatic': lambda a: a.GetIsAromatic(),
            'acceptor': lambda a: a.GetSymbol() in ('N', 'O', 'F') and not a.GetIsAromatic(),
            'donor': lambda a: (a.GetSymbol() in ('N', 'O') and 
                              sum(1 for n in a.GetNeighbors() if n.GetSymbol() == 'H') > 0)
        }
        
        descriptors = []
        
        for atom_type, type_func in atom_types.items():
            # è·å–ç‰¹å®šç±»å‹çš„åŸå­åæ ‡
            coords = []
            for i in range(mol.GetNumAtoms()):
                atom = mol.GetAtomWithIdx(i)
                if type_func(atom):
                    pos = conf.GetAtomPosition(i)
                    coords.append(np.array([pos.x, pos.y, pos.z]))
            
            if not coords:
                descriptors.extend([0] * 12)  # å¦‚æœæ²¡æœ‰è¯¥ç±»å‹çš„åŸå­ï¼Œå¡«å……0
                continue
                
            coords = np.array(coords)
            
            # è®¡ç®—å››ä¸ªå‚è€ƒç‚¹
            centroid = coords.mean(axis=0)
            dists_centroid = np.linalg.norm(coords - centroid, axis=1)
            idx_p1 = np.argmax(dists_centroid)
            p1 = coords[idx_p1]
            
            dists_p1 = np.linalg.norm(coords - p1, axis=1)
            idx_p2 = np.argmax(dists_p1)
            p2 = coords[idx_p2]
            
            dists_p2 = np.linalg.norm(coords - p2, axis=1)
            idx_p3 = np.argmax(dists_p2)
            p3 = coords[idx_p3]
            
            # è®¡ç®—åˆ°å››ä¸ªå‚è€ƒç‚¹çš„è·ç¦»çŸ©
            for ref_pt in [centroid, p1, p2, p3]:
                dists = np.linalg.norm(coords - ref_pt, axis=1)
                descriptors.extend([
                    dists.mean(),
                    dists.std(),
                    dists.min(),
                    dists.max()
                ])
        
        return np.array(descriptors)
    except:
        return None

def compute_pmi_ratios(mol):
    """è®¡ç®—PMIæ¯”ç‡"""
    if mol is None or mol.GetNumConformers() == 0:
        return None
    try:
        inertia = rdMolDescriptors.CalcPMIValues(mol)
        I1, I2, I3 = inertia
        if abs(I3) < 1e-8:
            return None
        return (I1/I3, I2/I3)
    except:
        return None

def calc_nearest_neighbor_distance(descriptors, cuda_available=False):
    """è®¡ç®—æœ€è¿‘é‚»è·ç¦»ï¼Œæ”¯æŒGPUåŠ é€Ÿ"""
    n = len(descriptors)
    if n < 2:
        return 0.0, 0.0
    
    # ä½¿ç”¨GPUåŠ é€Ÿ
    if cuda_available and torch.cuda.is_available():
        try:
            # è®°å½•GPUå†…å­˜ä½¿ç”¨å‰çŠ¶æ€
            mem_before = torch.cuda.memory_allocated() / 1024**2
            
            # è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶ç§»è‡³GPU
            desc_tensor = torch.tensor(descriptors, dtype=torch.float32).cuda()
            
            # è®¡ç®—æ¬§æ°è·ç¦»çŸ©é˜µ
            distances = torch.zeros((n, n), dtype=torch.float32).cuda()
            batch_size = 128  # æ‰¹å¤„ç†å¤§å°ï¼Œé¿å…æ˜¾å­˜æº¢å‡º
            
            for i in range(0, n, batch_size):
                end_i = min(i + batch_size, n)
                chunk_i = desc_tensor[i:end_i]
                
                for j in range(0, n, batch_size):
                    end_j = min(j + batch_size, n)
                    chunk_j = desc_tensor[j:end_j]
                    
                    # è®¡ç®—æ‰¹æ¬¡ä¹‹é—´çš„è·ç¦»
                    dist_chunk = torch.cdist(chunk_i, chunk_j)
                    distances[i:end_i, j:end_j] = dist_chunk
            
            # å°†è‡ªèº«è·ç¦»è®¾ä¸ºæ— ç©·å¤§
            eye_mask = torch.eye(n, dtype=torch.bool).cuda()
            distances[eye_mask] = float('inf')
            
            # æ¯ä¸ªåˆ†å­çš„æœ€è¿‘é‚»è·ç¦»
            min_dists, _ = torch.min(distances, dim=1)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            mean_d = min_dists.mean().item()
            std_d = min_dists.std().item()
            
            # è®°å½•GPUå†…å­˜ä½¿ç”¨åçŠ¶æ€
            mem_after = torch.cuda.memory_allocated() / 1024**2
            mem_diff = mem_after - mem_before
            
            # è®°å½•GPUä½¿ç”¨æƒ…å†µ
            st.session_state.gpu_nn_mem_usage = mem_diff
            st.session_state.gpu_nn_calls = st.session_state.get('gpu_nn_calls', 0) + 1
            
            # æ¸…ç†GPUå†…å­˜
            torch.cuda.empty_cache()
            
            return mean_d, std_d
        
        except Exception as e:
            st.warning(f"GPUåŠ é€Ÿæœ€è¿‘é‚»è®¡ç®—å¤±è´¥ï¼Œå°†ä½¿ç”¨CPU: {str(e)}")
    
    # ä½¿ç”¨CPUè®¡ç®—
    distances = []
    for i in range(n):
        d_i = float('inf')
        for j in range(n):
            if i == j:
                continue
            dist = np.linalg.norm(descriptors[i] - descriptors[j])
            if dist < d_i:
                d_i = dist
        distances.append(d_i)
    
    mean_d = np.mean(distances)
    std_d = np.std(distances)
    return mean_d, std_d

def perform_dimensionality_reduction(descriptors, method="t-SNE", cuda_available=False, **kwargs):
    """ä½¿ç”¨GPUåŠ é€Ÿçš„é™ç»´åˆ†æ"""
    if len(descriptors) == 0:
        return np.array([])
    
    # è®°å½•åˆå§‹çŠ¶æ€
    gpu_used = False
    start_time = time.time()
        
    if method == "t-SNE":
        if cuda_available and 'cp' in globals() and 'cuTSNE' in globals():
            try:
                # è®°å½•GPUå†…å­˜ä½¿ç”¨å‰çŠ¶æ€(å¦‚æœå¯ç”¨)
                if torch.cuda.is_available():
                    mem_before = torch.cuda.memory_allocated() / 1024**2
                
                tsne = cuTSNE(n_components=2, **kwargs)
                coords = tsne.fit_transform(cp.array(descriptors))
                
                # è®°å½•GPUä½¿ç”¨æƒ…å†µ
                if torch.cuda.is_available():
                    mem_after = torch.cuda.memory_allocated() / 1024**2
                    mem_diff = abs(mem_after - mem_before)  # cuMLå¯èƒ½ä½¿ç”¨ä¸åŒçš„å†…å­˜ç®¡ç†
                    st.session_state.gpu_tsne_mem = mem_diff
                
                gpu_used = True
                st.session_state.gpu_tsne_calls = st.session_state.get('gpu_tsne_calls', 0) + 1
                
                result = cp.asnumpy(coords)
            except Exception as e:
                st.warning(f"GPUåŠ é€Ÿt-SNEå¤±è´¥ï¼Œå°†ä½¿ç”¨CPU: {str(e)}")
                tsne = TSNE(n_components=2, **kwargs)
                result = tsne.fit_transform(descriptors)
        else:
            tsne = TSNE(n_components=2, **kwargs)
            result = tsne.fit_transform(descriptors)
    elif method == "UMAP":
        reducer = umap.UMAP(n_components=2, **kwargs)
        result = reducer.fit_transform(descriptors)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„é™ç»´æ–¹æ³•: {method}")
    
    # è®°å½•æ€»æ—¶é—´
    total_time = time.time() - start_time
    
    # å­˜å‚¨æ€§èƒ½ä¿¡æ¯
    st.session_state[f"{method}_time"] = total_time
    st.session_state[f"{method}_gpu_used"] = gpu_used
    
    return result

def plot_shape_space(coordsA, coordsB, title="å½¢çŠ¶ç©ºé—´åˆ†å¸ƒ"):
    """ç»˜åˆ¶å½¢çŠ¶ç©ºé—´åˆ†å¸ƒå›¾"""
    fig, ax = plt.subplots(figsize=(10, 8))
    ax.scatter(coordsA[:,0], coordsA[:,1], c='blue', alpha=0.5, label="æ•°æ®é›†A", s=10)
    ax.scatter(coordsB[:,0], coordsB[:,1], c='red', alpha=0.5, label="æ•°æ®é›†B", s=10)
    ax.set_title(title)
    ax.legend()
    plt.tight_layout()
    return fig

def plot_pmi_triangle(pmiA, pmiB, labelA="æ•°æ®é›†A", labelB="æ•°æ®é›†B"):
    """ç»˜åˆ¶PMIä¸‰è§’å›¾"""
    def to_triangle_coords(a, b):
        x = b + (a/2.0)
        y = (np.sqrt(3)/2.0) * a
        return (x, y)
    
    coordsA = [to_triangle_coords(a, b) for (a,b) in pmiA]
    coordsB = [to_triangle_coords(a, b) for (a,b) in pmiB]
    
    if not coordsA or not coordsB:
        return None
        
    coordsA = np.array(coordsA)
    coordsB = np.array(coordsB)
    
    fig, ax = plt.subplots(figsize=(10, 8))
    ax.scatter(coordsA[:,0], coordsA[:,1], c='blue', alpha=0.4, s=10, label=labelA)
    ax.scatter(coordsB[:,0], coordsB[:,1], c='red', alpha=0.4, s=10, label=labelB)
    ax.set_title("PMIä¸‰è§’å›¾ï¼ˆå½¢çŠ¶åˆ†å¸ƒï¼‰")
    ax.legend()
    plt.tight_layout()
    return fig

def calculate_distribution_metrics(coords_A, coords_B):
    """è®¡ç®—åˆ†å¸ƒç»Ÿè®¡æŒ‡æ ‡"""
    if len(coords_A) == 0 or len(coords_B) == 0:
        return {}
        
    metrics = {}
    
    # è®¡ç®—æ¯ä¸ªç»´åº¦çš„Wassersteinè·ç¦»
    for dim in range(coords_A.shape[1]):
        w_dist = wasserstein_distance(coords_A[:,dim], coords_B[:,dim])
        metrics[f'Wassersteinè·ç¦»_dim{dim+1}'] = w_dist
    
    # è®¡ç®—ä¸­å¿ƒè·ç¦»
    center_A = np.mean(coords_A, axis=0)
    center_B = np.mean(coords_B, axis=0)
    center_dist = np.linalg.norm(center_A - center_B)
    metrics['ä¸­å¿ƒè·ç¦»'] = center_dist
    
    # è®¡ç®—åˆ†å¸ƒé‡å åº¦
    try:
        kde_A = KernelDensity(kernel='gaussian').fit(coords_A)
        kde_B = KernelDensity(kernel='gaussian').fit(coords_B)
        
        overlap_score = (np.exp(kde_A.score_samples(coords_B)).mean() + 
                        np.exp(kde_B.score_samples(coords_A)).mean()) / 2
        metrics['åˆ†å¸ƒé‡å åº¦'] = overlap_score
    except:
        pass
    
    return metrics

def get_optimal_batch_size(mol_size, device=None):
    """æ ¹æ®åˆ†å­å¤§å°å’ŒGPUæ˜¾å­˜åŠ¨æ€è®¡ç®—æœ€ä¼˜æ‰¹å¤„ç†å¤§å°"""
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    if device.type != 'cuda':
        return 50  # CPUé»˜è®¤æ‰¹å¤„ç†å¤§å°
    
    try:
        # è·å–GPUæ€»æ˜¾å­˜(MB)
        total_mem = torch.cuda.get_device_properties(0).total_memory / 1024**2
        # è·å–å½“å‰å·²ç”¨æ˜¾å­˜(MB)
        used_mem = torch.cuda.memory_allocated(0) / 1024**2
        # å¯ç”¨æ˜¾å­˜(MB)
        available_mem = total_mem - used_mem
        
        # ä¼°ç®—å•ä¸ªåˆ†å­æ‰€éœ€æ˜¾å­˜(MB)
        # å‡è®¾æ¯ä¸ªåŸå­éœ€è¦çš„æ˜¾å­˜çº¦ä¸º: åæ ‡(3*4=12å­—èŠ‚) + è·ç¦»è®¡ç®—(4*4=16å­—èŠ‚) = 28å­—èŠ‚
        mem_per_mol = mol_size * 28 / 1024**2  # è½¬æ¢ä¸ºMB
        
        # é¢„ç•™30%æ˜¾å­˜ç»™å…¶ä»–è®¡ç®—
        safe_mem = available_mem * 0.7
        
        # è®¡ç®—æœ€å¤§æ‰¹å¤„ç†å¤§å°
        max_batch = int(safe_mem / mem_per_mol)
        
        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        optimal_batch = max(10, min(max_batch, 500))
        
        return optimal_batch
    
    except Exception as e:
        st.warning(f"è®¡ç®—æœ€ä¼˜æ‰¹å¤„ç†å¤§å°å¤±è´¥: {str(e)}")
        return 50  # è¿”å›é»˜è®¤å€¼

@torch.jit.script
def compute_distances_jit(coords: torch.Tensor, ref_point: torch.Tensor) -> torch.Tensor:
    """ä½¿ç”¨TorchScriptä¼˜åŒ–çš„è·ç¦»è®¡ç®—"""
    return torch.norm(coords - ref_point.unsqueeze(0), dim=1)

def process_usr_batch_optimized(mols_3d, cuda_available=False, batch_size=None):
    """ä¼˜åŒ–çš„USRæ‰¹å¤„ç†è®¡ç®—"""
    if not mols_3d:
        return []
    
    device = torch.device('cuda' if cuda_available and torch.cuda.is_available() else 'cpu')
    
    # è®°å½•å¼€å§‹æ—¶é—´å’ŒGPUå†…å­˜
    start_time = time.time()
    if device.type == 'cuda':
        mem_before = torch.cuda.memory_allocated() / 1024**2
    
    # è·å–ç¬¬ä¸€ä¸ªæœ‰æ•ˆåˆ†å­çš„å¤§å°ç”¨äºè®¡ç®—æ‰¹å¤„ç†å¤§å°
    mol_size = next((mol.GetNumAtoms() for mol in mols_3d if mol is not None), 0)
    if batch_size is None:
        batch_size = get_optimal_batch_size(mol_size, device)
    
    descriptors = []
    gpu_used = False
    
    # åˆ›å»ºå¼‚æ­¥æµ
    if device.type == 'cuda':
        stream = torch.cuda.Stream()
    
    # æ‰¹é‡å¤„ç†åˆ†å­
    for i in range(0, len(mols_3d), batch_size):
        batch = mols_3d[i:i + batch_size]
        batch_descriptors = []
        
        if device.type == 'cuda':
            with torch.cuda.stream(stream):
                for mol in batch:
                    if mol is not None:
                        desc = compute_usr_descriptor_gpu(mol, device)
                        gpu_used = True
                    else:
                        desc = None
                    batch_descriptors.append(desc)
                
                # åŒæ­¥æµ
                stream.synchronize()
        else:
            for mol in batch:
                if mol is not None:
                    desc = compute_usr_descriptor(mol)
                else:
                    desc = None
                batch_descriptors.append(desc)
        
        descriptors.extend(batch_descriptors)
        
        # æ¸…ç†GPUç¼“å­˜
        if device.type == 'cuda':
            torch.cuda.empty_cache()
    
    # è®¡ç®—æ€§èƒ½ç»Ÿè®¡
    total_time = time.time() - start_time
    if device.type == 'cuda':
        mem_after = torch.cuda.memory_allocated() / 1024**2
        mem_diff = mem_after - mem_before
        
        # æ›´æ–°ä¼šè¯çŠ¶æ€
        st.session_state.usr_batch_time = total_time
        st.session_state.usr_batch_mem = mem_diff
        st.session_state.usr_batch_gpu_used = gpu_used
        
        # è®°å½•å¤„ç†é€Ÿåº¦
        valid_mols = sum(1 for d in descriptors if d is not None)
        speed = valid_mols / total_time if total_time > 0 else 0
        st.session_state.usr_processing_speed = speed
    
    return descriptors

def batch_compute_shape_descriptors(mols, descriptor_type="USR", max_attempts=50, cuda_available=False, batch_size=None):
    """æ”¹è¿›çš„æ‰¹é‡è®¡ç®—å½¢çŠ¶æè¿°ç¬¦å‡½æ•°"""
    if not mols:
        return [], []
    
    descriptors = []
    pmi_ratios = []
    
    # è¿›åº¦æ˜¾ç¤º
    total = len(mols)
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # å°†åˆ†å­åˆ†æ‰¹å¤„ç†
    batch_mols_3d = []
    for m in mols:
        m3d = generate_3d_conformer(m, max_attempts)
        batch_mols_3d.append(m3d)
    
    # æ›´æ–°è¿›åº¦
    progress = len(batch_mols_3d) / total
    progress_bar.progress(progress)
    status_text.text(f"ç”Ÿæˆ3Dæ„è±¡: {len(batch_mols_3d)}/{total}")
        
    # è®¡ç®—PMIæ¯”ç‡
    status_text.text("è®¡ç®—PMIæ¯”ç‡...")
    for m3d in batch_mols_3d:
        if m3d:
            ratios = compute_pmi_ratios(m3d)
            if ratios:
                pmi_ratios.append(ratios)
    
    # è®¡ç®—å½¢çŠ¶æè¿°ç¬¦
    status_text.text(f"è®¡ç®—{descriptor_type}æè¿°ç¬¦...")
    if descriptor_type == "USR":
        descriptors = process_usr_batch_optimized(batch_mols_3d, cuda_available, batch_size)
    else:  # USRCAT
        descriptors = process_usrcat_batch(batch_mols_3d, cuda_available, batch_size)
    
    # æ¸…ç†è¿›åº¦æ˜¾ç¤º
    progress_bar.empty()
    status_text.empty()
    
    # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
    if cuda_available and torch.cuda.is_available():
        speed = st.session_state.get('usr_processing_speed', 0)
        mem_used = st.session_state.get('usr_batch_mem', 0)
        st.info(f"æ€§èƒ½ç»Ÿè®¡:\n"
                f"- å¤„ç†é€Ÿåº¦: {speed:.1f} åˆ†å­/ç§’\n"
                f"- GPUå†…å­˜ä½¿ç”¨: {mem_used:.1f} MB")
    
    return np.array([d for d in descriptors if d is not None]), pmi_ratios

def compute_usr_descriptor_gpu(mol, device=None):
    """ä½¿ç”¨GPUåŠ é€Ÿè®¡ç®—USR (Ultrafast Shape Recognition) æè¿°ç¬¦"""
    if mol is None or mol.GetNumConformers() == 0:
        return None
    
    conf = mol.GetConformer()
    if not conf.Is3D():
        return None
    
    try:
        # è®¾ç½®è®¾å¤‡
        if device is None and torch.cuda.is_available():
            device = torch.device('cuda')
        else:
            device = torch.device('cpu')
        
        # è®°å½•GPUå†…å­˜ä½¿ç”¨å‰çŠ¶æ€
        if device.type == 'cuda':
            mem_before = torch.cuda.memory_allocated() / 1024**2
        
        # æå–æ‰€æœ‰åŸå­åæ ‡å¹¶è½¬æ¢ä¸ºå¼ é‡
        coords = []
        for i in range(mol.GetNumAtoms()):
            pos = conf.GetAtomPosition(i)
            coords.append([pos.x, pos.y, pos.z])
            
        # æ£€æŸ¥åŸå­æ•°é‡
        if len(coords) < 2:
            return None
            
        coords_tensor = torch.tensor(coords, dtype=torch.float32).to(device)
        
        # 1) è´¨å¿ƒ
        centroid = torch.mean(coords_tensor, dim=0)
        
        # 2) ä¸è´¨å¿ƒæœ€è¿œçš„ç‚¹P1
        dists_centroid = torch.norm(coords_tensor - centroid, dim=1)
        idx_p1 = torch.argmax(dists_centroid)
        p1 = coords_tensor[idx_p1]
        
        # 3) ä¸p1æœ€è¿œçš„ç‚¹p2
        dists_p1 = torch.norm(coords_tensor - p1, dim=1)
        idx_p2 = torch.argmax(dists_p1)
        p2 = coords_tensor[idx_p2]
        
        # 4) ä¸p2æœ€è¿œçš„ç‚¹p3
        dists_p2 = torch.norm(coords_tensor - p2, dim=1)
        idx_p3 = torch.argmax(dists_p2)
        p3 = coords_tensor[idx_p3]
        
        # æ‰¹é‡è®¡ç®—æ‰€æœ‰å‚è€ƒç‚¹çš„è·ç¦»ç»Ÿè®¡
        ref_points = torch.stack([centroid, p1, p2, p3])
        
        # ä½¿ç”¨å¹¿æ’­æœºåˆ¶è®¡ç®—æ‰€æœ‰è·ç¦»
        # shape: [n_ref_points, n_atoms]
        all_dists = torch.norm(coords_tensor.unsqueeze(0) - ref_points.unsqueeze(1), dim=2)
        
        # è®¡ç®—æ¯ä¸ªå‚è€ƒç‚¹çš„ç»Ÿè®¡é‡ï¼Œä½¿ç”¨æ— åä¼°è®¡
        means = torch.mean(all_dists, dim=1)
        stds = torch.std(all_dists, dim=1, unbiased=True)
        mins = torch.min(all_dists, dim=1)[0]
        maxs = torch.max(all_dists, dim=1)[0]
        
        # å°†æ‰€æœ‰ç»Ÿè®¡é‡ç»„åˆæˆæœ€ç»ˆæè¿°ç¬¦
        descriptors = torch.stack([
            means, stds, mins, maxs
        ]).t().reshape(-1).cpu().numpy()
        
        # è®°å½•GPUå†…å­˜ä½¿ç”¨åçŠ¶æ€å’Œè°ƒç”¨æ¬¡æ•°
        if device.type == 'cuda':
            mem_after = torch.cuda.memory_allocated() / 1024**2
            mem_diff = mem_after - mem_before
            if mem_diff > 1.0:  # å¦‚æœå†…å­˜å˜åŒ–å¤§äº1MB
                st.session_state.gpu_usr_calls = st.session_state.get('gpu_usr_calls', 0) + 1
                st.session_state.gpu_usr_mem = mem_diff
        
        return descriptors
        
    except Exception as e:
        # åœ¨å‘ç”Ÿé”™è¯¯æ—¶å›é€€åˆ°CPUç‰ˆæœ¬
        st.warning(f"GPUè®¡ç®—USRæè¿°ç¬¦å¤±è´¥ï¼Œå›é€€åˆ°CPU: {str(e)}")
        return compute_usr_descriptor(mol)

def process_usr_batch(mols_3d, cuda_available=False):
    """æ‰¹é‡å¤„ç†USRæè¿°ç¬¦è®¡ç®—ï¼Œæ”¯æŒGPUåŠ é€Ÿ"""
    descriptors = []
    device = torch.device('cuda') if cuda_available and torch.cuda.is_available() else torch.device('cpu')
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # è®°å½•å¤„ç†å‰GPUå†…å­˜
    if device.type == 'cuda':
        mem_before = torch.cuda.memory_allocated() / 1024**2
    
    gpu_used = False
    
    for mol in mols_3d:
        if mol is not None:
            if cuda_available and torch.cuda.is_available():
                desc = compute_usr_descriptor_gpu(mol, device)
                gpu_used = True
            else:
                desc = compute_usr_descriptor(mol)
            descriptors.append(desc)
        else:
            descriptors.append(None)
    
    # è®¡ç®—æ€»æ—¶é—´
    total_time = time.time() - start_time
    
    # è®°å½•å¤„ç†åGPUå†…å­˜å’Œä½¿ç”¨æƒ…å†µ
    if device.type == 'cuda':
        mem_after = torch.cuda.memory_allocated() / 1024**2
        mem_diff = mem_after - mem_before
        st.session_state.usr_batch_time = total_time
        st.session_state.usr_batch_mem = mem_diff
        st.session_state.usr_batch_gpu_used = gpu_used
    
    return descriptors

def compute_usrcat_descriptor_gpu(mol, device=None):
    """ä½¿ç”¨GPUåŠ é€Ÿè®¡ç®—USRCATæè¿°ç¬¦"""
    if mol is None or mol.GetNumConformers() == 0:
        return None
    
    conf = mol.GetConformer()
    if not conf.Is3D():
        return None
    
    try:
        # å®šä¹‰åŸå­ç±»å‹
        atom_types = {
            'all': lambda a: True,
            'hydrophobic': lambda a: a.GetSymbol() in ('C', 'Cl', 'Br', 'I'),
            'aromatic': lambda a: a.GetIsAromatic(),
            'acceptor': lambda a: a.GetSymbol() in ('N', 'O', 'F') and not a.GetIsAromatic(),
            'donor': lambda a: (a.GetSymbol() in ('N', 'O') and 
                              sum(1 for n in a.GetNeighbors() if n.GetSymbol() == 'H') > 0)
        }
        
        if device is None and torch.cuda.is_available():
            device = torch.device('cuda')
        else:
            device = torch.device('cpu')
        
        # è®°å½•GPUå†…å­˜ä½¿ç”¨å‰çŠ¶æ€
        if device.type == 'cuda':
            mem_before = torch.cuda.memory_allocated() / 1024**2
        
        descriptors = []
        
        for atom_type, type_func in atom_types.items():
            # è·å–ç‰¹å®šç±»å‹çš„åŸå­åæ ‡
            coords = []
            for i in range(mol.GetNumAtoms()):
                atom = mol.GetAtomWithIdx(i)
                if type_func(atom):
                    pos = conf.GetAtomPosition(i)
                    coords.append([pos.x, pos.y, pos.z])
            
            if not coords:
                descriptors.extend([0] * 12)  # å¦‚æœæ²¡æœ‰è¯¥ç±»å‹çš„åŸå­ï¼Œå¡«å……0
                continue
                
            # è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶ç§»è‡³GPU
            coords_tensor = torch.tensor(coords, dtype=torch.float32).to(device)
            
            # 1) è´¨å¿ƒ
            centroid = torch.mean(coords_tensor, dim=0)
            
            # 2) ä¸è´¨å¿ƒæœ€è¿œçš„ç‚¹P1
            dists_centroid = torch.norm(coords_tensor - centroid, dim=1)
            idx_p1 = torch.argmax(dists_centroid)
            p1 = coords_tensor[idx_p1]
            
            # 3) ä¸p1æœ€è¿œçš„ç‚¹p2
            dists_p1 = torch.norm(coords_tensor - p1, dim=1)
            idx_p2 = torch.argmax(dists_p1)
            p2 = coords_tensor[idx_p2]
            
            # 4) ä¸p2æœ€è¿œçš„ç‚¹p3
            dists_p2 = torch.norm(coords_tensor - p2, dim=1)
            idx_p3 = torch.argmax(dists_p2)
            p3 = coords_tensor[idx_p3]
            
            # å››ä¸ªå‚è€ƒç‚¹
            for ref_pt in [centroid, p1, p2, p3]:
                dists = torch.norm(coords_tensor - ref_pt, dim=1)
                descriptors.extend([
                    dists.mean().item(),
                    dists.std().item(),
                    dists.min().item(),
                    dists.max().item()
                ])
        
        # è®°å½•GPUå†…å­˜ä½¿ç”¨åçŠ¶æ€
        if device.type == 'cuda':
            mem_after = torch.cuda.memory_allocated() / 1024**2
            mem_diff = mem_after - mem_before
            # å¦‚æœå†…å­˜å˜åŒ–å¤§äº1MBï¼Œåˆ™è®¤ä¸ºGPUçœŸæ­£è¢«ä½¿ç”¨
            if mem_diff > 1.0:
                st.session_state.gpu_usrcat_calls = st.session_state.get('gpu_usrcat_calls', 0) + 1
        
        return np.array(descriptors)
    except Exception as e:
        # åœ¨å‘ç”Ÿé”™è¯¯æ—¶å›é€€åˆ°CPUç‰ˆæœ¬
        st.warning(f"GPUè®¡ç®—USRCATæè¿°ç¬¦å¤±è´¥ï¼Œå›é€€åˆ°CPU: {str(e)}")
        return compute_usrcat_descriptor(mol)

def process_usrcat_batch(mols_3d, cuda_available=False):
    """æ‰¹é‡å¤„ç†USRCATæè¿°ç¬¦è®¡ç®—ï¼Œæ”¯æŒGPUåŠ é€Ÿ"""
    descriptors = []
    device = torch.device('cuda') if cuda_available and torch.cuda.is_available() else torch.device('cpu')
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # è®°å½•å¤„ç†å‰GPUå†…å­˜
    if device.type == 'cuda':
        mem_before = torch.cuda.memory_allocated() / 1024**2
    
    gpu_used = False
    
    for mol in mols_3d:
        if mol is not None:
            if cuda_available and torch.cuda.is_available():
                desc = compute_usrcat_descriptor_gpu(mol, device)
                gpu_used = True
            else:
                desc = compute_usrcat_descriptor(mol)
            descriptors.append(desc)
        else:
            descriptors.append(None)
    
    # è®¡ç®—æ€»æ—¶é—´
    total_time = time.time() - start_time
    
    # è®°å½•å¤„ç†åGPUå†…å­˜å’Œä½¿ç”¨æƒ…å†µ
    if device.type == 'cuda':
        mem_after = torch.cuda.memory_allocated() / 1024**2
        mem_diff = mem_after - mem_before
        st.session_state.usrcat_batch_time = total_time
        st.session_state.usrcat_batch_mem = mem_diff
        st.session_state.usrcat_batch_gpu_used = gpu_used
    
    return descriptors

def monitor_gpu_usage():
    """ç›‘æ§å¹¶è¿”å›GPUä½¿ç”¨æƒ…å†µ"""
    if not torch.cuda.is_available():
        return {"çŠ¶æ€": "GPUä¸å¯ç”¨"}
    
    try:
        metrics = {}
        metrics["æ€»æ˜¾å­˜(MB)"] = torch.cuda.get_device_properties(0).total_memory / 1024**2
        metrics["å·²ç”¨æ˜¾å­˜(MB)"] = torch.cuda.memory_allocated(0) / 1024**2
        metrics["å·²ç¼“å­˜æ˜¾å­˜(MB)"] = torch.cuda.memory_reserved(0) / 1024**2
        metrics["æ˜¾å­˜åˆ©ç”¨ç‡(%)"] = 100 * metrics["å·²ç”¨æ˜¾å­˜(MB)"] / metrics["æ€»æ˜¾å­˜(MB)"]
        return metrics
    except:
        return {"çŠ¶æ€": "æ— æ³•è·å–GPUä¿¡æ¯"}

# æ·»åŠ GPUä½¿ç”¨è®°å½•åŠŸèƒ½
def track_gpu_usage():
    """è®°å½•GPUä½¿ç”¨æƒ…å†µå¹¶è¿”å›å½“å‰çŠ¶æ€"""
    if not torch.cuda.is_available():
        return {'used': 0, 'free': 0, 'total': 0, 'utilization': 0}
    
    try:
        # è·å–GPUå†…å­˜ä¿¡æ¯
        total_mem = torch.cuda.get_device_properties(0).total_memory / 1024**2  # MB
        used_mem = torch.cuda.memory_allocated(0) / 1024**2  # MB
        reserved_mem = torch.cuda.memory_reserved(0) / 1024**2  # MB
        free_mem = total_mem - used_mem  # å¯ç”¨æ˜¾å­˜
        utilization = (used_mem / total_mem) * 100  # ä½¿ç”¨ç‡
        
        # è®°å½•åˆ°ä¼šè¯çŠ¶æ€
        if 'gpu_usage_history' not in st.session_state:
            st.session_state.gpu_usage_history = []
        
        # æ·»åŠ å½“å‰æ—¶é—´ç‚¹çš„è®°å½•
        st.session_state.gpu_usage_history.append({
            'time': time.time(),
            'used': used_mem,
            'reserved': reserved_mem,
            'free': free_mem,
            'total': total_mem,
            'utilization': utilization
        })
        
        # åªä¿ç•™æœ€è¿‘100ä¸ªè®°å½•
        if len(st.session_state.gpu_usage_history) > 100:
            st.session_state.gpu_usage_history = st.session_state.gpu_usage_history[-100:]
        
        return {
            'used': used_mem,
            'reserved': reserved_mem,
            'free': free_mem,
            'total': total_mem,
            'utilization': utilization
        }
    except Exception as e:
        st.error(f"è·å–GPUä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        return {'used': 0, 'free': 0, 'total': 0, 'utilization': 0}

# æ·»åŠ GPUä½¿ç”¨å†å²å¯è§†åŒ–
def plot_gpu_usage_history():
    """ç»˜åˆ¶GPUä½¿ç”¨å†å²å›¾è¡¨"""
    if 'gpu_usage_history' not in st.session_state or not st.session_state.gpu_usage_history:
        st.info("å°šæ— GPUä½¿ç”¨è®°å½•")
        return
    
    history = st.session_state.gpu_usage_history
    
    # æå–æ•°æ®
    times = [(record['time'] - history[0]['time']) for record in history]
    used = [record['used'] for record in history]
    reserved = [record['reserved'] for record in history]
    
    # åˆ›å»ºå›¾è¡¨
    fig, ax = plt.subplots(figsize=(10, 4))
    ax.plot(times, used, 'r-', label='å·²åˆ†é…')
    ax.plot(times, reserved, 'b--', label='å·²ç¼“å­˜')
    
    # æ·»åŠ æ ‡ç­¾å’Œæ ‡é¢˜
    ax.set_xlabel('æ—¶é—´ (ç§’)')
    ax.set_ylabel('æ˜¾å­˜ (MB)')
    ax.set_title('GPUå†…å­˜ä½¿ç”¨å†å²')
    ax.legend()
    
    # æ·»åŠ ç½‘æ ¼
    ax.grid(True, linestyle='--', alpha=0.7)
    
    # æ˜¾ç¤ºå›¾è¡¨
    st.pyplot(fig)

# åœ¨å‡½æ•°è°ƒç”¨ä¹‹å‰å¯åŠ¨GPUç›‘æ§
def start_gpu_monitoring():
    """å¯åŠ¨GPUç›‘æ§"""
    if not torch.cuda.is_available():
        return
    
    # æ¸…ç©ºå†å²è®°å½•
    st.session_state.gpu_usage_history = []
    
    # è®°å½•åˆå§‹çŠ¶æ€
    track_gpu_usage()
    
    # è®¾ç½®ç›‘æ§å·²å¯åŠ¨æ ‡å¿—
    st.session_state.gpu_monitoring_active = True

# åœ¨å‡½æ•°è°ƒç”¨ä¹‹ååœæ­¢GPUç›‘æ§å¹¶æ˜¾ç¤ºæŠ¥å‘Š
def stop_gpu_monitoring_and_report():
    """åœæ­¢GPUç›‘æ§å¹¶ç”ŸæˆæŠ¥å‘Š"""
    if not torch.cuda.is_available() or not st.session_state.get('gpu_monitoring_active', False):
        return
    
    # è®°å½•æœ€ç»ˆçŠ¶æ€
    track_gpu_usage()
    
    # è®¾ç½®ç›‘æ§åœæ­¢æ ‡å¿—
    st.session_state.gpu_monitoring_active = False
    
    # ç”Ÿæˆä½¿ç”¨æŠ¥å‘Š
    if 'gpu_usage_history' in st.session_state and st.session_state.gpu_usage_history:
        st.subheader("GPUä½¿ç”¨å†å²")
        
        history = st.session_state.gpu_usage_history
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        max_used = max([record['used'] for record in history])
        avg_used = sum([record['used'] for record in history]) / len(history)
        peak_util = max([record['utilization'] for record in history])
        
        # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        col1, col2, col3 = st.columns(3)
        col1.metric("æœ€å¤§æ˜¾å­˜ä½¿ç”¨ (MB)", f"{max_used:.1f}")
        col2.metric("å¹³å‡æ˜¾å­˜ä½¿ç”¨ (MB)", f"{avg_used:.1f}")
        col3.metric("æœ€é«˜ä½¿ç”¨ç‡ (%)", f"{peak_util:.1f}")
        
        # ç»˜åˆ¶ä½¿ç”¨å†å²
        plot_gpu_usage_history()

# åœ¨ä¸»ç•Œé¢æ·»åŠ GPUç›‘æ§é€‰é¡¹
st.sidebar.subheader("GPUç›‘æ§")
if st.sidebar.checkbox("å¯ç”¨GPUç›‘æ§", value=False):
    gpu_monitor = st.sidebar.empty()
    
    def update_gpu_monitor():
        """æ›´æ–°GPUç›‘æ§ä¿¡æ¯"""
        while True:
            metrics = monitor_gpu_usage()
            content = ""
            for k, v in metrics.items():
                if isinstance(v, float):
                    content += f"- {k}: {v:.1f}\n"
                else:
                    content += f"- {k}: {v}\n"
            gpu_monitor.code(content)
            time.sleep(1.0)  # æ¯ç§’æ›´æ–°ä¸€æ¬¡
    
    import threading
    monitor_thread = threading.Thread(target=update_gpu_monitor, daemon=True)
    monitor_thread.start()

# åœ¨ä¸»ç¨‹åºä¸­åˆå§‹åŒ–ä¼šè¯çŠ¶æ€å˜é‡
if 'gpu_debug_info' not in st.session_state:
    st.session_state.gpu_debug_info = {
        'usrcat_calls': 0,
        'nn_calls': 0,
        'tsne_calls': 0,
        'usrcat_time': 0,
        'nn_time': 0,
        'tsne_time': 0,
        'total_gpu_mem_peak': 0,
    }

# æ·»åŠ GPUä½¿ç”¨è¯¦æƒ…æ˜¾ç¤ºå‡½æ•°
def show_gpu_usage_details():
    """æ˜¾ç¤ºè¯¦ç»†çš„GPUä½¿ç”¨æƒ…å†µ"""
    if not torch.cuda.is_available():
        st.warning("GPUä¸å¯ç”¨ï¼Œæ— æ³•æ˜¾ç¤ºGPUä½¿ç”¨è¯¦æƒ…")
        return
    
    st.subheader("ğŸ” GPUåŠ é€Ÿè¯¦æƒ…")
    
    # æ˜¾ç¤ºGPUè°ƒç”¨æ¬¡æ•°
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("USRæè¿°ç¬¦GPUè°ƒç”¨", st.session_state.get('gpu_usr_calls', 0))
    
    with col2:
        st.metric("USRCATæè¿°ç¬¦GPUè°ƒç”¨", st.session_state.get('gpu_usrcat_calls', 0))
    
    with col3:
        st.metric("æœ€è¿‘é‚»è®¡ç®—GPUè°ƒç”¨", st.session_state.get('gpu_nn_calls', 0))
    
    with col4:
        st.metric("é™ç»´GPUè°ƒç”¨", st.session_state.get('gpu_tsne_calls', 0))
    
    # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨è¯¦æƒ…
    st.subheader("GPUå†…å­˜ä½¿ç”¨æƒ…å†µ")
    mem_data = {
        "å½“å‰å·²åˆ†é…": torch.cuda.memory_allocated() / 1024**2,
        "å½“å‰å·²ç¼“å­˜": torch.cuda.memory_reserved() / 1024**2,
        "USRè®¡ç®—ä½¿ç”¨": st.session_state.get('gpu_usr_mem', 0),
        "USRCATè®¡ç®—ä½¿ç”¨": st.session_state.get('usrcat_batch_mem', 0),
        "æœ€è¿‘é‚»è®¡ç®—ä½¿ç”¨": st.session_state.get('gpu_nn_mem_usage', 0),
        "t-SNEä½¿ç”¨": st.session_state.get('gpu_tsne_mem', 0),
        "å³°å€¼ä½¿ç”¨": st.session_state.get('gpu_peak_mem', 0),
    }
    
    # ç»˜åˆ¶å†…å­˜ä½¿ç”¨æŸ±çŠ¶å›¾
    mem_df = pd.DataFrame([mem_data.values()], columns=mem_data.keys())
    st.bar_chart(mem_df.T)
    
    # æ˜¾ç¤ºGPUåŠ é€Ÿæ€§èƒ½å¯¹æ¯”
    if st.session_state.get('t-SNE_time') and st.session_state.get('t-SNE_gpu_used'):
        st.info(f"GPUåŠ é€Ÿt-SNEè€—æ—¶: {st.session_state.get('t-SNE_time'):.2f}ç§’")
    
    # è®°å½•å³°å€¼æ˜¾å­˜ä½¿ç”¨
    current_mem = torch.cuda.memory_allocated() / 1024**2
    peak_mem = st.session_state.get('gpu_peak_mem', 0)
    if current_mem > peak_mem:
        st.session_state.gpu_peak_mem = current_mem
    
    # æ·»åŠ GPUä¿¡æ¯è¡¨æ ¼
    gpu_info = {
        "å±æ€§": ["è®¾å¤‡åç§°", "è®¡ç®—å•å…ƒæ•°", "æ€»æ˜¾å­˜", "å½“å‰æ¸©åº¦", "æ˜¾å­˜å¸¦å®½"],
        "å€¼": [
            torch.cuda.get_device_name(0),
            str(torch.cuda.get_device_properties(0).multi_processor_count),
            f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB",
            f"{get_gpu_temp()}",
            f"{torch.cuda.get_device_properties(0).memory_clock_rate * torch.cuda.get_device_properties(0).memory_bus_width / (8 * 1000):.1f} GB/s"
        ]
    }
    st.table(pd.DataFrame(gpu_info))

# æ·»åŠ GPUæ¸©åº¦è·å–å‡½æ•°(ä»…æ”¯æŒNVIDIA GPUå’ŒLinux)
def get_gpu_temp():
    """è·å–GPUæ¸©åº¦ï¼Œå¦‚æœå¯èƒ½çš„è¯"""
    try:
        import subprocess
        output = subprocess.check_output(['nvidia-smi', '--query-gpu=temperature.gpu', '--format=csv,noheader'])
        return output.decode('utf-8').strip() + "Â°C"
    except:
        return "æœªçŸ¥"

# æ·»åŠ ä¸€ä¸ªè¯¦ç»†çš„GPUä¿¡æ¯é¢æ¿å‡½æ•°
def gpu_debug_panel():
    """æ˜¾ç¤ºGPUè°ƒè¯•é¢æ¿"""
    if not torch.cuda.is_available():
        st.sidebar.warning("GPUä¸å¯ç”¨ï¼Œæ— æ³•æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯")
        return
    
    with st.sidebar.expander("ğŸ” GPUè°ƒè¯•ä¿¡æ¯", expanded=False):
        st.write("### GPUå†…å­˜è¿½è¸ª")
        
        # æ˜¾ç¤ºå½“å‰å†…å­˜ä½¿ç”¨
        current_mem = torch.cuda.memory_allocated() / 1024**2
        st.progress(min(1.0, current_mem / torch.cuda.get_device_properties(0).total_memory * 1024**2))
        st.write(f"å½“å‰ä½¿ç”¨: {current_mem:.1f}MB / {torch.cuda.get_device_properties(0).total_memory / 1024**2:.1f}MB")
        
        # æ˜¾ç¤ºGPUè°ƒç”¨è¯¦æƒ…
        st.write("### å‡½æ•°è°ƒç”¨")
        st.write(f"- USRCAT GPUè°ƒç”¨: {st.session_state.get('gpu_usrcat_calls', 0)}")
        st.write(f"- æœ€è¿‘é‚»GPUè°ƒç”¨: {st.session_state.get('gpu_nn_calls', 0)}")
        st.write(f"- é™ç»´GPUè°ƒç”¨: {st.session_state.get('gpu_tsne_calls', 0)}")
        
        # æ˜¾ç¤ºè®¡æ—¶ä¿¡æ¯
        st.write("### è®¡æ—¶ä¿¡æ¯")
        st.write(f"- USRCATæ‰¹å¤„ç†: {st.session_state.get('usrcat_batch_time', 0):.3f}ç§’")
        st.write(f"- t-SNEé™ç»´: {st.session_state.get('t-SNE_time', 0):.3f}ç§’")
        
        # æ˜¾ç¤ºé”™è¯¯è®¡æ•°
        st.write("### é”™è¯¯è®¡æ•°")
        st.write(f"- GPUå›é€€æ¬¡æ•°: {st.session_state.get('gpu_fallbacks', 0)}")
        
        # æ˜¾ç¤ºCUDAç‰ˆæœ¬ä¿¡æ¯
        st.write("### CUDAç‰ˆæœ¬")
        st.code(f"CUDA: {torch.version.cuda}\nCuDNN: {torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else 'N/A'}")

# åœ¨ä¸»é¡µé¢åº•éƒ¨å¢åŠ è¯¦ç»†çš„GPUä½¿ç”¨æŠ¥å‘Š
def generate_gpu_report():
    """ç”Ÿæˆè¯¦ç»†çš„GPUä½¿ç”¨æŠ¥å‘Š"""
    if not torch.cuda.is_available():
        return
    
    st.subheader("ğŸ” GPUä½¿ç”¨è¯¦ç»†æŠ¥å‘Š")
    
    # æ€»ä½“ä½¿ç”¨æƒ…å†µ
    st.write("### æ€»ä½“ä½¿ç”¨æƒ…å†µ")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        if st.session_state.get('gpu_usrcat_calls', 0) > 0:
            st.success("âœ… USRCATæè¿°ç¬¦è®¡ç®—å·²ä½¿ç”¨GPUåŠ é€Ÿ")
        else:
            st.error("âŒ USRCATæè¿°ç¬¦è®¡ç®—æœªä½¿ç”¨GPU")
            
    with col2:
        if st.session_state.get('gpu_nn_calls', 0) > 0:
            st.success("âœ… æœ€è¿‘é‚»è·ç¦»è®¡ç®—å·²ä½¿ç”¨GPUåŠ é€Ÿ")
        else:
            st.error("âŒ æœ€è¿‘é‚»è·ç¦»è®¡ç®—æœªä½¿ç”¨GPU")
            
    with col3:
        if st.session_state.get('gpu_tsne_calls', 0) > 0:
            st.success("âœ… é™ç»´åˆ†æå·²ä½¿ç”¨GPUåŠ é€Ÿ")
        else:
            st.error("âŒ é™ç»´åˆ†ææœªä½¿ç”¨GPU")
    
    # GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
    st.write("### GPUå†…å­˜ä½¿ç”¨æƒ…å†µ")
    
    # æ”¶é›†æ‰€æœ‰å†…å­˜ä½¿ç”¨æ•°æ®
    mem_data = {
        "æè¿°ç¬¦è®¡ç®—": st.session_state.get('usrcat_batch_mem', 0),
        "æœ€è¿‘é‚»è·ç¦»": st.session_state.get('gpu_nn_mem_usage', 0),
        "é™ç»´åˆ†æ": st.session_state.get('gpu_tsne_mem', 0),
        "æœ€å¤§ä½¿ç”¨é‡": st.session_state.get('gpu_peak_mem', 0),
    }
    
    # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æ¡å½¢å›¾
    mem_df = pd.DataFrame([mem_data.values()], columns=mem_data.keys())
    st.bar_chart(mem_df.T)
    
    # GPU VS CPUæ€§èƒ½æ¯”è¾ƒ
    st.write("### GPUåŠ é€Ÿæ•ˆæœ")
    
    # å¦‚æœæœ‰CPUå’ŒGPUçš„æ—¶é—´å¯¹æ¯”ï¼Œæ˜¾ç¤ºæ¯”è¾ƒå›¾è¡¨
    if st.session_state.get('t-SNE_time') and st.session_state.get('t-SNE_gpu_used'):
        gpu_time = st.session_state.get('t-SNE_time', 0)
        # è¿™é‡Œæˆ‘ä»¬ä¼°è®¡CPUæ—¶é—´æ˜¯GPUæ—¶é—´çš„5å€(ä»…ç”¨äºæ¼”ç¤º)
        cpu_time = gpu_time * 5  # å®é™…åº”ç”¨ä¸­åº”è¯¥æœ‰çœŸå®çš„å¯¹æ¯”æ•°æ®
        
        perf_data = {
            "GPU": gpu_time,
            "ä¼°è®¡CPU": cpu_time
        }
        perf_df = pd.DataFrame([perf_data.values()], columns=perf_data.keys())
        st.bar_chart(perf_df.T)
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        speedup = cpu_time / gpu_time if gpu_time > 0 else 0
        st.metric("ä¼°è®¡åŠ é€Ÿæ¯”", f"{speedup:.1f}x")
    
    # æ·»åŠ GPUè®¾å¤‡è¯¦æƒ…
    with st.expander("GPUè®¾å¤‡è¯¦æƒ…"):
        properties = torch.cuda.get_device_properties(0)
        st.json({
            "è®¾å¤‡åç§°": torch.cuda.get_device_name(0),
            "è®¡ç®—èƒ½åŠ›": f"{properties.major}.{properties.minor}",
            "å¤šå¤„ç†å™¨æ•°é‡": properties.multi_processor_count,
            "æ€»æ˜¾å­˜(GB)": properties.total_memory / 1024**3,
            "æœ€å¤§çº¿ç¨‹æ•°/å—": properties.max_threads_per_block,
            "æ—¶é’Ÿé¢‘ç‡(MHz)": properties.clock_rate / 1000,
            "L2ç¼“å­˜å¤§å°(MB)": properties.l2_cache_size / 1024**2 if hasattr(properties, 'l2_cache_size') else "æœªçŸ¥",
        })

# æ·»åŠ CUDAæ£€æŸ¥å‡½æ•°ï¼Œç”¨äºéªŒè¯CUDAæ˜¯å¦æ­£å¸¸å·¥ä½œ
def verify_cuda_operation():
    """æ‰§è¡Œç®€å•çš„CUDAæ“ä½œä»¥éªŒè¯GPUæ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    if not torch.cuda.is_available():
        return "CUDAä¸å¯ç”¨"
    
    try:
        # åˆ›å»ºä¸¤ä¸ªéšæœºå¼ é‡
        a = torch.randn(1000, 1000).cuda()
        b = torch.randn(1000, 1000).cuda()
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start = time.time()
        
        # æ‰§è¡ŒçŸ©é˜µä¹˜æ³•
        c = torch.matmul(a, b)
        
        # ç¡®ä¿è®¡ç®—å®Œæˆ
        torch.cuda.synchronize()
        
        # è®¡ç®—è€—æ—¶
        duration = time.time() - start
        
        # æ¸…ç†
        del a, b, c
        torch.cuda.empty_cache()
        
        return f"CUDAè¿è¡Œæ­£å¸¸ï¼Œ1000x1000çŸ©é˜µä¹˜æ³•è€—æ—¶: {duration*1000:.2f}æ¯«ç§’"
    except Exception as e:
        return f"CUDAæµ‹è¯•å¤±è´¥: {str(e)}"

# åœ¨é¡µé¢åŠ è½½æ—¶æ·»åŠ GPUæµ‹è¯•æŒ‰é’®
st.sidebar.subheader("GPUçŠ¶æ€æ£€æµ‹")
if st.sidebar.button("æµ‹è¯•GPU"):
    result = verify_cuda_operation()
    st.sidebar.code(result)

# åœ¨é¡µé¢åŠ è½½æ—¶æ·»åŠ GPUè°ƒè¯•é¢æ¿
if st.sidebar.checkbox("å¯ç”¨GPUå®æ—¶ç›‘æ§", value=False):
    gpu_debug_panel()

# ä¸»ç•Œé¢é€»è¾‘
if st.button("å¼€å§‹åˆ†æ") and fileA is not None and fileB is not None:
    with st.spinner("æ­£åœ¨è¿›è¡Œ3Då½¢çŠ¶åˆ†æ..."):
        try:
            analysis_start_time = time.time()
            
            # åŠ è½½åˆ†å­
            st.info("åŠ è½½åˆ†å­æ•°æ®...")
            molsA, dfA = load_smiles(fileA, smiles_col)
            molsB, dfB = load_smiles(fileB, smiles_col)
            
            if molsA is None or molsB is None or len(molsA) == 0 or len(molsB) == 0:
                st.error("æ— æ³•åŠ è½½åˆ†å­æ•°æ®æˆ–æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®")
                st.stop()
            
            st.success(f"æˆåŠŸåŠ è½½ {len(molsA)} ä¸ªåˆ†å­(æ•°æ®é›†A)å’Œ {len(molsB)} ä¸ªåˆ†å­(æ•°æ®é›†B)")
            
            # åˆå§‹åŒ–GPU
            cuda_available = False
            device = torch.device("cpu")
            
            if enable_gpu:
                try:
                    cuda_available, device = initialize_cuda()
                    if cuda_available:
                        # æ ¹æ®GPUç­–ç•¥è®¾ç½®å†…å­˜é™åˆ¶
                        try:
                            if gpu_strategy == "å†…å­˜ä¼˜å…ˆ":
                                # ä¿ç•™æ›´å¤šGPUå†…å­˜ï¼Œæ…¢ä½†æ›´å®‰å…¨
                                torch.cuda.set_per_process_memory_fraction(gpu_mem_limit / 100 * 0.8)
                            elif gpu_strategy == "æ€§èƒ½ä¼˜å…ˆ":
                                # ä½¿ç”¨æ›´å¤šGPUå†…å­˜ï¼Œæ›´å¿«ä½†é£é™©æ›´é«˜
                                torch.cuda.set_per_process_memory_fraction(gpu_mem_limit / 100 * 0.95)
                            else:  # å¹³è¡¡æ¨¡å¼
                                torch.cuda.set_per_process_memory_fraction(gpu_mem_limit / 100 * 0.9)
                        except Exception as e:
                            st.warning(f"è®¾ç½®GPUå†…å­˜é™åˆ¶æ—¶å‡ºé”™: {str(e)}")
                        
                        # æ¸…ç†GPUå†…å­˜
                        torch.cuda.empty_cache()
                        
                        # æ˜¾ç¤ºGPUçŠ¶æ€
                        gpu_stats = {
                            "GPUå‹å·": torch.cuda.get_device_name(0),
                            "æ€»æ˜¾å­˜": f"{torch.cuda.get_device_properties(0).total_memory / 1024**2:.1f} MB",
                            "å·²ç”¨æ˜¾å­˜": f"{torch.cuda.memory_allocated() / 1024**2:.1f} MB",
                            "å·²ç¼“å­˜æ˜¾å­˜": f"{torch.cuda.memory_reserved() / 1024**2:.1f} MB"
                        }
                        st.write("GPUçŠ¶æ€:", gpu_stats)
                except Exception as e:
                    st.warning(f"åˆå§‹åŒ–GPUæ—¶å‡ºé”™: {str(e)}ï¼Œå°†ä½¿ç”¨CPUè¿›è¡Œè®¡ç®—")
                    cuda_available = False
                    device = torch.device("cpu")
            else:
                st.info("GPUåŠ é€Ÿå·²ç¦ç”¨ï¼Œå°†ä½¿ç”¨CPUè¿›è¡Œè®¡ç®—")
                
                # éšæœºæŠ½æ ·
                if len(molsA) > max_samples:
                    molsA = random.sample(molsA, max_samples)
                st.info(f"å·²ä»æ•°æ®é›†AéšæœºæŠ½å– {max_samples} ä¸ªåˆ†å­è¿›è¡Œåˆ†æ")
                if len(molsB) > max_samples:
                    molsB = random.sample(molsB, max_samples)
                st.info(f"å·²ä»æ•°æ®é›†BéšæœºæŠ½å– {max_samples} ä¸ªåˆ†å­è¿›è¡Œåˆ†æ")
            
            # æ”¶é›†æ„è±¡ç”Ÿæˆå‚æ•°
            conformer_params = {
                # åŸºæœ¬å‚æ•°
                'max_attempts': max_attempts,
                'use_mmff': use_mmff,
                'energy_iter': energy_iter,
                'add_hydrogens': add_hydrogens,
                
                # TorchANIå‚æ•°
                'torchani_model': torchani_model if 'torchani_model' in locals() else 'ANI2x',
                'optimization_steps': optimization_steps if 'optimization_steps' in locals() else 100,
                'torchani_batch_size': torchani_batch_size if 'torchani_batch_size' in locals() else 32,
                'use_torchani_optimization': use_torchani_optimization if 'use_torchani_optimization' in locals() else True,
                'learning_rate': learning_rate if 'learning_rate' in locals() else 0.01,
                'use_mixed_precision_torchani': use_mixed_precision_torchani if 'use_mixed_precision_torchani' in locals() else True,
                'max_atoms_per_batch': max_atoms_per_batch if 'max_atoms_per_batch' in locals() else 5000,
                'gradient_clipping': gradient_clipping if 'gradient_clipping' in locals() else True,
                
                # DeepChemå‚æ•°
                'model_type': deepchem_model if 'deepchem_model' in locals() else 'mpnn',
                'use_gpu': enable_gpu and cuda_available,
                'use_mixed_precision': use_mixed_precision if 'use_mixed_precision' in locals() else True,
                'dc_force_field': dc_force_field if 'dc_force_field' in locals() else 'mmff94s',
                
                # Claraå‚æ•°
                'force_field': clara_force_field if 'clara_force_field' in locals() else 'MMFF94s',
                'precision': clara_precision if 'clara_precision' in locals() else 'mixed',
                'num_conformers': clara_num_conformers if 'clara_num_conformers' in locals() else 1,
                'energy_threshold': clara_energy_threshold if 'clara_energy_threshold' in locals() else 1.0,
                'clara_optimization_steps': clara_optimization_steps if 'clara_optimization_steps' in locals() else 500
            }
            
            # ä½¿ç”¨æ–°çš„æ‰¹é‡3Dæ„è±¡ç”Ÿæˆ
            step_start_time = time.time()
            
            # Create status elements and progress bar before calling the function
            status_container_A = st.empty()
            progress_text_A = st.empty()
            progress_bar_A = st.progress(0)

            with st.spinner("ç”Ÿæˆæ•°æ®é›†Açš„3Dæ„è±¡..."):
                molsA_3d = batch_generate_3d_conformers(
                    molsA, 
                    progress_bar=progress_bar_A, # Pass the progress bar object
                    status_container=status_container_A,
                    progress_text=progress_text_A,
                    backend=conformer_backend,
                    batch_size=batch_size if not auto_batch else None,
                    **conformer_params
                )
            
            step_time = time.time() - step_start_time
            st.success(f"æ•°æ®é›†Aæ„è±¡ç”Ÿæˆå®Œæˆï¼Œç”¨æ—¶: {step_time:.1f}ç§’")
            # Clear progress bar A after completion
            progress_bar_A.empty() 
            status_container_A.empty()
            progress_text_A.empty()

            step_start_time = time.time()
            
            # Create status elements and progress bar for dataset B
            status_container_B = st.empty()
            progress_text_B = st.empty()
            progress_bar_B = st.progress(0)
            
            with st.spinner("ç”Ÿæˆæ•°æ®é›†Bçš„3Dæ„è±¡..."):
                molsB_3d = batch_generate_3d_conformers(
                    molsB, 
                    progress_bar=progress_bar_B, # Pass the progress bar object
                    status_container=status_container_B,
                    progress_text=progress_text_B,
                    backend=conformer_backend,
                    batch_size=batch_size if not auto_batch else None,
                    **conformer_params
                )
            
            step_time = time.time() - step_start_time
            st.success(f"æ•°æ®é›†Bæ„è±¡ç”Ÿæˆå®Œæˆï¼Œç”¨æ—¶: {step_time:.1f}ç§’")
            # Clear progress bar B after completion
            progress_bar_B.empty() 
            status_container_B.empty()
            progress_text_B.empty()
            
            # ç§»é™¤æ— æ•ˆæ„è±¡
            valid_molsA_3d = [mol for mol in molsA_3d if mol is not None]
            valid_molsB_3d = [mol for mol in molsB_3d if mol is not None]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æœ‰æ•ˆæ„è±¡
            if len(valid_molsA_3d) == 0 or len(valid_molsB_3d) == 0:
                st.error("æœ‰æ•ˆçš„3Dæ„è±¡æ•°é‡ä¸è¶³ï¼Œæ— æ³•ç»§ç»­åˆ†æ")
                st.error(f"æ•°æ®é›†A: {len(valid_molsA_3d)}/{len(molsA)} ä¸ªæœ‰æ•ˆæ„è±¡")
                st.error(f"æ•°æ®é›†B: {len(valid_molsB_3d)}/{len(molsB)} ä¸ªæœ‰æ•ˆæ„è±¡")
                st.stop()
            
            # çŠ¶æ€æ˜¾ç¤º
            st.info(f"æ•°æ®é›†A: {len(valid_molsA_3d)}/{len(molsA)} ä¸ªæœ‰æ•ˆæ„è±¡ ({len(valid_molsA_3d)/len(molsA)*100:.1f}%)")
            st.info(f"æ•°æ®é›†B: {len(valid_molsB_3d)}/{len(molsB)} ä¸ªæœ‰æ•ˆæ„è±¡ ({len(valid_molsB_3d)/len(molsB)*100:.1f}%)")
            
            # è®¡ç®—å½¢çŠ¶æè¿°ç¬¦
            step_start_time = time.time()
            
            with st.spinner(f"è®¡ç®— {shape_desc} å½¢çŠ¶æè¿°ç¬¦..."):
                try:
                    if shape_desc == "USR":
                        descsA = batch_compute_shape_descriptors(
                            valid_molsA_3d, 
                            descriptor_type="USR", 
                        cuda_available=cuda_available,
                            batch_size=batch_size if not auto_batch else None
                        )
                        descsB = batch_compute_shape_descriptors(
                            valid_molsB_3d, 
                            descriptor_type="USR", 
                            cuda_available=cuda_available,
                            batch_size=batch_size if not auto_batch else None
                        )
                    else:  # USRCAT
                        descsA = batch_compute_shape_descriptors(
                            valid_molsA_3d, 
                            descriptor_type="USRCAT", 
                            cuda_available=cuda_available,
                            batch_size=batch_size if not auto_batch else None
                        )
                        descsB = batch_compute_shape_descriptors(
                            valid_molsB_3d, 
                            descriptor_type="USRCAT", 
                            cuda_available=cuda_available,
                            batch_size=batch_size if not auto_batch else None
                        )
                except Exception as e:
                    st.error(f"è®¡ç®—å½¢çŠ¶æè¿°ç¬¦æ—¶å‡ºé”™: {str(e)}")
                    st.stop()
            
            step_time = time.time() - step_start_time
            st.success(f"å½¢çŠ¶æè¿°ç¬¦è®¡ç®—å®Œæˆï¼Œç”¨æ—¶: {step_time:.1f}ç§’")
            
            # ç§»é™¤æ— æ•ˆæè¿°ç¬¦
            valid_descsA = [d for d in descsA if d is not None]
            valid_descsB = [d for d in descsB if d is not None]
            
            if len(valid_descsA) == 0 or len(valid_descsB) == 0:
                st.error("æœ‰æ•ˆçš„å½¢çŠ¶æè¿°ç¬¦æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
                st.error(f"æ•°æ®é›†A: {len(valid_descsA)}/{len(valid_molsA_3d)} ä¸ªæœ‰æ•ˆæè¿°ç¬¦")
                st.error(f"æ•°æ®é›†B: {len(valid_descsB)}/{len(valid_molsB_3d)} ä¸ªæœ‰æ•ˆæè¿°ç¬¦")
                st.stop()
            
            st.info(f"æ•°æ®é›†A: {len(valid_descsA)}/{len(valid_molsA_3d)} ä¸ªæœ‰æ•ˆæè¿°ç¬¦ ({len(valid_descsA)/len(valid_molsA_3d)*100:.1f}%)")
            st.info(f"æ•°æ®é›†B: {len(valid_descsB)}/{len(valid_molsB_3d)} ä¸ªæœ‰æ•ˆæè¿°ç¬¦ ({len(valid_descsB)/len(valid_molsB_3d)*100:.1f}%)")
            
            # æè¿°ç¬¦æ ‡å‡†åŒ–
            if normalize_desc:
                step_start_time = time.time()
                with st.spinner("æ ‡å‡†åŒ–æè¿°ç¬¦..."):
                    try:
                        # æ£€æŸ¥æè¿°ç¬¦æœ‰æ•ˆæ€§å’Œå½¢çŠ¶ä¸€è‡´æ€§
                        if len(valid_descsA) == 0 or len(valid_descsB) == 0:
                            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„æè¿°ç¬¦å¯ä¾›æ ‡å‡†åŒ–")
                        
                        # æ£€æŸ¥æ‰€æœ‰æè¿°ç¬¦æ˜¯å¦å…·æœ‰ç›¸åŒçš„å½¢çŠ¶
                        desc_shapes_A = [d.shape if hasattr(d, 'shape') else len(d) for d in valid_descsA]
                        desc_shapes_B = [d.shape if hasattr(d, 'shape') else len(d) for d in valid_descsB]
                        
                        # ç¡®ä¿æ‰€æœ‰æè¿°ç¬¦éƒ½æ˜¯numpyæ•°ç»„ä¸”å½¢çŠ¶ä¸€è‡´
                        valid_descsA_clean = []
                        valid_descsB_clean = []
                        
                        # è·å–æœŸæœ›çš„æè¿°ç¬¦é•¿åº¦ï¼ˆä»ç¬¬ä¸€ä¸ªæœ‰æ•ˆæè¿°ç¬¦ï¼‰
                        expected_length = None
                        for desc in valid_descsA + valid_descsB:
                            if desc is not None and hasattr(desc, '__len__'):
                                expected_length = len(desc)
                                break
                        
                        if expected_length is None:
                            raise ValueError("æ— æ³•ç¡®å®šæè¿°ç¬¦çš„æœŸæœ›é•¿åº¦")
                        
                        # è¿‡æ»¤å’Œæ¸…ç†æè¿°ç¬¦
                        for desc in valid_descsA:
                            if desc is not None and hasattr(desc, '__len__') and len(desc) == expected_length:
                                if hasattr(desc, 'shape'):
                                    valid_descsA_clean.append(desc)
                                else:
                                    valid_descsA_clean.append(np.array(desc))
                        
                        for desc in valid_descsB:
                            if desc is not None and hasattr(desc, '__len__') and len(desc) == expected_length:
                                if hasattr(desc, 'shape'):
                                    valid_descsB_clean.append(desc)
                                else:
                                    valid_descsB_clean.append(np.array(desc))
                        
                        if len(valid_descsA_clean) == 0 or len(valid_descsB_clean) == 0:
                            raise ValueError("æ¸…ç†åæ²¡æœ‰æœ‰æ•ˆçš„æè¿°ç¬¦å¯ä¾›æ ‡å‡†åŒ–")
                        
                        # è½¬æ¢ä¸ºnumpyæ•°ç»„
                        valid_descsA = np.array(valid_descsA_clean)
                        valid_descsB = np.array(valid_descsB_clean)
                        
                        # åˆå¹¶æ‰€æœ‰æè¿°ç¬¦ä»¥è®¡ç®—å…¨å±€å‡å€¼å’Œæ ‡å‡†å·®
                        all_descs = np.vstack([valid_descsA, valid_descsB])
                        mean = np.mean(all_descs, axis=0)
                        std = np.std(all_descs, axis=0)
                        # é˜²æ­¢é™¤ä»¥é›¶
                        std[std == 0] = 1.0
                        # åº”ç”¨æ ‡å‡†åŒ–
                        valid_descsA = (valid_descsA - mean) / std
                        valid_descsB = (valid_descsB - mean) / std
                        
                        st.info(f"æ ‡å‡†åŒ–å®Œæˆ: A={valid_descsA.shape}, B={valid_descsB.shape}")
                        
                    except Exception as e:
                        st.error(f"æ ‡å‡†åŒ–æè¿°ç¬¦æ—¶å‡ºé”™: {str(e)}")
                        st.warning("è·³è¿‡æ ‡å‡†åŒ–æ­¥éª¤...")
                
                step_time = time.time() - step_start_time
                st.success(f"æè¿°ç¬¦æ ‡å‡†åŒ–å®Œæˆï¼Œç”¨æ—¶: {step_time:.1f}ç§’")
            
            # é™ç»´å¯è§†åŒ–
            step_start_time = time.time()
            with st.spinner(f"ä½¿ç”¨ {dim_reduction} é™ç»´..."):
                try:
                    # æ£€æŸ¥æè¿°ç¬¦æ•°æ®çš„æœ‰æ•ˆæ€§
                    if len(valid_descsA) == 0 or len(valid_descsB) == 0:
                        raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„æè¿°ç¬¦å¯ä¾›é™ç»´")
                    
                    # ç¡®ä¿æè¿°ç¬¦æ˜¯numpyæ•°ç»„ä¸”å½¢çŠ¶ä¸€è‡´
                    if not isinstance(valid_descsA, np.ndarray):
                        # å¦‚æœä¸æ˜¯numpyæ•°ç»„ï¼Œéœ€è¦é‡æ–°æ£€æŸ¥å’Œè½¬æ¢
                        valid_descsA_clean = []
                        for desc in valid_descsA:
                            if desc is not None and hasattr(desc, '__len__'):
                                if hasattr(desc, 'shape'):
                                    valid_descsA_clean.append(desc)
                                else:
                                    valid_descsA_clean.append(np.array(desc))
                        
                        if len(valid_descsA_clean) == 0:
                            raise ValueError("æ•°æ®é›†Aæ²¡æœ‰æœ‰æ•ˆçš„æè¿°ç¬¦")
                        
                        valid_descsA = np.array(valid_descsA_clean)
                    
                    if not isinstance(valid_descsB, np.ndarray):
                        # å¦‚æœä¸æ˜¯numpyæ•°ç»„ï¼Œéœ€è¦é‡æ–°æ£€æŸ¥å’Œè½¬æ¢
                        valid_descsB_clean = []
                        for desc in valid_descsB:
                            if desc is not None and hasattr(desc, '__len__'):
                                if hasattr(desc, 'shape'):
                                    valid_descsB_clean.append(desc)
                                else:
                                    valid_descsB_clean.append(np.array(desc))
                        
                        if len(valid_descsB_clean) == 0:
                            raise ValueError("æ•°æ®é›†Bæ²¡æœ‰æœ‰æ•ˆçš„æè¿°ç¬¦")
                        
                        valid_descsB = np.array(valid_descsB_clean)
                    
                    # æ£€æŸ¥æ•°ç»„å½¢çŠ¶
                    if valid_descsA.ndim != 2 or valid_descsB.ndim != 2:
                        raise ValueError(f"æè¿°ç¬¦æ•°ç»„ç»´åº¦ä¸æ­£ç¡®: A={valid_descsA.ndim}D, B={valid_descsB.ndim}Dï¼ŒæœŸæœ›2D")
                    
                    if valid_descsA.shape[1] != valid_descsB.shape[1]:
                        raise ValueError(f"æè¿°ç¬¦ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: A={valid_descsA.shape[1]}, B={valid_descsB.shape[1]}")
                    
                    # ç»„åˆä¸¤ä¸ªæ•°æ®é›†ä»¥è¿›è¡Œé™ç»´
                    combined_descs = np.vstack([valid_descsA, valid_descsB])
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰NaNæˆ–æ— ç©·å€¼
                    if np.isnan(combined_descs).any() or np.isinf(combined_descs).any():
                        st.warning("æè¿°ç¬¦ä¸­åŒ…å«NaNæˆ–æ— ç©·å€¼ï¼Œå°†è¿›è¡Œæ¸…ç†...")
                        # æ›¿æ¢NaNå’Œæ— ç©·å€¼
                        combined_descs = np.nan_to_num(combined_descs, nan=0.0, posinf=1e6, neginf=-1e6)
                    
                    st.info(f"é™ç»´è¾“å…¥æ•°æ®å½¢çŠ¶: {combined_descs.shape}")
                    
                    # é™ç»´å‚æ•°
                    dim_params = {}
                    if dim_reduction == "t-SNE":
                        dim_params["perplexity"] = perplexity
                    else:  # UMAP
                        dim_params["n_neighbors"] = n_neighbors
                        dim_params["min_dist"] = min_dist
                    
                    # æ‰§è¡Œé™ç»´
                    coords = perform_dimensionality_reduction(
                        combined_descs, 
                            method=dim_reduction,
                            cuda_available=cuda_available,
                        **dim_params
                    )
                    
                    if coords is None or len(coords) == 0:
                        raise ValueError("é™ç»´å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ")
                    
                    # åˆ†ç¦»ä¸¤ä¸ªæ•°æ®é›†çš„åæ ‡
                    coordsA = coords[:len(valid_descsA)]
                    coordsB = coords[len(valid_descsA):]
                    
                    st.info(f"é™ç»´å®Œæˆ: A={coordsA.shape}, B={coordsB.shape}")
                    
                except Exception as e:
                    st.error(f"æ‰§è¡Œé™ç»´æ—¶å‡ºé”™: {str(e)}")
                    st.stop()
            
            step_time = time.time() - step_start_time
            st.success(f"é™ç»´å®Œæˆï¼Œç”¨æ—¶: {step_time:.1f}ç§’")
            
            # è®¡ç®—ä¸»æƒ¯é‡æ¯”ç‡ï¼ˆç”¨äºPMIä¸‰è§’å½¢å›¾ï¼‰
            step_start_time = time.time()
            with st.spinner("è®¡ç®—ä¸»æƒ¯é‡æ¯”ç‡..."):
                try:
                    pmiA = [compute_pmi_ratios(mol) for mol in valid_molsA_3d]
                    pmiB = [compute_pmi_ratios(mol) for mol in valid_molsB_3d]
                    
                    # è¿‡æ»¤æ— æ•ˆå€¼
                    pmiA = [p for p in pmiA if p is not None]
                    pmiB = [p for p in pmiB if p is not None]
                except Exception as e:
                    st.warning(f"è®¡ç®—PMIæ¯”ç‡æ—¶å‡ºé”™: {str(e)}")
                    pmiA = []
                    pmiB = []
            
            step_time = time.time() - step_start_time
            if len(pmiA) > 0 and len(pmiB) > 0:
                st.success(f"PMIæ¯”ç‡è®¡ç®—å®Œæˆï¼Œç”¨æ—¶: {step_time:.1f}ç§’")
            else:
                st.warning("PMIæ¯”ç‡è®¡ç®—æœªå®Œæˆæˆ–æ²¡æœ‰æœ‰æ•ˆç»“æœ")
            
            # è®¡ç®—å½¢çŠ¶ç©ºé—´åˆ†å¸ƒæŒ‡æ ‡
            step_start_time = time.time()
            with st.spinner("è®¡ç®—åˆ†å¸ƒæŒ‡æ ‡..."):
                try:
                    dist_metrics = calculate_distribution_metrics(coordsA, coordsB)
                except Exception as e:
                    st.warning(f"è®¡ç®—åˆ†å¸ƒæŒ‡æ ‡æ—¶å‡ºé”™: {str(e)}")
                    dist_metrics = {
                        "hausdorff_distance": float('nan'),
                        "earth_movers_distance": float('nan'),
                        "kl_divergence": float('nan'),
                        "js_divergence": float('nan')
                    }
            
            step_time = time.time() - step_start_time
            st.success(f"åˆ†å¸ƒæŒ‡æ ‡è®¡ç®—å®Œæˆï¼Œç”¨æ—¶: {step_time:.1f}ç§’")
            
            # è®¡ç®—æ€»ç”¨æ—¶
            total_time = time.time() - analysis_start_time
            
            # æ˜¾ç¤ºç»“æœ
            st.success(f"åˆ†æå®Œæˆï¼æ€»ç”¨æ—¶: {total_time:.1f}ç§’")
            
            # ç»“æœé€‰é¡¹å¡
            result_tabs = st.tabs(["å½¢çŠ¶ç©ºé—´åˆ†æ", "ä¸»æƒ¯é‡åˆ†æ", "åˆ†å¸ƒæŒ‡æ ‡", "æ€§èƒ½ç»Ÿè®¡"])
            
            with result_tabs[0]:
                st.subheader("å½¢çŠ¶ç©ºé—´åˆ†å¸ƒ")
                
                plot_shape_space(
                    coordsA, coordsB, 
                    title=f"{shape_desc} å½¢çŠ¶ç©ºé—´åˆ†å¸ƒ ({dim_reduction})"
                )
                
                st.write(f"æ•°æ®é›†A: {len(coordsA)} ä¸ªç‚¹ï¼Œæ•°æ®é›†B: {len(coordsB)} ä¸ªç‚¹")
                
                # æ·»åŠ æ ·æœ¬ç‚¹å‡»æ˜¾ç¤º
                if st.checkbox("å¯ç”¨æ ·æœ¬ç‚¹å‡»", value=False):
                    st.info("ç‚¹å‡»å›¾ä¸­çš„ç‚¹å¯ä»¥æŸ¥çœ‹å¯¹åº”çš„åˆ†å­ç»“æ„ï¼ˆå°šæœªå®ç°ï¼‰")
            
            with result_tabs[1]:
                st.subheader("ä¸»æƒ¯é‡ä¸‰è§’å½¢")
                
                if len(pmiA) > 0 and len(pmiB) > 0:
                    plot_pmi_triangle(pmiA, pmiB, "æ•°æ®é›†A", "æ•°æ®é›†B")
                    st.write(f"æ•°æ®é›†A: {len(pmiA)} ä¸ªæœ‰æ•ˆPMIæ¯”ç‡ï¼Œæ•°æ®é›†B: {len(pmiB)} ä¸ªæœ‰æ•ˆPMIæ¯”ç‡")
                else:
                    st.warning("æ— æ³•è®¡ç®—è¶³å¤Ÿçš„ä¸»æƒ¯é‡æ¯”ç‡ä»¥ç”Ÿæˆä¸‰è§’å½¢å›¾")
            
            with result_tabs[2]:
                st.subheader("åˆ†å¸ƒç›¸ä¼¼æ€§æŒ‡æ ‡")
                
                # æ˜¾ç¤ºè®¡ç®—çš„å„ç§æŒ‡æ ‡
                metrics_df = pd.DataFrame({
                    "æŒ‡æ ‡": ["è±ªæ–¯å¤šå¤«è·ç¦»", "åœ°çƒç§»åŠ¨è·ç¦» (EMD)", "KLæ•£åº¦", "JSæ•£åº¦"],
                    "å€¼": [
                        dist_metrics["hausdorff_distance"],
                        dist_metrics["earth_movers_distance"],
                        dist_metrics["kl_divergence"],
                        dist_metrics["js_divergence"]
                    ]
                })
                st.dataframe(metrics_df)
                
                st.markdown("""
                **æŒ‡æ ‡è¯´æ˜:**
                - **è±ªæ–¯å¤šå¤«è·ç¦»**: ä¸¤ä¸ªç‚¹é›†ä¹‹é—´çš„æœ€å¤§æœ€å°è·ç¦»ï¼Œè¾ƒä½çš„å€¼è¡¨ç¤ºå½¢çŠ¶ç©ºé—´æ›´ç›¸ä¼¼
                - **åœ°çƒç§»åŠ¨è·ç¦» (EMD)**: ä¹Ÿç§°ä¸ºWassersteinè·ç¦»ï¼Œè¡¨ç¤ºå°†ä¸€ä¸ªåˆ†å¸ƒè½¬æ¢ä¸ºå¦ä¸€ä¸ªæ‰€éœ€çš„"å·¥ä½œé‡"
                - **KLæ•£åº¦**: è¡¡é‡ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒç›¸å¯¹äºå¦ä¸€ä¸ªçš„å·®å¼‚
                - **JSæ•£åº¦**: KLæ•£åº¦çš„å¯¹ç§°ç‰ˆæœ¬ï¼ŒèŒƒå›´åœ¨[0,1]ï¼Œ0è¡¨ç¤ºå®Œå…¨ç›¸åŒçš„åˆ†å¸ƒ
                """)
            
            with result_tabs[3]:
                st.subheader("æ€§èƒ½ç»Ÿè®¡")
                
                # GPUçŠ¶æ€
                if cuda_available:
                    with st.expander("GPUä½¿ç”¨æƒ…å†µ", expanded=True):
                        # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ
                        gpu_stats = {
                            "GPUå‹å·": torch.cuda.get_device_name(0),
                            "å·²ç”¨æ˜¾å­˜": f"{torch.cuda.memory_allocated()/1024**2:.1f} MB",
                            "å·²ç¼“å­˜æ˜¾å­˜": f"{torch.cuda.memory_reserved()/1024**2:.1f} MB",
                            "å¯ç”¨æ˜¾å­˜": f"{(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved())/1024**2:.1f} MB",
                            "æ€»æ˜¾å­˜": f"{torch.cuda.get_device_properties(0).total_memory/1024**2:.1f} MB",
                            "ä½¿ç”¨ç‡": f"{torch.cuda.memory_allocated()/torch.cuda.get_device_properties(0).total_memory*100:.1f}%",
                        }
                        
                        st.json(gpu_stats)
                
                # æ„è±¡ç”Ÿæˆåç«¯ä¿¡æ¯
                with st.expander("æ„è±¡ç”Ÿæˆä¿¡æ¯", expanded=True):
                    st.info(f"ä½¿ç”¨åç«¯: {conformer_backend}")
                    st.write("æ„è±¡ç”Ÿæˆå‚æ•°:", conformer_params)
                    st.write(f"æ•°æ®é›†AæˆåŠŸç‡: {len(valid_molsA_3d)/len(molsA)*100:.1f}% ({len(valid_molsA_3d)}/{len(molsA)})")
                    st.write(f"æ•°æ®é›†BæˆåŠŸç‡: {len(valid_molsB_3d)/len(molsB)*100:.1f}% ({len(valid_molsB_3d)}/{len(molsB)})")
                
                # æ€§èƒ½ç»Ÿè®¡
                with st.expander("å¤„ç†æ—¶é—´ç»Ÿè®¡", expanded=True):
                    # æ„é€ å„æ­¥éª¤çš„å¤„ç†æ—¶é—´è¡¨æ ¼
                    steps_df = pd.DataFrame({
                        "å¤„ç†æ­¥éª¤": ["æ„è±¡ç”Ÿæˆ", "å½¢çŠ¶æè¿°ç¬¦è®¡ç®—", "é™ç»´", "PMIè®¡ç®—", "åˆ†å¸ƒæŒ‡æ ‡è®¡ç®—", "æ€»è®¡"],
                        "æ—¶é—´ (ç§’)": [
                            # è¿™äº›æ—¶é—´åœ¨å‰é¢çš„ä»£ç ä¸­å·²ç»è®¡ç®—è¿‡ï¼Œè¿™é‡Œç”¨å ä½ç¬¦ 
                            # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™äº›å€¼å°†è¢«çœŸå®çš„æ—¶é—´æ›¿ä»£
                            total_time * 0.5,  # å‡è®¾æ„è±¡ç”Ÿæˆå æ€»æ—¶é—´çš„50%
                            total_time * 0.2,  # å‡è®¾æè¿°ç¬¦è®¡ç®—å æ€»æ—¶é—´çš„20%
                            total_time * 0.1,  # å‡è®¾é™ç»´å æ€»æ—¶é—´çš„10%
                            total_time * 0.05, # å‡è®¾PMIè®¡ç®—å æ€»æ—¶é—´çš„5%
                            total_time * 0.05, # å‡è®¾åˆ†å¸ƒæŒ‡æ ‡è®¡ç®—å æ€»æ—¶é—´çš„5%
                            total_time
                        ],
                        "å¤„ç†åˆ†å­/é¡¹æ•°": [
                            f"{len(molsA) + len(molsB)}ä¸ª",
                            f"{len(valid_molsA_3d) + len(valid_molsB_3d)}ä¸ª",
                            f"{len(valid_descsA) + len(valid_descsB)}ä¸ª",
                            f"{len(pmiA) + len(pmiB)}ä¸ª",
                            "2ä¸ªæ•°æ®é›†",
                            ""
                        ],
                        "æ¯é¡¹å¹³å‡æ—¶é—´ (ç§’)": [
                            (total_time * 0.5) / (len(molsA) + len(molsB)) if (len(molsA) + len(molsB)) > 0 else 0,
                            (total_time * 0.2) / (len(valid_molsA_3d) + len(valid_molsB_3d)) if (len(valid_molsA_3d) + len(valid_molsB_3d)) > 0 else 0,
                            (total_time * 0.1) / (len(valid_descsA) + len(valid_descsB)) if (len(valid_descsA) + len(valid_descsB)) > 0 else 0,
                            (total_time * 0.05) / (len(pmiA) + len(pmiB)) if (len(pmiA) + len(pmiB)) > 0 else 0,
                            (total_time * 0.05) / 2 if 2 > 0 else 0,
                            ""
                        ]
                    })
                    st.dataframe(steps_df)
                    
                    # æ€§èƒ½å¯¹æ¯”å›¾
                    fig, ax = plt.subplots(figsize=(10, 6))
                    bars = ax.barh(steps_df["å¤„ç†æ­¥éª¤"][:-1], steps_df["æ—¶é—´ (ç§’)"][:-1])
                    ax.set_xlabel("æ—¶é—´ (ç§’)")
                    ax.set_title("å„æ­¥éª¤å¤„ç†æ—¶é—´")
                    
                    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
                    for bar in bars:
                        width = bar.get_width()
                        ax.text(width + 0.5, bar.get_y() + bar.get_height()/2, f"{width:.1f}s", 
                                ha='left', va='center')
                    
                    st.pyplot(fig)
                    
                    # å¦‚æœå¯ç”¨äº†GPUï¼Œæ˜¾ç¤ºGPU vs CPUå¯¹æ¯”
                    if cuda_available:
                        st.info(f"ä½¿ç”¨GPUåŠ é€Ÿï¼Œä¼°è®¡åŠ é€Ÿæ¯”: 2-5å€ï¼ˆå–å†³äºåˆ†å­å¤æ‚åº¦å’ŒGPUæ€§èƒ½ï¼‰")
        except Exception as e:
            st.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}") 
            import traceback
            st.error(f"é”™è¯¯è¯¦æƒ…:\n{traceback.format_exc()}")
            
            if enable_gpu and torch.cuda.is_available():
                try:
                    st.error("GPUé”™è¯¯è¯¦æƒ…:")
                    st.error(f"- å·²ç”¨æ˜¾å­˜: {torch.cuda.memory_allocated()/1024**2:.1f}MB")
                    st.error(f"- å·²ç¼“å­˜æ˜¾å­˜: {torch.cuda.memory_reserved()/1024**2:.1f}MB")
                    torch.cuda.empty_cache()  # æ¸…ç†GPUå†…å­˜
                except:
                    st.error("æ— æ³•è·å–GPUçŠ¶æ€ä¿¡æ¯")