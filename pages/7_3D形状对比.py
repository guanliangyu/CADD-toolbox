"""
åˆ†å­åº“ä»£è¡¨æ€§å­é›†é€‰æ‹©ç³»ç»Ÿ - 3Då½¢çŠ¶å¯¹æ¯”é¡µé¢
"""
import os

# æŠ‘åˆ¶ TensorFlow å’Œ CUDA è­¦å‘Š
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 3 = ERRORï¼Œä»…æ˜¾ç¤ºé”™è¯¯
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
# æŠ‘åˆ¶CUDAç›¸å…³é‡å¤æ³¨å†Œè­¦å‘Š
os.environ['TF_CUDNN_DETERMINISTIC'] = '1'
# è®¾ç½®TFæ—¥å¿—çº§åˆ«
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
# ç¦ç”¨è­¦å‘Š
os.environ['TF_ENABLE_DEPRECATION_WARNINGS'] = 'false'

# æŠ‘åˆ¶ PyTorch è­¦å‘Š (å¯é€‰)
import warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

import sys
import random
import numpy as np
import pandas as pd
import streamlit as st
from rdkit import Chem
from rdkit.Chem import AllChem, rdMolDescriptors
import matplotlib.pyplot as plt
import torch
import seaborn as sns
from sklearn.manifold import TSNE
import umap
import time
from scipy.stats import wasserstein_distance
from sklearn.neighbors import KernelDensity

# å°è¯•å¯¼å…¥GPUåŠ é€Ÿç›¸å…³çš„åº“
try:
    import cupy as cp
    from cuml.manifold import TSNE as cuTSNE
    HAS_CUML = True
except ImportError:
    HAS_CUML = False
    warnings.warn("cuMLæœªå®‰è£…ï¼Œå°†ä½¿ç”¨CPUç‰ˆæœ¬çš„t-SNE")

try:
    import openmm as mm
    import openmm.app as app
    from openmm import unit
    HAS_OPENMM = True
except ImportError:
    HAS_OPENMM = False
    warnings.warn("OpenMMæœªå®‰è£…ï¼Œå°†ä¸èƒ½ä½¿ç”¨OpenMMåç«¯")
    
try:
    import torchani
    HAS_TORCHANI = True
    
    # å°è¯•æ£€æµ‹ CUDA æ”¯æŒï¼Œä½¿ç”¨æ›´å®‰å…¨çš„æ–¹æ³•
    try:
        # ä»…æ£€æŸ¥æ˜¯å¦èƒ½åˆ›å»ºæ¨¡å‹å¹¶ç§»è‡³ GPU
        if torch.cuda.is_available():
            # å°è¯•åŠ è½½æ¨¡å‹åˆ° GPUï¼Œè¿™æ¯”æ£€æŸ¥ torch.classes.torchani æ›´å¯é 
            model = torchani.models.ANI1x(model_index=0)
            device = torch.device('cuda')
            model = model.to(device)
            # å¦‚æœèƒ½èµ°åˆ°è¿™æ­¥ï¼Œè¯´æ˜æ”¯æŒ CUDA
            HAS_TORCHANI_CUDA = True
            del model  # æ¸…ç†
            torch.cuda.empty_cache()
        else:
            HAS_TORCHANI_CUDA = False
    except Exception:
        HAS_TORCHANI_CUDA = False
        warnings.warn("TorchANI CUDA åŠ é€Ÿæ£€æµ‹å¤±è´¥ï¼Œå°†ä½¿ç”¨ CPU ç‰ˆæœ¬")
except ImportError:
    HAS_TORCHANI = False
    HAS_TORCHANI_CUDA = False
    warnings.warn("TorchANI æœªå®‰è£…ï¼Œå°†ä¸èƒ½ä½¿ç”¨ TorchANI åç«¯")
    
# DeepChemå’ŒTensorFlow
HAS_DEEPCHEM = False
HAS_DEEPCHEM_GPU = False
try:
    # é˜²æ­¢TensorFlowäº§ç”Ÿä¸å¿…è¦çš„æ—¥å¿—
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # ä»…æ˜¾ç¤ºERROR
    try:
        import tensorflow as tf
        # å®‰å…¨åœ°æ£€æŸ¥GPU
        physical_devices = tf.config.list_physical_devices('GPU')
        tf_gpu_available = len(physical_devices) > 0
        # è®¾ç½®å†…å­˜å¢é•¿
        if tf_gpu_available:
            for device in physical_devices:
                try:
                    tf.config.experimental.set_memory_growth(device, True)
                except:
                    pass
    except ImportError:
        tf = None
        tf_gpu_available = False
    
    # å°è¯•å¯¼å…¥DeepChem
    try:
        import deepchem as dc
        HAS_DEEPCHEM = True
        HAS_DEEPCHEM_GPU = tf_gpu_available
    except ImportError:
        warnings.warn("DeepChemæœªå®‰è£…ï¼Œå°†ä¸èƒ½ä½¿ç”¨DeepChemåç«¯")
except Exception as e:
    warnings.warn(f"åˆå§‹åŒ–TensorFlow/DeepChemæ—¶å‡ºé”™: {str(e)}")

try:
    import clara.conformer as clara_conf
    import clara.molecule as clara_mol
    HAS_CLARA = True
except ImportError:
    HAS_CLARA = False
    warnings.warn("NVIDIA Claraæœªå®‰è£…ï¼Œå°†ä¸èƒ½ä½¿ç”¨Claraåç«¯")

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(script_dir)

# è®¾ç½®Streamlité…ç½®
st.set_page_config(
    page_title="åˆ†å­åº“ä»£è¡¨æ€§å­é›†é€‰æ‹©ç³»ç»Ÿ - 3Då½¢çŠ¶å¯¹æ¯”",
    page_icon="ğŸ§¬",
    layout="wide"
)

# ç¦ç”¨æ–‡ä»¶ç›‘è§†å™¨ä»¥é¿å…PyTorchç›¸å…³é”™è¯¯
if hasattr(st, 'server'):
    st.server.server.server_options["watcher_type"] = "none"

# æ¸…ç†ç¼“å­˜ï¼ˆä½¿ç”¨æ–°çš„æ–¹æ³•ï¼‰
if st.session_state.get('clear_cache', False):
    st.cache_data.clear()
    st.cache_resource.clear()
    st.session_state.clear_cache = False

st.title("3Då½¢çŠ¶å¯¹æ¯”")

col1, col2 = st.columns(2)

with col1:
    st.subheader("æ•°æ®é›†A")
    fileA = st.file_uploader("ä¸Šä¼ ç¬¬ä¸€ä¸ªCSVæ–‡ä»¶", type="csv")
    
with col2:
    st.subheader("æ•°æ®é›†B")
    fileB = st.file_uploader("ä¸Šä¼ ç¬¬äºŒä¸ªCSVæ–‡ä»¶", type="csv")

# ç»Ÿä¸€å‚æ•°è®¾ç½®
st.subheader("å‚æ•°è®¾ç½®")
# åˆ›å»ºä¸»è¦çš„è®¾ç½®é€‰é¡¹å¡
main_tabs = st.tabs(["æ•°æ®è®¾ç½®", "æ„è±¡ç”Ÿæˆè®¾ç½®", "åˆ†æè®¾ç½®", "GPUè®¾ç½®"])

# æ•°æ®è®¾ç½®é€‰é¡¹å¡
with main_tabs[0]:
    col1, col2 = st.columns(2)
    with col1:
        smiles_col = st.text_input("SMILESåˆ—å", value="SMILES")
        max_samples = st.number_input("æ¯ä¸ªæ•°æ®é›†çš„æœ€å¤§æ ·æœ¬æ•°", 100, 5000, 500)
    
    with col2:
        shape_desc = st.selectbox("å½¢çŠ¶æè¿°ç¬¦ç±»å‹", ["USR", "USRCAT"], 
                                help="USR: è¶…å¿«å½¢çŠ¶è¯†åˆ«ï¼›USRCAT: åŒ…å«åŸå­ç±»å‹ä¿¡æ¯çš„USR")
        normalize_desc = st.checkbox("æ ‡å‡†åŒ–æè¿°ç¬¦", value=True, 
                                   help="åº”ç”¨æ ‡å‡†åŒ–ä»¥å¹³è¡¡ä¸åŒå°ºåº¦çš„ç‰¹å¾")

# æ„è±¡ç”Ÿæˆè®¾ç½®é€‰é¡¹å¡
with main_tabs[1]:
    # æ„è±¡ç”Ÿæˆå¼•æ“é€‰æ‹©
    available_backends = ["rdkit"]
    if HAS_OPENMM:
        available_backends.append("openmm")
    if HAS_TORCHANI:
        available_backends.append("torchani")
    if HAS_DEEPCHEM:
        available_backends.append("deepchem")
    if HAS_CLARA:
        available_backends.append("clara")
    available_backends.insert(0, "auto")
    
    conformer_backend = st.selectbox(
        "3Dæ„è±¡ç”Ÿæˆåç«¯",
        available_backends,
        help="é€‰æ‹©ç”¨äºç”Ÿæˆ3Dæ„è±¡çš„è®¡ç®—åç«¯"
    )
    
    # åŸºç¡€è®¾ç½®
    col1, col2, col3 = st.columns(3)
    with col1:
        max_attempts = st.slider("æ„è±¡ç”Ÿæˆæœ€å¤§å°è¯•æ¬¡æ•°", 10, 100, 50)
        add_hydrogens = st.checkbox("æ·»åŠ æ°¢åŸå­", value=True, 
                                  help="åœ¨æ„è±¡ç”Ÿæˆå‰æ·»åŠ æ°¢åŸå­")
    
    with col2:
        use_mmff = st.checkbox("ä½¿ç”¨åŠ›åœºä¼˜åŒ–", value=True, 
                              help="ä½¿ç”¨åˆ†å­åŠ›åœºä¼˜åŒ–æ„è±¡")
        energy_iter = st.slider("èƒ½é‡ä¼˜åŒ–è¿­ä»£æ¬¡æ•°", 0, 500, 200, 
                              disabled=not use_mmff)
    
    with col3:
        if conformer_backend == "auto":
            st.info("è‡ªåŠ¨æ¨¡å¼ï¼šç³»ç»Ÿå°†æ ¹æ®åˆ†å­å¤§å°å’Œå¯ç”¨èµ„æºé€‰æ‹©æœ€ä½³åç«¯")
            auto_select_info = """
            â€¢ å°åˆ†å­ (<50åŸå­)ï¼šä¼˜å…ˆä½¿ç”¨TorchANI
            â€¢ ä¸­ç­‰åˆ†å­ (50-100åŸå­)ï¼šä¼˜å…ˆä½¿ç”¨DeepChem
            â€¢ å¤§åˆ†å­ (>100åŸå­)ï¼šä¼˜å…ˆä½¿ç”¨OpenMMæˆ–Clara
            â€¢ å¦‚æœæ²¡æœ‰å¯ç”¨GPUï¼šä½¿ç”¨RDKit
            """
            st.markdown(auto_select_info)
    
    # åˆ›å»ºç‰¹å®šåç«¯è®¾ç½®é€‰é¡¹å¡
    backend_tabs = st.tabs(["OpenMM", "TorchANI", "DeepChem", "Clara"])
    
    # OpenMMè®¾ç½®
    with backend_tabs[0]:
        if HAS_OPENMM and (conformer_backend in ["openmm", "auto"]):
            st.write("OpenMMè®¾ç½®")
            openmm_forcefield = st.selectbox(
                "åŠ›åœº",
                ["amber14-all", "charmm36", "openff-2.0.0"],
                help="é€‰æ‹©ç”¨äºä¼˜åŒ–çš„åŠ›åœº"
            )
            openmm_platform = st.selectbox(
                "è®¡ç®—å¹³å°",
                ["CUDA", "OpenCL", "CPU"],
                help="é€‰æ‹©OpenMMçš„è®¡ç®—å¹³å°"
            )
        else:
            st.info("OpenMMä¸å¯ç”¨æˆ–æœªé€‰æ‹©")
    
    # TorchANIè®¾ç½®
    with backend_tabs[1]:
        if HAS_TORCHANI and (conformer_backend in ["torchani", "auto"]):
            st.write("TorchANIè®¾ç½®")
            torchani_model = st.selectbox(
                "ç¥ç»ç½‘ç»œæ¨¡å‹",
                ["ANI2x", "ANI1x", "ANI1ccx"],
                help="é€‰æ‹©TorchANIçš„ç¥ç»ç½‘ç»œæ¨¡å‹"
            )
            optimization_steps = st.slider("ä¼˜åŒ–æ­¥æ•°", 50, 500, 100)
        else:
            st.info("TorchANIä¸å¯ç”¨æˆ–æœªé€‰æ‹©")
    
    # DeepChemè®¾ç½®
    with backend_tabs[2]:
        if HAS_DEEPCHEM and (conformer_backend in ["deepchem", "auto"]):
            st.write("DeepChemè®¾ç½®")
            deepchem_model = st.selectbox(
                "æ¨¡å‹ç±»å‹",
                ["mpnn", "schnet", "cgcnn"],
                help="é€‰æ‹©DeepChemçš„åˆ†å­è¡¨ç¤ºæ¨¡å‹"
            )
            use_mixed_precision = st.checkbox(
                "ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ",
                value=True,
                help="å¯ç”¨FP16æ··åˆç²¾åº¦ä»¥æé«˜æ€§èƒ½"
            )
            batch_size_dc = st.slider(
                "æ‰¹å¤„ç†å¤§å°",
                16, 256, 64,
                help="DeepChemçš„æ‰¹å¤„ç†å¤§å°"
            )
        else:
            st.info("DeepChemä¸å¯ç”¨æˆ–æœªé€‰æ‹©")
    
    # Claraè®¾ç½®
    with backend_tabs[3]:
        if HAS_CLARA and (conformer_backend in ["clara", "auto"]):
            st.write("NVIDIA Claraè®¾ç½®")
            clara_force_field = st.selectbox(
                "åŠ›åœº",
                ["MMFF94s", "UFF", "GAFF"],
                help="é€‰æ‹©Claraçš„åŠ›åœº"
            )
            clara_precision = st.selectbox(
                "è®¡ç®—ç²¾åº¦",
                ["mixed", "fp32", "fp16"],
                help="é€‰æ‹©è®¡ç®—ç²¾åº¦"
            )
            clara_num_conformers = st.slider(
                "æ„è±¡æ•°é‡",
                1, 10, 1,
                help="ç”Ÿæˆçš„æ„è±¡æ•°é‡"
            )
            clara_energy_threshold = st.slider(
                "èƒ½é‡é˜ˆå€¼(kcal/mol)",
                0.1, 10.0, 1.0,
                help="èƒ½é‡ç­›é€‰é˜ˆå€¼"
            )
        else:
            st.info("NVIDIA Claraä¸å¯ç”¨æˆ–æœªé€‰æ‹©")

# åˆ†æè®¾ç½®é€‰é¡¹å¡
with main_tabs[2]:
    col1, col2 = st.columns(2)
    with col1:
        dim_reduction = st.selectbox("é™ç»´æ–¹æ³•", ["t-SNE", "UMAP"])
        if dim_reduction == "t-SNE":
            perplexity = st.slider("t-SNEå›°æƒ‘åº¦", 5, 50, 30)
        else:
            n_neighbors = st.slider("UMAPé‚»å±…æ•°", 5, 50, 15)
            min_dist = st.slider("UMAPæœ€å°è·ç¦»", 0.01, 0.99, 0.1, 0.01)
    
    with col2:
        st.write("å¯è§†åŒ–è®¾ç½®")
        plot_height = st.slider("å›¾è¡¨é«˜åº¦", 400, 1000, 600)
        plot_width = st.slider("å›¾è¡¨å®½åº¦", 400, 1000, 800)
        color_scheme = st.selectbox("é…è‰²æ–¹æ¡ˆ", 
                           ["viridis", "plasma", "inferno", "magma", "cividis"])

# GPUè®¾ç½®é€‰é¡¹å¡
with main_tabs[3]:
    col1, col2 = st.columns(2)
    with col1:
        enable_gpu = st.checkbox("å¯ç”¨GPUåŠ é€Ÿ", value=True, 
                                help="ä½¿ç”¨GPUåŠ é€Ÿè®¡ç®—")
        auto_batch = st.checkbox("è‡ªåŠ¨æ‰¹å¤„ç†å¤§å°", value=True, 
                                 help="æ ¹æ®GPUå†…å­˜è‡ªåŠ¨è°ƒæ•´æ‰¹å¤„ç†å¤§å°")
        batch_size = st.slider("æ‰¹å¤„ç†å¤§å°", 10, 500, 50, 
                             disabled=auto_batch)
    
    with col2:
        if enable_gpu:
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                gpu_mem_total = torch.cuda.get_device_properties(0).total_memory / 1024**2
                
                st.success(f"âœ… GPUå¯ç”¨: {gpu_name}")
                st.info(f"æ€»æ˜¾å­˜: {gpu_mem_total:.1f} MB")
                
                # GPUä½¿ç”¨ç­–ç•¥
                gpu_strategy = st.selectbox(
                    "GPUä½¿ç”¨ç­–ç•¥",
                    ["å¹³è¡¡", "æ€§èƒ½ä¼˜å…ˆ", "å†…å­˜ä¼˜å…ˆ"],
                    help="å¹³è¡¡ï¼šå¹³è¡¡é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨ï¼›æ€§èƒ½ä¼˜å…ˆï¼šæ›´å¿«ä½†ä½¿ç”¨æ›´å¤šå†…å­˜ï¼›å†…å­˜ä¼˜å…ˆï¼šèŠ‚çœå†…å­˜ä½†è¾ƒæ…¢"
                )
                
                # GPUå†…å­˜é™åˆ¶
                gpu_mem_limit = st.slider(
                    "GPUå†…å­˜ä½¿ç”¨é™åˆ¶ (%)",
                    10, 95, 80,
                    help="é™åˆ¶GPUå†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯”ä»¥é˜²æ­¢å´©æºƒ"
                )
            else:
                st.error("âŒ æœªæ£€æµ‹åˆ°å¯ç”¨çš„GPU")
                st.info("å°†ä½¿ç”¨CPUè¿›è¡Œè®¡ç®—ï¼Œé€Ÿåº¦å¯èƒ½è¾ƒæ…¢")
        else:
            st.info("GPUåŠ é€Ÿå·²ç¦ç”¨ï¼Œå°†ä½¿ç”¨CPUè¿›è¡Œè®¡ç®—")

def initialize_cuda():
    """åˆå§‹åŒ–CUDAè®¾å¤‡å¹¶è¿”å›è®¾å¤‡ä¿¡æ¯"""
    try:
        cuda_available = torch.cuda.is_available()
        device = torch.device("cuda" if cuda_available else "cpu")
        
        if cuda_available:
            torch.cuda.empty_cache()
            gpu_name = torch.cuda.get_device_name(0)
            gpu_mem_total = torch.cuda.get_device_properties(0).total_memory / 1024**2
            gpu_mem_alloc = torch.cuda.memory_allocated(0) / 1024**2
            gpu_mem_cached = torch.cuda.memory_reserved(0) / 1024**2
            
            st.sidebar.success("âœ… CUDAå¯ç”¨ï¼Œå°†ä½¿ç”¨GPUåŠ é€Ÿ")
            st.sidebar.info(
                f"GPUä¿¡æ¯:\n"
                f"- è®¾å¤‡: {gpu_name}\n"
                f"- æ€»æ˜¾å­˜: {gpu_mem_total:.1f}MB\n"
                f"- å·²åˆ†é…: {gpu_mem_alloc:.1f}MB\n"
                f"- å·²ç¼“å­˜: {gpu_mem_cached:.1f}MB"
            )
        else:
            st.sidebar.info("â„¹ï¸ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®¡ç®—")
        
        return cuda_available, device
    except Exception as e:
        st.sidebar.error(f"GPUåˆå§‹åŒ–é”™è¯¯: {str(e)}")
        return False, torch.device("cpu")

def load_smiles(file, smiles_col='SMILES'):
    """ä»CSVè¯»å–SMILESå¹¶è½¬æ¢ä¸ºRDKit Molå¯¹è±¡"""
    try:
        df = pd.read_csv(file)
        if smiles_col not in df.columns:
            st.error(f"æœªæ‰¾åˆ°SMILESåˆ—: {smiles_col}")
            return None, None
        
        mols = []
        valid_indices = []
        for i, row in df.iterrows():
            smi = str(row[smiles_col]).strip()
            mol = Chem.MolFromSmiles(smi)
            if mol:
                mols.append(mol)
                valid_indices.append(i)
        
        return mols, df.iloc[valid_indices]
    except Exception as e:
        st.error(f"è¯»å–CSVæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return None, None

def generate_3d_conformer(mol, max_attempts=50, use_mmff=True, energy_iter=200, add_hydrogens=True):
    """ç”Ÿæˆ3Dæ„è±¡ï¼Œæ”¯æŒæ›´å¤šé…ç½®é€‰é¡¹"""
    if mol is None:
        return None
    
    try:
        # æ ¹æ®éœ€è¦æ·»åŠ æ°¢åŸå­
        mol_3d = Chem.AddHs(mol) if add_hydrogens else Chem.Mol(mol)
        
        # è®¾ç½®ETKDGå‚æ•°
        ps = AllChem.ETKDGv3()
        ps.maxAttempts = max_attempts
        ps.randomSeed = 42  # è®¾ç½®éšæœºç§å­ä»¥æé«˜å¯é‡å¤æ€§
        ps.numThreads = 0  # ä½¿ç”¨æ‰€æœ‰å¯ç”¨çº¿ç¨‹
        ps.useRandomCoords = True  # ä½¿ç”¨éšæœºåˆå§‹åæ ‡
        
        # åµŒå…¥åˆ†å­
        cid = AllChem.EmbedMolecule(mol_3d, ps)
        if cid < 0:
            return None
        
        # åº”ç”¨åŠ›åœºä¼˜åŒ–
        if use_mmff:
            try:
                # å°è¯•MMFFä¼˜åŒ–
                AllChem.MMFFOptimizeMolecule(mol_3d, maxIters=energy_iter)
            except:
                # å¦‚æœMMFFå¤±è´¥ï¼Œå°è¯•UFF
                AllChem.UFFOptimizeMolecule(mol_3d, maxIters=energy_iter)
        
        # å¦‚æœæ·»åŠ äº†æ°¢åŸå­ï¼Œç°åœ¨å»é™¤å®ƒä»¬
        if add_hydrogens:
            mol_3d = Chem.RemoveHs(mol_3d)
        
        return mol_3d
    except Exception as e:
        st.warning(f"3Dæ„è±¡ç”Ÿæˆå¤±è´¥: {str(e)}")
        return None

def generate_3d_conformer_openmm(mol, forcefield='amber14-all', platform='CUDA', max_iterations=200):
    """ä½¿ç”¨OpenMMç”Ÿæˆå’Œä¼˜åŒ–3Dæ„è±¡"""
    if not HAS_OPENMM:
        return None
    
    try:
        # æ·»åŠ æ°¢åŸå­
        mol = Chem.AddHs(mol)
        
        # ä½¿ç”¨ETKDGç”Ÿæˆåˆå§‹æ„è±¡
        AllChem.EmbedMolecule(mol, AllChem.ETKDGv3())
        
        # è½¬æ¢ä¸ºPDBæ ¼å¼ä»¥ä¾›OpenMMä½¿ç”¨
        pdb_string = Chem.MolToPDBBlock(mol)
        
        # åˆ›å»ºOpenMMç³»ç»Ÿ
        pdb = app.PDBFile(pdb_string)
        forcefield = app.ForceField(f'{forcefield}.xml')
        system = forcefield.createSystem(
            pdb.topology,
            nonbondedMethod=app.NoCutoff,
            constraints=None,
            rigidWater=False
        )
        
        # åˆ›å»ºç§¯åˆ†å™¨
        integrator = mm.LangevinMiddleIntegrator(
            300*unit.kelvin,
            1/unit.picosecond,
            0.002*unit.picoseconds
        )
        
        # ä½¿ç”¨æŒ‡å®šå¹³å°
        platform = mm.Platform.getPlatformByName(platform)
        
        # åˆ›å»ºæ¨¡æ‹Ÿå¯¹è±¡
        simulation = app.Simulation(
            pdb.topology,
            system,
            integrator,
            platform
        )
        
        # è®¾ç½®åˆå§‹åæ ‡
        simulation.context.setPositions(pdb.positions)
        
        # èƒ½é‡æœ€å°åŒ–
        simulation.minimizeEnergy(maxIterations=max_iterations)
        
        # è·å–ä¼˜åŒ–åçš„åæ ‡
        state = simulation.context.getState(getPositions=True)
        positions = state.getPositions(asNumpy=True).value_in_unit(unit.angstrom)
        
        # æ›´æ–°RDKitåˆ†å­çš„åæ ‡
        conf = mol.GetConformer()
        for i, pos in enumerate(positions):
            conf.SetAtomPosition(i, pos)
        
        # ç§»é™¤æ°¢åŸå­
        mol = Chem.RemoveHs(mol)
        
        return mol
    except Exception as e:
        st.warning(f"OpenMMæ„è±¡ç”Ÿæˆå¤±è´¥: {str(e)}")
        return None

def generate_3d_conformer_torchani(mol, model_name='ANI2x', optimization_steps=100, device=None):
    """ä½¿ç”¨TorchANIç”Ÿæˆå’Œä¼˜åŒ–3Dæ„è±¡"""
    if not HAS_TORCHANI:
        return None
    
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    try:
        # åˆå§‹åŒ–æ¨¡å‹
        if model_name == 'ANI2x':
            model = torchani.models.ANI2x(model_index=0).to(device)
        elif model_name == 'ANI1x':
            model = torchani.models.ANI1x(model_index=0).to(device)
        elif model_name == 'ANI1ccx':
            model = torchani.models.ANI1ccx(model_index=0).to(device)
        else:
            model = torchani.models.ANI2x(model_index=0).to(device)
        
        # æ·»åŠ æ°¢åŸå­å¹¶ç”Ÿæˆåˆå§‹æ„è±¡
        mol = Chem.AddHs(mol)
        AllChem.EmbedMolecule(mol, AllChem.ETKDGv3())
        
        # æå–ç‰©ç§å’Œåæ ‡
        species = []
        coordinates = []
        
        for atom in mol.GetAtoms():
            atom_num = atom.GetAtomicNum()
            # æ£€æŸ¥åŸå­ç±»å‹æ˜¯å¦è¢«TorchANIæ”¯æŒ
            if atom_num not in [1, 6, 7, 8, 9, 16, 17]:
                return None  # ä¸æ”¯æŒçš„åŸå­ç±»å‹
            species.append(atom_num)
        
        conf = mol.GetConformer()
        for i in range(mol.GetNumAtoms()):
            pos = conf.GetAtomPosition(i)
            coordinates.append([pos.x, pos.y, pos.z])
        
        # è½¬æ¢ä¸ºå¼ é‡å¹¶ç§»è‡³GPU
        species_tensor = torch.tensor([species], device=device)
        coordinates_tensor = torch.tensor([coordinates], device=device, requires_grad=True)
        
        # æ˜ å°„åˆ°ANIçš„åŸå­ç¼–å·æ ¼å¼
        species_converter = torchani.utils.map_atomic_numbers_to_elements
        species_idx = species_converter(species_tensor)
        
        # ä¼˜åŒ–
        optimizer = torch.optim.LBFGS([coordinates_tensor], max_iter=optimization_steps)
        
        def closure():
            optimizer.zero_grad()
            energy = model((species_idx, coordinates_tensor)).energies
            energy.backward()
            return energy
        
        # è¿è¡Œä¼˜åŒ–
        for _ in range(5):  # å°è¯•å¤šæ¬¡LBFGSè¿­ä»£
            optimizer.step(closure)
        
        # æ›´æ–°RDKitåˆ†å­çš„åæ ‡
        optimized_coords = coordinates_tensor.detach().cpu().numpy()[0]
        for i, pos in enumerate(optimized_coords):
            conf.SetAtomPosition(i, (float(pos[0]), float(pos[1]), float(pos[2])))
        
        # ç§»é™¤æ°¢åŸå­
        mol = Chem.RemoveHs(mol)
        
        return mol
    except Exception as e:
        st.warning(f"TorchANIæ„è±¡ç”Ÿæˆå¤±è´¥: {str(e)}")
        return None

def generate_3d_conformer_deepchem(mol, use_gpu=True, model_type='mpnn'):
    """ä½¿ç”¨DeepChemç”Ÿæˆ3Dæ„è±¡ï¼Œæ”¯æŒGPUåŠ é€Ÿ"""
    if not HAS_DEEPCHEM:
        return None
        
    try:
        # è®¾ç½®GPU/CPUè®¾å¤‡
        if use_gpu and tf.config.list_physical_devices('GPU'):
            with tf.device('/GPU:0'):
                # Initialize conformer generator without problematic arguments
                conf_gen = dc.utils.conformers.ConformerGenerator(
                    max_conformers=1,
                    force_field='mmff94s',
                    pool_multiplier=1,
                    # model_type=model_type,  # Removed
                    # model_dir=None,  # Removed
                    # use_gpu=True  # Removed
                )
                
                # Generate conformers
                mol = conf_gen.generate_conformers(mol)
        else:
            # Fallback to CPU version
            conf_gen = dc.utils.conformers.ConformerGenerator(
                max_conformers=1,
                force_field='mmff94s',
                pool_multiplier=1
            )
            mol = conf_gen.generate_conformers(mol)
            
        return mol
        
    except Exception as e:
        # Add the original model_type to the warning for context
        st.warning(f"DeepChemæ„è±¡ç”Ÿæˆå¤±è´¥(model_type='{model_type}'): {str(e)}")
        return None

def generate_3d_conformer_clara(mol, force_field='MMFF94s', precision='mixed', num_conformers=1, energy_threshold=1.0):
    """ä½¿ç”¨NVIDIA Claraç”Ÿæˆ3Dæ„è±¡"""
    if not HAS_CLARA:
        return None
        
    try:
        # è½¬æ¢ä¸ºClaraåˆ†å­æ ¼å¼
        clara_molecule = clara_mol.Molecule.from_rdkit(mol)
        
        # åˆ›å»ºæ„è±¡ç”Ÿæˆå™¨
        conf_gen = clara_conf.ConformerGenerator(
            num_conformers=num_conformers,
            use_gpu=True,
            energy_minimization=True,
            force_field=force_field,
            precision=precision,
            energy_threshold=energy_threshold
        )
        
        # ç”Ÿæˆæ„è±¡
        conformers = conf_gen.generate(clara_molecule)
        
        # è·å–æœ€ä½èƒ½é‡æ„è±¡
        best_conf = min(conformers, key=lambda x: x.energy)
        
        # è½¬æ¢å›RDKitåˆ†å­
        mol_with_conf = best_conf.to_rdkit()
        
        return mol_with_conf
        
    except Exception as e:
        st.warning(f"NVIDIA Claraæ„è±¡ç”Ÿæˆå¤±è´¥: {str(e)}")
        return None

def generate_3d_conformer_multi(mol, backend='auto', **kwargs):
    """å¤šåç«¯3Dæ„è±¡ç”Ÿæˆå™¨"""
    if mol is None:
        return None
        
    if backend == 'auto':
        # æ ¹æ®åˆ†å­å¤§å°å’Œå¯ç”¨èµ„æºè‡ªåŠ¨é€‰æ‹©åç«¯
        num_atoms = mol.GetNumAtoms()
        gpu_available = torch.cuda.is_available()
        
        # æ£€æŸ¥åç«¯å¯ç”¨æ€§å¹¶è€ƒè™‘GPUæ”¯æŒ
        if HAS_CLARA and gpu_available:
            backend = 'clara'  # ä¼˜å…ˆä½¿ç”¨Clara
        elif num_atoms > 100 and HAS_OPENMM and gpu_available:
            backend = 'openmm'
        elif num_atoms <= 50 and HAS_TORCHANI and (gpu_available or HAS_TORCHANI_CUDA):
            backend = 'torchani'
        elif HAS_DEEPCHEM and HAS_DEEPCHEM_GPU:
            backend = 'deepchem'
        else:
            backend = 'rdkit'  # é»˜è®¤å›é€€åˆ°RDKit
            
        st.info(f"è‡ªåŠ¨é€‰æ‹©æ„è±¡ç”Ÿæˆåç«¯: {backend}")
    
    try:
        if backend == 'clara' and HAS_CLARA:
            st.info("ä½¿ç”¨NVIDIA Claraç”Ÿæˆæ„è±¡...")
            try:
                return generate_3d_conformer_clara(
                    mol,
                    force_field=kwargs.get('force_field', 'MMFF94s'),
                    precision=kwargs.get('precision', 'mixed'),
                    num_conformers=kwargs.get('num_conformers', 1),
                    energy_threshold=kwargs.get('energy_threshold', 1.0)
                )
            except Exception as e:
                st.warning(f"NVIDIA Claraæ„è±¡ç”Ÿæˆå¤±è´¥: {str(e)}ï¼Œå›é€€åˆ°RDKit")
                return generate_3d_conformer(mol, **kwargs)
                
        elif backend == 'openmm' and HAS_OPENMM:
            st.info("ä½¿ç”¨OpenMMç”Ÿæˆæ„è±¡...")
            try:
                # æ£€æµ‹å¯ç”¨çš„OpenMMå¹³å°
                platform = kwargs.get('openmm_platform', 'CUDA')
                available_platforms = [mm.Platform.getPlatform(i).getName() 
                                     for i in range(mm.Platform.getNumPlatforms())]
                
                if platform not in available_platforms:
                    st.warning(f"å¹³å° {platform} ä¸å¯ç”¨ï¼Œå¯ç”¨å¹³å°: {available_platforms}")
                    # å°è¯•è‡ªåŠ¨é€‰æ‹©æœ€ä½³å¹³å°
                    if 'CUDA' in available_platforms:
                        platform = 'CUDA'
                    elif 'OpenCL' in available_platforms:
                        platform = 'OpenCL'
                    else:
                        platform = available_platforms[0]
                    st.info(f"è‡ªåŠ¨é€‰æ‹©å¹³å°: {platform}")
                
                return generate_3d_conformer_openmm(
                    mol,
                    forcefield=kwargs.get('openmm_forcefield', 'amber14-all'),
                    platform=platform,
                    max_iterations=kwargs.get('energy_iter', 200)
                )
            except Exception as e:
                st.warning(f"OpenMMæ„è±¡ç”Ÿæˆå¤±è´¥: {str(e)}ï¼Œå›é€€åˆ°RDKit")
                return generate_3d_conformer(mol, **kwargs)
                
        elif backend == 'torchani' and HAS_TORCHANI:
            st.info("ä½¿ç”¨TorchANIç”Ÿæˆæ„è±¡...")
            try:
                # ç¡®å®šè®¾å¤‡
                device = None
                if torch.cuda.is_available() and HAS_TORCHANI_CUDA:
                    device = torch.device('cuda')
                else:
                    device = torch.device('cpu')
                    st.info("TorchANIä½¿ç”¨CPUè®¡ç®—")
                
                return generate_3d_conformer_torchani(
                    mol,
                    model_name=kwargs.get('torchani_model', 'ANI2x'),
                    optimization_steps=kwargs.get('optimization_steps', 100),
                    device=device
                )
            except Exception as e:
                st.warning(f"TorchANIæ„è±¡ç”Ÿæˆå¤±è´¥: {str(e)}ï¼Œå›é€€åˆ°RDKit")
                return generate_3d_conformer(mol, **kwargs)
                
        elif backend == 'deepchem' and HAS_DEEPCHEM:
            st.info("ä½¿ç”¨DeepChemç”Ÿæˆæ„è±¡...")
            try:
                return generate_3d_conformer_deepchem(
                    mol,
                    use_gpu=HAS_DEEPCHEM_GPU and kwargs.get('use_gpu', True),
                    model_type=kwargs.get('model_type', 'mpnn')
                )
            except Exception as e:
                st.warning(f"DeepChemæ„è±¡ç”Ÿæˆå¤±è´¥: {str(e)}ï¼Œå›é€€åˆ°RDKit")
                return generate_3d_conformer(mol, **kwargs)
                
        else:
            # ä½¿ç”¨RDKitä½œä¸ºé»˜è®¤å’Œå›é€€é€‰é¡¹
            st.info("ä½¿ç”¨RDKitç”Ÿæˆæ„è±¡...")
            return generate_3d_conformer(
                mol,
                max_attempts=kwargs.get('max_attempts', 50),
                use_mmff=kwargs.get('use_mmff', True),
                energy_iter=kwargs.get('energy_iter', 200),
                add_hydrogens=kwargs.get('add_hydrogens', True)
            )
            
    except Exception as e:
        st.error(f"æ„è±¡ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {str(e)}")
        # å°è¯•æœ€åŸºæœ¬çš„æ„è±¡ç”Ÿæˆä½œä¸ºæœ€åçš„å›é€€é€‰é¡¹
        try:
            st.warning("å°è¯•ä½¿ç”¨æœ€åŸºæœ¬çš„RDKitæ„è±¡ç”Ÿæˆ...")
            mol_copy = Chem.Mol(mol)
            Chem.AllChem.EmbedMolecule(mol_copy)
            return mol_copy
        except:
            st.error("æ„è±¡ç”Ÿæˆå®Œå…¨å¤±è´¥")
            return None

# Modified function definition to accept progress_bar
def batch_generate_3d_conformers(mols, progress_bar, status_container, progress_text, backend='auto', batch_size=None, **kwargs):
    """æ‰¹é‡ç”Ÿæˆ3Dæ„è±¡"""
    if not mols:
        return []
    
    # è‡ªåŠ¨é€‰æ‹©æ‰¹å¤„ç†å¤§å°
    if batch_size is None:
        if backend == 'clara' or backend == 'openmm':
            batch_size = min(10, len(mols))  # è¾ƒå°æ‰¹æ¬¡ä»¥å‡å°‘GPUå†…å­˜å‹åŠ›
        elif backend == 'torchani':
            batch_size = min(32, len(mols))
        elif backend == 'deepchem':
            batch_size = min(64, len(mols))
        else:
            batch_size = min(100, len(mols))
    
    results = []
    failures = 0
    
    # Use passed status elements
    # status_container = st.empty() # Removed
    # progress_text = st.empty() # Removed
    
    # Removed internal context manager for progress bar
    # with st.progress(0) as progress_bar: 
    start_time = time.time()
    status_container.info(f"å¼€å§‹ç”Ÿæˆæ„è±¡ï¼Œå…± {len(mols)} ä¸ªåˆ†å­ï¼Œä½¿ç”¨ {backend} åç«¯")
    
    for i in range(0, len(mols), batch_size):
        batch = mols[i:min(i+batch_size, len(mols))]
        batch_size_actual = len(batch)
        
        # æ›´æ–°çŠ¶æ€
        batch_start_time = time.time()
        progress_text.text(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(mols)-1)//batch_size + 1}ï¼Œåˆ†å­ {i+1}-{min(i+batch_size_actual, len(mols))}/{len(mols)}")
        
        # ä½¿ç”¨å¤šåç«¯ç”Ÿæˆæ„è±¡
        batch_results = []
        batch_failures = 0
        
        for j, mol in enumerate(batch):
            # æ›´æ–°è¿›åº¦ (using the passed progress_bar)
            progress = (i + j) / len(mols)
            if progress_bar: # Check if progress_bar is not None
                progress_bar.progress(progress, text=f"å¤„ç†åˆ†å­ {i+j+1}/{len(mols)}")
            
            # ç”Ÿæˆæ„è±¡
            mol_3d = generate_3d_conformer_multi(mol, backend=backend, **kwargs)
            
            if mol_3d is None:
                batch_failures += 1
                failures += 1
            
            batch_results.append(mol_3d)
            
            # æ˜¾ç¤ºå½“å‰å¤„ç†é€Ÿåº¦
            elapsed = time.time() - start_time
            molecules_per_second = (i + j + 1) / elapsed if elapsed > 0 else 0
            remaining = (len(mols) - (i + j + 1)) / molecules_per_second if molecules_per_second > 0 else 0
            
            if (j + 1) % max(1, batch_size_actual // 5) == 0 or j == batch_size_actual - 1:
                progress_text.text(
                    f"æ‰¹æ¬¡ {i//batch_size + 1}/{(len(mols)-1)//batch_size + 1}"
                    f"ï¼Œåˆ†å­ {i+j+1}/{len(mols)}"
                    f"ï¼Œå¤„ç†é€Ÿåº¦: {molecules_per_second:.2f} åˆ†å­/ç§’"
                    f"ï¼Œå‰©ä½™æ—¶é—´: {int(remaining//60)}åˆ†{int(remaining%60)}ç§’"
                )
        
        results.extend(batch_results)
        
        # æ˜¾ç¤ºæ‰¹æ¬¡çŠ¶æ€
        batch_time = time.time() - batch_start_time
        status_container.info(
            f"å®Œæˆæ‰¹æ¬¡ {i//batch_size + 1}/{(len(mols)-1)//batch_size + 1}"
            f"ï¼Œæ„è±¡ç”ŸæˆæˆåŠŸç‡: {(batch_size_actual-batch_failures)/batch_size_actual*100:.1f}%"
            f"ï¼Œæ‰¹æ¬¡è€—æ—¶: {batch_time:.1f}ç§’"
            f"ï¼Œæ¯åˆ†å­: {batch_time/batch_size_actual:.2f}ç§’"
        )
        
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    total_time = time.time() - start_time
    success_rate = (len(mols) - failures) / len(mols) * 100 if len(mols) > 0 else 0
    
    status_container.success(
        f"æ„è±¡ç”Ÿæˆå®Œæˆ! æˆåŠŸç‡: {success_rate:.1f}% ({len(mols)-failures}/{len(mols)})"
        f"ï¼Œæ€»è€—æ—¶: {total_time:.1f}ç§’"
        f"ï¼Œå¹³å‡æ¯åˆ†å­: {total_time/len(mols):.2f}ç§’"
    )
    
    if failures > 0:
        st.warning(f"è­¦å‘Š: {failures}ä¸ªåˆ†å­çš„æ„è±¡ç”Ÿæˆå¤±è´¥")
    
    # Reset progress bar to 0 after completion
    if progress_bar:
        progress_bar.progress(1.0) # Set to 100%
        # Optionally clear the text/info elements
        # progress_text.empty()
        # status_container.empty()

    return results

def compute_usr_descriptor(mol):
    """è®¡ç®—USR (Ultrafast Shape Recognition) æè¿°ç¬¦"""
    if mol is None or mol.GetNumConformers() == 0:
        return None
    
    conf = mol.GetConformer()
    if not conf.Is3D():
        return None
    
    try:
        # æå–æ‰€æœ‰åŸå­åæ ‡
        coords = []
        for i in range(mol.GetNumAtoms()):
            pos = conf.GetAtomPosition(i)
            coords.append(np.array([pos.x, pos.y, pos.z]))
        coords = np.array(coords)
        
        # 1) è´¨å¿ƒ
        centroid = coords.mean(axis=0)
        # 2) ä¸è´¨å¿ƒæœ€è¿œçš„ç‚¹P1
        dists_centroid = np.linalg.norm(coords - centroid, axis=1)
        idx_p1 = np.argmax(dists_centroid)
        p1 = coords[idx_p1]
        # 3) ä¸p1æœ€è¿œçš„ç‚¹p2
        dists_p1 = np.linalg.norm(coords - p1, axis=1)
        idx_p2 = np.argmax(dists_p1)
        p2 = coords[idx_p2]
        # 4) ä¸p2æœ€è¿œçš„ç‚¹p3
        dists_p2 = np.linalg.norm(coords - p2, axis=1)
        idx_p3 = np.argmax(dists_p2)
        p3 = coords[idx_p3]
        
        # å››ä¸ªå‚è€ƒç‚¹
        P = [centroid, p1, p2, p3]
        
        # è®¡ç®—åˆ°è¿™4ç‚¹çš„è·ç¦»åˆ†å¸ƒ
        descriptor = []
        for ref_pt in P:
            dists = np.linalg.norm(coords - ref_pt, axis=1)
            d_mean = dists.mean()
            d_std = dists.std()
            d_min = dists.min()
            d_max = dists.max()
            descriptor.extend([d_mean, d_std, d_min, d_max])
        
        return np.array(descriptor)
    except:
        return None

def compute_usrcat_descriptor(mol):
    """è®¡ç®—USRCAT (USR-CAT) æè¿°ç¬¦"""
    if mol is None or mol.GetNumConformers() == 0:
        return None
    
    conf = mol.GetConformer()
    if not conf.Is3D():
        return None
    
    try:
        # å®šä¹‰åŸå­ç±»å‹
        atom_types = {
            'all': lambda a: True,
            'hydrophobic': lambda a: a.GetSymbol() in ('C', 'Cl', 'Br', 'I'),
            'aromatic': lambda a: a.GetIsAromatic(),
            'acceptor': lambda a: a.GetSymbol() in ('N', 'O', 'F') and not a.GetIsAromatic(),
            'donor': lambda a: (a.GetSymbol() in ('N', 'O') and 
                              sum(1 for n in a.GetNeighbors() if n.GetSymbol() == 'H') > 0)
        }
        
        descriptors = []
        
        for atom_type, type_func in atom_types.items():
            # è·å–ç‰¹å®šç±»å‹çš„åŸå­åæ ‡
            coords = []
            for i in range(mol.GetNumAtoms()):
                atom = mol.GetAtomWithIdx(i)
                if type_func(atom):
                    pos = conf.GetAtomPosition(i)
                    coords.append(np.array([pos.x, pos.y, pos.z]))
            
            if not coords:
                descriptors.extend([0] * 12)  # å¦‚æœæ²¡æœ‰è¯¥ç±»å‹çš„åŸå­ï¼Œå¡«å……0
                continue
                
            coords = np.array(coords)
            
            # è®¡ç®—å››ä¸ªå‚è€ƒç‚¹
            centroid = coords.mean(axis=0)
            dists_centroid = np.linalg.norm(coords - centroid, axis=1)
            idx_p1 = np.argmax(dists_centroid)
            p1 = coords[idx_p1]
            
            dists_p1 = np.linalg.norm(coords - p1, axis=1)
            idx_p2 = np.argmax(dists_p1)
            p2 = coords[idx_p2]
            
            dists_p2 = np.linalg.norm(coords - p2, axis=1)
            idx_p3 = np.argmax(dists_p2)
            p3 = coords[idx_p3]
            
            # è®¡ç®—åˆ°å››ä¸ªå‚è€ƒç‚¹çš„è·ç¦»çŸ©
            for ref_pt in [centroid, p1, p2, p3]:
                dists = np.linalg.norm(coords - ref_pt, axis=1)
                descriptors.extend([
                    dists.mean(),
                    dists.std(),
                    dists.min(),
                    dists.max()
                ])
        
        return np.array(descriptors)
    except:
        return None

def compute_pmi_ratios(mol):
    """è®¡ç®—PMIæ¯”ç‡"""
    if mol is None or mol.GetNumConformers() == 0:
        return None
    try:
        inertia = rdMolDescriptors.CalcPMIValues(mol)
        I1, I2, I3 = inertia
        if abs(I3) < 1e-8:
            return None
        return (I1/I3, I2/I3)
    except:
        return None

def calc_nearest_neighbor_distance(descriptors, cuda_available=False):
    """è®¡ç®—æœ€è¿‘é‚»è·ç¦»ï¼Œæ”¯æŒGPUåŠ é€Ÿ"""
    n = len(descriptors)
    if n < 2:
        return 0.0, 0.0
    
    # ä½¿ç”¨GPUåŠ é€Ÿ
    if cuda_available and torch.cuda.is_available():
        try:
            # è®°å½•GPUå†…å­˜ä½¿ç”¨å‰çŠ¶æ€
            mem_before = torch.cuda.memory_allocated() / 1024**2
            
            # è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶ç§»è‡³GPU
            desc_tensor = torch.tensor(descriptors, dtype=torch.float32).cuda()
            
            # è®¡ç®—æ¬§æ°è·ç¦»çŸ©é˜µ
            distances = torch.zeros((n, n), dtype=torch.float32).cuda()
            batch_size = 128  # æ‰¹å¤„ç†å¤§å°ï¼Œé¿å…æ˜¾å­˜æº¢å‡º
            
            for i in range(0, n, batch_size):
                end_i = min(i + batch_size, n)
                chunk_i = desc_tensor[i:end_i]
                
                for j in range(0, n, batch_size):
                    end_j = min(j + batch_size, n)
                    chunk_j = desc_tensor[j:end_j]
                    
                    # è®¡ç®—æ‰¹æ¬¡ä¹‹é—´çš„è·ç¦»
                    dist_chunk = torch.cdist(chunk_i, chunk_j)
                    distances[i:end_i, j:end_j] = dist_chunk
            
            # å°†è‡ªèº«è·ç¦»è®¾ä¸ºæ— ç©·å¤§
            eye_mask = torch.eye(n, dtype=torch.bool).cuda()
            distances[eye_mask] = float('inf')
            
            # æ¯ä¸ªåˆ†å­çš„æœ€è¿‘é‚»è·ç¦»
            min_dists, _ = torch.min(distances, dim=1)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            mean_d = min_dists.mean().item()
            std_d = min_dists.std().item()
            
            # è®°å½•GPUå†…å­˜ä½¿ç”¨åçŠ¶æ€
            mem_after = torch.cuda.memory_allocated() / 1024**2
            mem_diff = mem_after - mem_before
            
            # è®°å½•GPUä½¿ç”¨æƒ…å†µ
            st.session_state.gpu_nn_mem_usage = mem_diff
            st.session_state.gpu_nn_calls = st.session_state.get('gpu_nn_calls', 0) + 1
            
            # æ¸…ç†GPUå†…å­˜
            torch.cuda.empty_cache()
            
            return mean_d, std_d
        
        except Exception as e:
            st.warning(f"GPUåŠ é€Ÿæœ€è¿‘é‚»è®¡ç®—å¤±è´¥ï¼Œå°†ä½¿ç”¨CPU: {str(e)}")
    
    # ä½¿ç”¨CPUè®¡ç®—
    distances = []
    for i in range(n):
        d_i = float('inf')
        for j in range(n):
            if i == j:
                continue
            dist = np.linalg.norm(descriptors[i] - descriptors[j])
            if dist < d_i:
                d_i = dist
        distances.append(d_i)
    
    mean_d = np.mean(distances)
    std_d = np.std(distances)
    return mean_d, std_d

def perform_dimensionality_reduction(descriptors, method="t-SNE", cuda_available=False, **kwargs):
    """ä½¿ç”¨GPUåŠ é€Ÿçš„é™ç»´åˆ†æ"""
    if len(descriptors) == 0:
        return np.array([])
    
    # è®°å½•åˆå§‹çŠ¶æ€
    gpu_used = False
    start_time = time.time()
        
    if method == "t-SNE":
        if cuda_available and 'cp' in globals() and 'cuTSNE' in globals():
            try:
                # è®°å½•GPUå†…å­˜ä½¿ç”¨å‰çŠ¶æ€(å¦‚æœå¯ç”¨)
                if torch.cuda.is_available():
                    mem_before = torch.cuda.memory_allocated() / 1024**2
                
                tsne = cuTSNE(n_components=2, **kwargs)
                coords = tsne.fit_transform(cp.array(descriptors))
                
                # è®°å½•GPUä½¿ç”¨æƒ…å†µ
                if torch.cuda.is_available():
                    mem_after = torch.cuda.memory_allocated() / 1024**2
                    mem_diff = abs(mem_after - mem_before)  # cuMLå¯èƒ½ä½¿ç”¨ä¸åŒçš„å†…å­˜ç®¡ç†
                    st.session_state.gpu_tsne_mem = mem_diff
                
                gpu_used = True
                st.session_state.gpu_tsne_calls = st.session_state.get('gpu_tsne_calls', 0) + 1
                
                result = cp.asnumpy(coords)
            except Exception as e:
                st.warning(f"GPUåŠ é€Ÿt-SNEå¤±è´¥ï¼Œå°†ä½¿ç”¨CPU: {str(e)}")
                tsne = TSNE(n_components=2, **kwargs)
                result = tsne.fit_transform(descriptors)
        else:
            tsne = TSNE(n_components=2, **kwargs)
            result = tsne.fit_transform(descriptors)
    elif method == "UMAP":
        reducer = umap.UMAP(n_components=2, **kwargs)
        result = reducer.fit_transform(descriptors)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„é™ç»´æ–¹æ³•: {method}")
    
    # è®°å½•æ€»æ—¶é—´
    total_time = time.time() - start_time
    
    # å­˜å‚¨æ€§èƒ½ä¿¡æ¯
    st.session_state[f"{method}_time"] = total_time
    st.session_state[f"{method}_gpu_used"] = gpu_used
    
    return result

def plot_shape_space(coordsA, coordsB, title="å½¢çŠ¶ç©ºé—´åˆ†å¸ƒ"):
    """ç»˜åˆ¶å½¢çŠ¶ç©ºé—´åˆ†å¸ƒå›¾"""
    fig, ax = plt.subplots(figsize=(10, 8))
    ax.scatter(coordsA[:,0], coordsA[:,1], c='blue', alpha=0.5, label="æ•°æ®é›†A", s=10)
    ax.scatter(coordsB[:,0], coordsB[:,1], c='red', alpha=0.5, label="æ•°æ®é›†B", s=10)
    ax.set_title(title)
    ax.legend()
    plt.tight_layout()
    return fig

def plot_pmi_triangle(pmiA, pmiB, labelA="æ•°æ®é›†A", labelB="æ•°æ®é›†B"):
    """ç»˜åˆ¶PMIä¸‰è§’å›¾"""
    def to_triangle_coords(a, b):
        x = b + (a/2.0)
        y = (np.sqrt(3)/2.0) * a
        return (x, y)
    
    coordsA = [to_triangle_coords(a, b) for (a,b) in pmiA]
    coordsB = [to_triangle_coords(a, b) for (a,b) in pmiB]
    
    if not coordsA or not coordsB:
        return None
        
    coordsA = np.array(coordsA)
    coordsB = np.array(coordsB)
    
    fig, ax = plt.subplots(figsize=(10, 8))
    ax.scatter(coordsA[:,0], coordsA[:,1], c='blue', alpha=0.4, s=10, label=labelA)
    ax.scatter(coordsB[:,0], coordsB[:,1], c='red', alpha=0.4, s=10, label=labelB)
    ax.set_title("PMIä¸‰è§’å›¾ï¼ˆå½¢çŠ¶åˆ†å¸ƒï¼‰")
    ax.legend()
    plt.tight_layout()
    return fig

def calculate_distribution_metrics(coords_A, coords_B):
    """è®¡ç®—åˆ†å¸ƒç»Ÿè®¡æŒ‡æ ‡"""
    if len(coords_A) == 0 or len(coords_B) == 0:
        return {}
        
    metrics = {}
    
    # è®¡ç®—æ¯ä¸ªç»´åº¦çš„Wassersteinè·ç¦»
    for dim in range(coords_A.shape[1]):
        w_dist = wasserstein_distance(coords_A[:,dim], coords_B[:,dim])
        metrics[f'Wassersteinè·ç¦»_dim{dim+1}'] = w_dist
    
    # è®¡ç®—ä¸­å¿ƒè·ç¦»
    center_A = np.mean(coords_A, axis=0)
    center_B = np.mean(coords_B, axis=0)
    center_dist = np.linalg.norm(center_A - center_B)
    metrics['ä¸­å¿ƒè·ç¦»'] = center_dist
    
    # è®¡ç®—åˆ†å¸ƒé‡å åº¦
    try:
        kde_A = KernelDensity(kernel='gaussian').fit(coords_A)
        kde_B = KernelDensity(kernel='gaussian').fit(coords_B)
        
        overlap_score = (np.exp(kde_A.score_samples(coords_B)).mean() + 
                        np.exp(kde_B.score_samples(coords_A)).mean()) / 2
        metrics['åˆ†å¸ƒé‡å åº¦'] = overlap_score
    except:
        pass
    
    return metrics

def get_optimal_batch_size(mol_size, device=None):
    """æ ¹æ®åˆ†å­å¤§å°å’ŒGPUæ˜¾å­˜åŠ¨æ€è®¡ç®—æœ€ä¼˜æ‰¹å¤„ç†å¤§å°"""
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    if device.type != 'cuda':
        return 50  # CPUé»˜è®¤æ‰¹å¤„ç†å¤§å°
    
    try:
        # è·å–GPUæ€»æ˜¾å­˜(MB)
        total_mem = torch.cuda.get_device_properties(0).total_memory / 1024**2
        # è·å–å½“å‰å·²ç”¨æ˜¾å­˜(MB)
        used_mem = torch.cuda.memory_allocated(0) / 1024**2
        # å¯ç”¨æ˜¾å­˜(MB)
        available_mem = total_mem - used_mem
        
        # ä¼°ç®—å•ä¸ªåˆ†å­æ‰€éœ€æ˜¾å­˜(MB)
        # å‡è®¾æ¯ä¸ªåŸå­éœ€è¦çš„æ˜¾å­˜çº¦ä¸º: åæ ‡(3*4=12å­—èŠ‚) + è·ç¦»è®¡ç®—(4*4=16å­—èŠ‚) = 28å­—èŠ‚
        mem_per_mol = mol_size * 28 / 1024**2  # è½¬æ¢ä¸ºMB
        
        # é¢„ç•™30%æ˜¾å­˜ç»™å…¶ä»–è®¡ç®—
        safe_mem = available_mem * 0.7
        
        # è®¡ç®—æœ€å¤§æ‰¹å¤„ç†å¤§å°
        max_batch = int(safe_mem / mem_per_mol)
        
        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        optimal_batch = max(10, min(max_batch, 500))
        
        return optimal_batch
    
    except Exception as e:
        st.warning(f"è®¡ç®—æœ€ä¼˜æ‰¹å¤„ç†å¤§å°å¤±è´¥: {str(e)}")
        return 50  # è¿”å›é»˜è®¤å€¼

@torch.jit.script
def compute_distances_jit(coords: torch.Tensor, ref_point: torch.Tensor) -> torch.Tensor:
    """ä½¿ç”¨TorchScriptä¼˜åŒ–çš„è·ç¦»è®¡ç®—"""
    return torch.norm(coords - ref_point.unsqueeze(0), dim=1)

def process_usr_batch_optimized(mols_3d, cuda_available=False, batch_size=None):
    """ä¼˜åŒ–çš„USRæ‰¹å¤„ç†è®¡ç®—"""
    if not mols_3d:
        return []
    
    device = torch.device('cuda' if cuda_available and torch.cuda.is_available() else 'cpu')
    
    # è®°å½•å¼€å§‹æ—¶é—´å’ŒGPUå†…å­˜
    start_time = time.time()
    if device.type == 'cuda':
        mem_before = torch.cuda.memory_allocated() / 1024**2
    
    # è·å–ç¬¬ä¸€ä¸ªæœ‰æ•ˆåˆ†å­çš„å¤§å°ç”¨äºè®¡ç®—æ‰¹å¤„ç†å¤§å°
    mol_size = next((mol.GetNumAtoms() for mol in mols_3d if mol is not None), 0)
    if batch_size is None:
        batch_size = get_optimal_batch_size(mol_size, device)
    
    descriptors = []
    gpu_used = False
    
    # åˆ›å»ºå¼‚æ­¥æµ
    if device.type == 'cuda':
        stream = torch.cuda.Stream()
    
    # æ‰¹é‡å¤„ç†åˆ†å­
    for i in range(0, len(mols_3d), batch_size):
        batch = mols_3d[i:i + batch_size]
        batch_descriptors = []
        
        if device.type == 'cuda':
            with torch.cuda.stream(stream):
                for mol in batch:
                    if mol is not None:
                        desc = compute_usr_descriptor_gpu(mol, device)
                        gpu_used = True
                    else:
                        desc = None
                    batch_descriptors.append(desc)
                
                # åŒæ­¥æµ
                stream.synchronize()
        else:
            for mol in batch:
                if mol is not None:
                    desc = compute_usr_descriptor(mol)
                else:
                    desc = None
                batch_descriptors.append(desc)
        
        descriptors.extend(batch_descriptors)
        
        # æ¸…ç†GPUç¼“å­˜
        if device.type == 'cuda':
            torch.cuda.empty_cache()
    
    # è®¡ç®—æ€§èƒ½ç»Ÿè®¡
    total_time = time.time() - start_time
    if device.type == 'cuda':
        mem_after = torch.cuda.memory_allocated() / 1024**2
        mem_diff = mem_after - mem_before
        
        # æ›´æ–°ä¼šè¯çŠ¶æ€
        st.session_state.usr_batch_time = total_time
        st.session_state.usr_batch_mem = mem_diff
        st.session_state.usr_batch_gpu_used = gpu_used
        
        # è®°å½•å¤„ç†é€Ÿåº¦
        valid_mols = sum(1 for d in descriptors if d is not None)
        speed = valid_mols / total_time if total_time > 0 else 0
        st.session_state.usr_processing_speed = speed
    
    return descriptors

def batch_compute_shape_descriptors(mols, descriptor_type="USR", max_attempts=50, cuda_available=False, batch_size=None):
    """æ”¹è¿›çš„æ‰¹é‡è®¡ç®—å½¢çŠ¶æè¿°ç¬¦å‡½æ•°"""
    if not mols:
        return [], []
    
    descriptors = []
    pmi_ratios = []
    
    # è¿›åº¦æ˜¾ç¤º
    total = len(mols)
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # å°†åˆ†å­åˆ†æ‰¹å¤„ç†
    batch_mols_3d = []
    for m in mols:
        m3d = generate_3d_conformer(m, max_attempts)
        batch_mols_3d.append(m3d)
    
    # æ›´æ–°è¿›åº¦
    progress = len(batch_mols_3d) / total
    progress_bar.progress(progress)
    status_text.text(f"ç”Ÿæˆ3Dæ„è±¡: {len(batch_mols_3d)}/{total}")
        
    # è®¡ç®—PMIæ¯”ç‡
    status_text.text("è®¡ç®—PMIæ¯”ç‡...")
    for m3d in batch_mols_3d:
        if m3d:
            ratios = compute_pmi_ratios(m3d)
            if ratios:
                pmi_ratios.append(ratios)
    
    # è®¡ç®—å½¢çŠ¶æè¿°ç¬¦
    status_text.text(f"è®¡ç®—{descriptor_type}æè¿°ç¬¦...")
    if descriptor_type == "USR":
        descriptors = process_usr_batch_optimized(batch_mols_3d, cuda_available, batch_size)
    else:  # USRCAT
        descriptors = process_usrcat_batch(batch_mols_3d, cuda_available, batch_size)
    
    # æ¸…ç†è¿›åº¦æ˜¾ç¤º
    progress_bar.empty()
    status_text.empty()
    
    # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
    if cuda_available and torch.cuda.is_available():
        speed = st.session_state.get('usr_processing_speed', 0)
        mem_used = st.session_state.get('usr_batch_mem', 0)
        st.info(f"æ€§èƒ½ç»Ÿè®¡:\n"
                f"- å¤„ç†é€Ÿåº¦: {speed:.1f} åˆ†å­/ç§’\n"
                f"- GPUå†…å­˜ä½¿ç”¨: {mem_used:.1f} MB")
    
    return np.array([d for d in descriptors if d is not None]), pmi_ratios

def compute_usr_descriptor_gpu(mol, device=None):
    """ä½¿ç”¨GPUåŠ é€Ÿè®¡ç®—USR (Ultrafast Shape Recognition) æè¿°ç¬¦"""
    if mol is None or mol.GetNumConformers() == 0:
        return None
    
    conf = mol.GetConformer()
    if not conf.Is3D():
        return None
    
    try:
        # è®¾ç½®è®¾å¤‡
        if device is None and torch.cuda.is_available():
            device = torch.device('cuda')
        else:
            device = torch.device('cpu')
        
        # è®°å½•GPUå†…å­˜ä½¿ç”¨å‰çŠ¶æ€
        if device.type == 'cuda':
            mem_before = torch.cuda.memory_allocated() / 1024**2
        
        # æå–æ‰€æœ‰åŸå­åæ ‡å¹¶è½¬æ¢ä¸ºå¼ é‡
        coords = []
        for i in range(mol.GetNumAtoms()):
            pos = conf.GetAtomPosition(i)
            coords.append([pos.x, pos.y, pos.z])
            
        # æ£€æŸ¥åŸå­æ•°é‡
        if len(coords) < 2:
            return None
            
        coords_tensor = torch.tensor(coords, dtype=torch.float32).to(device)
        
        # 1) è´¨å¿ƒ
        centroid = torch.mean(coords_tensor, dim=0)
        
        # 2) ä¸è´¨å¿ƒæœ€è¿œçš„ç‚¹P1
        dists_centroid = torch.norm(coords_tensor - centroid, dim=1)
        idx_p1 = torch.argmax(dists_centroid)
        p1 = coords_tensor[idx_p1]
        
        # 3) ä¸p1æœ€è¿œçš„ç‚¹p2
        dists_p1 = torch.norm(coords_tensor - p1, dim=1)
        idx_p2 = torch.argmax(dists_p1)
        p2 = coords_tensor[idx_p2]
        
        # 4) ä¸p2æœ€è¿œçš„ç‚¹p3
        dists_p2 = torch.norm(coords_tensor - p2, dim=1)
        idx_p3 = torch.argmax(dists_p2)
        p3 = coords_tensor[idx_p3]
        
        # æ‰¹é‡è®¡ç®—æ‰€æœ‰å‚è€ƒç‚¹çš„è·ç¦»ç»Ÿè®¡
        ref_points = torch.stack([centroid, p1, p2, p3])
        
        # ä½¿ç”¨å¹¿æ’­æœºåˆ¶è®¡ç®—æ‰€æœ‰è·ç¦»
        # shape: [n_ref_points, n_atoms]
        all_dists = torch.norm(coords_tensor.unsqueeze(0) - ref_points.unsqueeze(1), dim=2)
        
        # è®¡ç®—æ¯ä¸ªå‚è€ƒç‚¹çš„ç»Ÿè®¡é‡ï¼Œä½¿ç”¨æ— åä¼°è®¡
        means = torch.mean(all_dists, dim=1)
        stds = torch.std(all_dists, dim=1, unbiased=True)
        mins = torch.min(all_dists, dim=1)[0]
        maxs = torch.max(all_dists, dim=1)[0]
        
        # å°†æ‰€æœ‰ç»Ÿè®¡é‡ç»„åˆæˆæœ€ç»ˆæè¿°ç¬¦
        descriptors = torch.stack([
            means, stds, mins, maxs
        ]).t().reshape(-1).cpu().numpy()
        
        # è®°å½•GPUå†…å­˜ä½¿ç”¨åçŠ¶æ€å’Œè°ƒç”¨æ¬¡æ•°
        if device.type == 'cuda':
            mem_after = torch.cuda.memory_allocated() / 1024**2
            mem_diff = mem_after - mem_before
            if mem_diff > 1.0:  # å¦‚æœå†…å­˜å˜åŒ–å¤§äº1MB
                st.session_state.gpu_usr_calls = st.session_state.get('gpu_usr_calls', 0) + 1
                st.session_state.gpu_usr_mem = mem_diff
        
        return descriptors
        
    except Exception as e:
        # åœ¨å‘ç”Ÿé”™è¯¯æ—¶å›é€€åˆ°CPUç‰ˆæœ¬
        st.warning(f"GPUè®¡ç®—USRæè¿°ç¬¦å¤±è´¥ï¼Œå›é€€åˆ°CPU: {str(e)}")
        return compute_usr_descriptor(mol)

def process_usr_batch(mols_3d, cuda_available=False):
    """æ‰¹é‡å¤„ç†USRæè¿°ç¬¦è®¡ç®—ï¼Œæ”¯æŒGPUåŠ é€Ÿ"""
    descriptors = []
    device = torch.device('cuda') if cuda_available and torch.cuda.is_available() else torch.device('cpu')
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # è®°å½•å¤„ç†å‰GPUå†…å­˜
    if device.type == 'cuda':
        mem_before = torch.cuda.memory_allocated() / 1024**2
    
    gpu_used = False
    
    for mol in mols_3d:
        if mol is not None:
            if cuda_available and torch.cuda.is_available():
                desc = compute_usr_descriptor_gpu(mol, device)
                gpu_used = True
            else:
                desc = compute_usr_descriptor(mol)
            descriptors.append(desc)
        else:
            descriptors.append(None)
    
    # è®¡ç®—æ€»æ—¶é—´
    total_time = time.time() - start_time
    
    # è®°å½•å¤„ç†åGPUå†…å­˜å’Œä½¿ç”¨æƒ…å†µ
    if device.type == 'cuda':
        mem_after = torch.cuda.memory_allocated() / 1024**2
        mem_diff = mem_after - mem_before
        st.session_state.usr_batch_time = total_time
        st.session_state.usr_batch_mem = mem_diff
        st.session_state.usr_batch_gpu_used = gpu_used
    
    return descriptors

def compute_usrcat_descriptor_gpu(mol, device=None):
    """ä½¿ç”¨GPUåŠ é€Ÿè®¡ç®—USRCATæè¿°ç¬¦"""
    if mol is None or mol.GetNumConformers() == 0:
        return None
    
    conf = mol.GetConformer()
    if not conf.Is3D():
        return None
    
    try:
        # å®šä¹‰åŸå­ç±»å‹
        atom_types = {
            'all': lambda a: True,
            'hydrophobic': lambda a: a.GetSymbol() in ('C', 'Cl', 'Br', 'I'),
            'aromatic': lambda a: a.GetIsAromatic(),
            'acceptor': lambda a: a.GetSymbol() in ('N', 'O', 'F') and not a.GetIsAromatic(),
            'donor': lambda a: (a.GetSymbol() in ('N', 'O') and 
                              sum(1 for n in a.GetNeighbors() if n.GetSymbol() == 'H') > 0)
        }
        
        if device is None and torch.cuda.is_available():
            device = torch.device('cuda')
        else:
            device = torch.device('cpu')
        
        # è®°å½•GPUå†…å­˜ä½¿ç”¨å‰çŠ¶æ€
        if device.type == 'cuda':
            mem_before = torch.cuda.memory_allocated() / 1024**2
        
        descriptors = []
        
        for atom_type, type_func in atom_types.items():
            # è·å–ç‰¹å®šç±»å‹çš„åŸå­åæ ‡
            coords = []
            for i in range(mol.GetNumAtoms()):
                atom = mol.GetAtomWithIdx(i)
                if type_func(atom):
                    pos = conf.GetAtomPosition(i)
                    coords.append([pos.x, pos.y, pos.z])
            
            if not coords:
                descriptors.extend([0] * 12)  # å¦‚æœæ²¡æœ‰è¯¥ç±»å‹çš„åŸå­ï¼Œå¡«å……0
                continue
                
            # è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶ç§»è‡³GPU
            coords_tensor = torch.tensor(coords, dtype=torch.float32).to(device)
            
            # 1) è´¨å¿ƒ
            centroid = torch.mean(coords_tensor, dim=0)
            
            # 2) ä¸è´¨å¿ƒæœ€è¿œçš„ç‚¹P1
            dists_centroid = torch.norm(coords_tensor - centroid, dim=1)
            idx_p1 = torch.argmax(dists_centroid)
            p1 = coords_tensor[idx_p1]
            
            # 3) ä¸p1æœ€è¿œçš„ç‚¹p2
            dists_p1 = torch.norm(coords_tensor - p1, dim=1)
            idx_p2 = torch.argmax(dists_p1)
            p2 = coords_tensor[idx_p2]
            
            # 4) ä¸p2æœ€è¿œçš„ç‚¹p3
            dists_p2 = torch.norm(coords_tensor - p2, dim=1)
            idx_p3 = torch.argmax(dists_p2)
            p3 = coords_tensor[idx_p3]
            
            # å››ä¸ªå‚è€ƒç‚¹
            for ref_pt in [centroid, p1, p2, p3]:
                dists = torch.norm(coords_tensor - ref_pt, dim=1)
                descriptors.extend([
                    dists.mean().item(),
                    dists.std().item(),
                    dists.min().item(),
                    dists.max().item()
                ])
        
        # è®°å½•GPUå†…å­˜ä½¿ç”¨åçŠ¶æ€
        if device.type == 'cuda':
            mem_after = torch.cuda.memory_allocated() / 1024**2
            mem_diff = mem_after - mem_before
            # å¦‚æœå†…å­˜å˜åŒ–å¤§äº1MBï¼Œåˆ™è®¤ä¸ºGPUçœŸæ­£è¢«ä½¿ç”¨
            if mem_diff > 1.0:
                st.session_state.gpu_usrcat_calls = st.session_state.get('gpu_usrcat_calls', 0) + 1
        
        return np.array(descriptors)
    except Exception as e:
        # åœ¨å‘ç”Ÿé”™è¯¯æ—¶å›é€€åˆ°CPUç‰ˆæœ¬
        st.warning(f"GPUè®¡ç®—USRCATæè¿°ç¬¦å¤±è´¥ï¼Œå›é€€åˆ°CPU: {str(e)}")
        return compute_usrcat_descriptor(mol)

def process_usrcat_batch(mols_3d, cuda_available=False):
    """æ‰¹é‡å¤„ç†USRCATæè¿°ç¬¦è®¡ç®—ï¼Œæ”¯æŒGPUåŠ é€Ÿ"""
    descriptors = []
    device = torch.device('cuda') if cuda_available and torch.cuda.is_available() else torch.device('cpu')
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # è®°å½•å¤„ç†å‰GPUå†…å­˜
    if device.type == 'cuda':
        mem_before = torch.cuda.memory_allocated() / 1024**2
    
    gpu_used = False
    
    for mol in mols_3d:
        if mol is not None:
            if cuda_available and torch.cuda.is_available():
                desc = compute_usrcat_descriptor_gpu(mol, device)
                gpu_used = True
            else:
                desc = compute_usrcat_descriptor(mol)
            descriptors.append(desc)
        else:
            descriptors.append(None)
    
    # è®¡ç®—æ€»æ—¶é—´
    total_time = time.time() - start_time
    
    # è®°å½•å¤„ç†åGPUå†…å­˜å’Œä½¿ç”¨æƒ…å†µ
    if device.type == 'cuda':
        mem_after = torch.cuda.memory_allocated() / 1024**2
        mem_diff = mem_after - mem_before
        st.session_state.usrcat_batch_time = total_time
        st.session_state.usrcat_batch_mem = mem_diff
        st.session_state.usrcat_batch_gpu_used = gpu_used
    
    return descriptors

def monitor_gpu_usage():
    """ç›‘æ§å¹¶è¿”å›GPUä½¿ç”¨æƒ…å†µ"""
    if not torch.cuda.is_available():
        return {"çŠ¶æ€": "GPUä¸å¯ç”¨"}
    
    try:
        metrics = {}
        metrics["æ€»æ˜¾å­˜(MB)"] = torch.cuda.get_device_properties(0).total_memory / 1024**2
        metrics["å·²ç”¨æ˜¾å­˜(MB)"] = torch.cuda.memory_allocated(0) / 1024**2
        metrics["å·²ç¼“å­˜æ˜¾å­˜(MB)"] = torch.cuda.memory_reserved(0) / 1024**2
        metrics["æ˜¾å­˜åˆ©ç”¨ç‡(%)"] = 100 * metrics["å·²ç”¨æ˜¾å­˜(MB)"] / metrics["æ€»æ˜¾å­˜(MB)"]
        return metrics
    except:
        return {"çŠ¶æ€": "æ— æ³•è·å–GPUä¿¡æ¯"}

# æ·»åŠ GPUä½¿ç”¨è®°å½•åŠŸèƒ½
def track_gpu_usage():
    """è®°å½•GPUä½¿ç”¨æƒ…å†µå¹¶è¿”å›å½“å‰çŠ¶æ€"""
    if not torch.cuda.is_available():
        return {'used': 0, 'free': 0, 'total': 0, 'utilization': 0}
    
    try:
        # è·å–GPUå†…å­˜ä¿¡æ¯
        total_mem = torch.cuda.get_device_properties(0).total_memory / 1024**2  # MB
        used_mem = torch.cuda.memory_allocated(0) / 1024**2  # MB
        reserved_mem = torch.cuda.memory_reserved(0) / 1024**2  # MB
        free_mem = total_mem - used_mem  # å¯ç”¨æ˜¾å­˜
        utilization = (used_mem / total_mem) * 100  # ä½¿ç”¨ç‡
        
        # è®°å½•åˆ°ä¼šè¯çŠ¶æ€
        if 'gpu_usage_history' not in st.session_state:
            st.session_state.gpu_usage_history = []
        
        # æ·»åŠ å½“å‰æ—¶é—´ç‚¹çš„è®°å½•
        st.session_state.gpu_usage_history.append({
            'time': time.time(),
            'used': used_mem,
            'reserved': reserved_mem,
            'free': free_mem,
            'total': total_mem,
            'utilization': utilization
        })
        
        # åªä¿ç•™æœ€è¿‘100ä¸ªè®°å½•
        if len(st.session_state.gpu_usage_history) > 100:
            st.session_state.gpu_usage_history = st.session_state.gpu_usage_history[-100:]
        
        return {
            'used': used_mem,
            'reserved': reserved_mem,
            'free': free_mem,
            'total': total_mem,
            'utilization': utilization
        }
    except Exception as e:
        st.error(f"è·å–GPUä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        return {'used': 0, 'free': 0, 'total': 0, 'utilization': 0}

# æ·»åŠ GPUä½¿ç”¨å†å²å¯è§†åŒ–
def plot_gpu_usage_history():
    """ç»˜åˆ¶GPUä½¿ç”¨å†å²å›¾è¡¨"""
    if 'gpu_usage_history' not in st.session_state or not st.session_state.gpu_usage_history:
        st.info("å°šæ— GPUä½¿ç”¨è®°å½•")
        return
    
    history = st.session_state.gpu_usage_history
    
    # æå–æ•°æ®
    times = [(record['time'] - history[0]['time']) for record in history]
    used = [record['used'] for record in history]
    reserved = [record['reserved'] for record in history]
    
    # åˆ›å»ºå›¾è¡¨
    fig, ax = plt.subplots(figsize=(10, 4))
    ax.plot(times, used, 'r-', label='å·²åˆ†é…')
    ax.plot(times, reserved, 'b--', label='å·²ç¼“å­˜')
    
    # æ·»åŠ æ ‡ç­¾å’Œæ ‡é¢˜
    ax.set_xlabel('æ—¶é—´ (ç§’)')
    ax.set_ylabel('æ˜¾å­˜ (MB)')
    ax.set_title('GPUå†…å­˜ä½¿ç”¨å†å²')
    ax.legend()
    
    # æ·»åŠ ç½‘æ ¼
    ax.grid(True, linestyle='--', alpha=0.7)
    
    # æ˜¾ç¤ºå›¾è¡¨
    st.pyplot(fig)

# åœ¨å‡½æ•°è°ƒç”¨ä¹‹å‰å¯åŠ¨GPUç›‘æ§
def start_gpu_monitoring():
    """å¯åŠ¨GPUç›‘æ§"""
    if not torch.cuda.is_available():
        return
    
    # æ¸…ç©ºå†å²è®°å½•
    st.session_state.gpu_usage_history = []
    
    # è®°å½•åˆå§‹çŠ¶æ€
    track_gpu_usage()
    
    # è®¾ç½®ç›‘æ§å·²å¯åŠ¨æ ‡å¿—
    st.session_state.gpu_monitoring_active = True

# åœ¨å‡½æ•°è°ƒç”¨ä¹‹ååœæ­¢GPUç›‘æ§å¹¶æ˜¾ç¤ºæŠ¥å‘Š
def stop_gpu_monitoring_and_report():
    """åœæ­¢GPUç›‘æ§å¹¶ç”ŸæˆæŠ¥å‘Š"""
    if not torch.cuda.is_available() or not st.session_state.get('gpu_monitoring_active', False):
        return
    
    # è®°å½•æœ€ç»ˆçŠ¶æ€
    track_gpu_usage()
    
    # è®¾ç½®ç›‘æ§åœæ­¢æ ‡å¿—
    st.session_state.gpu_monitoring_active = False
    
    # ç”Ÿæˆä½¿ç”¨æŠ¥å‘Š
    if 'gpu_usage_history' in st.session_state and st.session_state.gpu_usage_history:
        st.subheader("GPUä½¿ç”¨å†å²")
        
        history = st.session_state.gpu_usage_history
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        max_used = max([record['used'] for record in history])
        avg_used = sum([record['used'] for record in history]) / len(history)
        peak_util = max([record['utilization'] for record in history])
        
        # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        col1, col2, col3 = st.columns(3)
        col1.metric("æœ€å¤§æ˜¾å­˜ä½¿ç”¨ (MB)", f"{max_used:.1f}")
        col2.metric("å¹³å‡æ˜¾å­˜ä½¿ç”¨ (MB)", f"{avg_used:.1f}")
        col3.metric("æœ€é«˜ä½¿ç”¨ç‡ (%)", f"{peak_util:.1f}")
        
        # ç»˜åˆ¶ä½¿ç”¨å†å²
        plot_gpu_usage_history()

# åœ¨ä¸»ç•Œé¢æ·»åŠ GPUç›‘æ§é€‰é¡¹
st.sidebar.subheader("GPUç›‘æ§")
if st.sidebar.checkbox("å¯ç”¨GPUç›‘æ§", value=False):
    gpu_monitor = st.sidebar.empty()
    
    def update_gpu_monitor():
        """æ›´æ–°GPUç›‘æ§ä¿¡æ¯"""
        while True:
            metrics = monitor_gpu_usage()
            content = ""
            for k, v in metrics.items():
                if isinstance(v, float):
                    content += f"- {k}: {v:.1f}\n"
                else:
                    content += f"- {k}: {v}\n"
            gpu_monitor.code(content)
            time.sleep(1.0)  # æ¯ç§’æ›´æ–°ä¸€æ¬¡
    
    import threading
    monitor_thread = threading.Thread(target=update_gpu_monitor, daemon=True)
    monitor_thread.start()

# åœ¨ä¸»ç¨‹åºä¸­åˆå§‹åŒ–ä¼šè¯çŠ¶æ€å˜é‡
if 'gpu_debug_info' not in st.session_state:
    st.session_state.gpu_debug_info = {
        'usrcat_calls': 0,
        'nn_calls': 0,
        'tsne_calls': 0,
        'usrcat_time': 0,
        'nn_time': 0,
        'tsne_time': 0,
        'total_gpu_mem_peak': 0,
    }

# æ·»åŠ GPUä½¿ç”¨è¯¦æƒ…æ˜¾ç¤ºå‡½æ•°
def show_gpu_usage_details():
    """æ˜¾ç¤ºè¯¦ç»†çš„GPUä½¿ç”¨æƒ…å†µ"""
    if not torch.cuda.is_available():
        st.warning("GPUä¸å¯ç”¨ï¼Œæ— æ³•æ˜¾ç¤ºGPUä½¿ç”¨è¯¦æƒ…")
        return
    
    st.subheader("ğŸ” GPUåŠ é€Ÿè¯¦æƒ…")
    
    # æ˜¾ç¤ºGPUè°ƒç”¨æ¬¡æ•°
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("USRæè¿°ç¬¦GPUè°ƒç”¨", st.session_state.get('gpu_usr_calls', 0))
    
    with col2:
        st.metric("USRCATæè¿°ç¬¦GPUè°ƒç”¨", st.session_state.get('gpu_usrcat_calls', 0))
    
    with col3:
        st.metric("æœ€è¿‘é‚»è®¡ç®—GPUè°ƒç”¨", st.session_state.get('gpu_nn_calls', 0))
    
    with col4:
        st.metric("é™ç»´GPUè°ƒç”¨", st.session_state.get('gpu_tsne_calls', 0))
    
    # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨è¯¦æƒ…
    st.subheader("GPUå†…å­˜ä½¿ç”¨æƒ…å†µ")
    mem_data = {
        "å½“å‰å·²åˆ†é…": torch.cuda.memory_allocated() / 1024**2,
        "å½“å‰å·²ç¼“å­˜": torch.cuda.memory_reserved() / 1024**2,
        "USRè®¡ç®—ä½¿ç”¨": st.session_state.get('gpu_usr_mem', 0),
        "USRCATè®¡ç®—ä½¿ç”¨": st.session_state.get('usrcat_batch_mem', 0),
        "æœ€è¿‘é‚»è®¡ç®—ä½¿ç”¨": st.session_state.get('gpu_nn_mem_usage', 0),
        "t-SNEä½¿ç”¨": st.session_state.get('gpu_tsne_mem', 0),
        "å³°å€¼ä½¿ç”¨": st.session_state.get('gpu_peak_mem', 0),
    }
    
    # ç»˜åˆ¶å†…å­˜ä½¿ç”¨æŸ±çŠ¶å›¾
    mem_df = pd.DataFrame([mem_data.values()], columns=mem_data.keys())
    st.bar_chart(mem_df.T)
    
    # æ˜¾ç¤ºGPUåŠ é€Ÿæ€§èƒ½å¯¹æ¯”
    if st.session_state.get('t-SNE_time') and st.session_state.get('t-SNE_gpu_used'):
        st.info(f"GPUåŠ é€Ÿt-SNEè€—æ—¶: {st.session_state.get('t-SNE_time'):.2f}ç§’")
    
    # è®°å½•å³°å€¼æ˜¾å­˜ä½¿ç”¨
    current_mem = torch.cuda.memory_allocated() / 1024**2
    peak_mem = st.session_state.get('gpu_peak_mem', 0)
    if current_mem > peak_mem:
        st.session_state.gpu_peak_mem = current_mem
    
    # æ·»åŠ GPUä¿¡æ¯è¡¨æ ¼
    gpu_info = {
        "å±æ€§": ["è®¾å¤‡åç§°", "è®¡ç®—å•å…ƒæ•°", "æ€»æ˜¾å­˜", "å½“å‰æ¸©åº¦", "æ˜¾å­˜å¸¦å®½"],
        "å€¼": [
            torch.cuda.get_device_name(0),
            str(torch.cuda.get_device_properties(0).multi_processor_count),
            f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB",
            f"{get_gpu_temp()}",
            f"{torch.cuda.get_device_properties(0).memory_clock_rate * torch.cuda.get_device_properties(0).memory_bus_width / (8 * 1000):.1f} GB/s"
        ]
    }
    st.table(pd.DataFrame(gpu_info))

# æ·»åŠ GPUæ¸©åº¦è·å–å‡½æ•°(ä»…æ”¯æŒNVIDIA GPUå’ŒLinux)
def get_gpu_temp():
    """è·å–GPUæ¸©åº¦ï¼Œå¦‚æœå¯èƒ½çš„è¯"""
    try:
        import subprocess
        output = subprocess.check_output(['nvidia-smi', '--query-gpu=temperature.gpu', '--format=csv,noheader'])
        return output.decode('utf-8').strip() + "Â°C"
    except:
        return "æœªçŸ¥"

# æ·»åŠ ä¸€ä¸ªè¯¦ç»†çš„GPUä¿¡æ¯é¢æ¿å‡½æ•°
def gpu_debug_panel():
    """æ˜¾ç¤ºGPUè°ƒè¯•é¢æ¿"""
    if not torch.cuda.is_available():
        st.sidebar.warning("GPUä¸å¯ç”¨ï¼Œæ— æ³•æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯")
        return
    
    with st.sidebar.expander("ğŸ” GPUè°ƒè¯•ä¿¡æ¯", expanded=False):
        st.write("### GPUå†…å­˜è¿½è¸ª")
        
        # æ˜¾ç¤ºå½“å‰å†…å­˜ä½¿ç”¨
        current_mem = torch.cuda.memory_allocated() / 1024**2
        st.progress(min(1.0, current_mem / torch.cuda.get_device_properties(0).total_memory * 1024**2))
        st.write(f"å½“å‰ä½¿ç”¨: {current_mem:.1f}MB / {torch.cuda.get_device_properties(0).total_memory / 1024**2:.1f}MB")
        
        # æ˜¾ç¤ºGPUè°ƒç”¨è¯¦æƒ…
        st.write("### å‡½æ•°è°ƒç”¨")
        st.write(f"- USRCAT GPUè°ƒç”¨: {st.session_state.get('gpu_usrcat_calls', 0)}")
        st.write(f"- æœ€è¿‘é‚»GPUè°ƒç”¨: {st.session_state.get('gpu_nn_calls', 0)}")
        st.write(f"- é™ç»´GPUè°ƒç”¨: {st.session_state.get('gpu_tsne_calls', 0)}")
        
        # æ˜¾ç¤ºè®¡æ—¶ä¿¡æ¯
        st.write("### è®¡æ—¶ä¿¡æ¯")
        st.write(f"- USRCATæ‰¹å¤„ç†: {st.session_state.get('usrcat_batch_time', 0):.3f}ç§’")
        st.write(f"- t-SNEé™ç»´: {st.session_state.get('t-SNE_time', 0):.3f}ç§’")
        
        # æ˜¾ç¤ºé”™è¯¯è®¡æ•°
        st.write("### é”™è¯¯è®¡æ•°")
        st.write(f"- GPUå›é€€æ¬¡æ•°: {st.session_state.get('gpu_fallbacks', 0)}")
        
        # æ˜¾ç¤ºCUDAç‰ˆæœ¬ä¿¡æ¯
        st.write("### CUDAç‰ˆæœ¬")
        st.code(f"CUDA: {torch.version.cuda}\nCuDNN: {torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else 'N/A'}")

# åœ¨ä¸»é¡µé¢åº•éƒ¨å¢åŠ è¯¦ç»†çš„GPUä½¿ç”¨æŠ¥å‘Š
def generate_gpu_report():
    """ç”Ÿæˆè¯¦ç»†çš„GPUä½¿ç”¨æŠ¥å‘Š"""
    if not torch.cuda.is_available():
        return
    
    st.subheader("ğŸ” GPUä½¿ç”¨è¯¦ç»†æŠ¥å‘Š")
    
    # æ€»ä½“ä½¿ç”¨æƒ…å†µ
    st.write("### æ€»ä½“ä½¿ç”¨æƒ…å†µ")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        if st.session_state.get('gpu_usrcat_calls', 0) > 0:
            st.success("âœ… USRCATæè¿°ç¬¦è®¡ç®—å·²ä½¿ç”¨GPUåŠ é€Ÿ")
        else:
            st.error("âŒ USRCATæè¿°ç¬¦è®¡ç®—æœªä½¿ç”¨GPU")
            
    with col2:
        if st.session_state.get('gpu_nn_calls', 0) > 0:
            st.success("âœ… æœ€è¿‘é‚»è·ç¦»è®¡ç®—å·²ä½¿ç”¨GPUåŠ é€Ÿ")
        else:
            st.error("âŒ æœ€è¿‘é‚»è·ç¦»è®¡ç®—æœªä½¿ç”¨GPU")
            
    with col3:
        if st.session_state.get('gpu_tsne_calls', 0) > 0:
            st.success("âœ… é™ç»´åˆ†æå·²ä½¿ç”¨GPUåŠ é€Ÿ")
        else:
            st.error("âŒ é™ç»´åˆ†ææœªä½¿ç”¨GPU")
    
    # GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
    st.write("### GPUå†…å­˜ä½¿ç”¨æƒ…å†µ")
    
    # æ”¶é›†æ‰€æœ‰å†…å­˜ä½¿ç”¨æ•°æ®
    mem_data = {
        "æè¿°ç¬¦è®¡ç®—": st.session_state.get('usrcat_batch_mem', 0),
        "æœ€è¿‘é‚»è·ç¦»": st.session_state.get('gpu_nn_mem_usage', 0),
        "é™ç»´åˆ†æ": st.session_state.get('gpu_tsne_mem', 0),
        "æœ€å¤§ä½¿ç”¨é‡": st.session_state.get('gpu_peak_mem', 0),
    }
    
    # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æ¡å½¢å›¾
    mem_df = pd.DataFrame([mem_data.values()], columns=mem_data.keys())
    st.bar_chart(mem_df.T)
    
    # GPU VS CPUæ€§èƒ½æ¯”è¾ƒ
    st.write("### GPUåŠ é€Ÿæ•ˆæœ")
    
    # å¦‚æœæœ‰CPUå’ŒGPUçš„æ—¶é—´å¯¹æ¯”ï¼Œæ˜¾ç¤ºæ¯”è¾ƒå›¾è¡¨
    if st.session_state.get('t-SNE_time') and st.session_state.get('t-SNE_gpu_used'):
        gpu_time = st.session_state.get('t-SNE_time', 0)
        # è¿™é‡Œæˆ‘ä»¬ä¼°è®¡CPUæ—¶é—´æ˜¯GPUæ—¶é—´çš„5å€(ä»…ç”¨äºæ¼”ç¤º)
        cpu_time = gpu_time * 5  # å®é™…åº”ç”¨ä¸­åº”è¯¥æœ‰çœŸå®çš„å¯¹æ¯”æ•°æ®
        
        perf_data = {
            "GPU": gpu_time,
            "ä¼°è®¡CPU": cpu_time
        }
        perf_df = pd.DataFrame([perf_data.values()], columns=perf_data.keys())
        st.bar_chart(perf_df.T)
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        speedup = cpu_time / gpu_time if gpu_time > 0 else 0
        st.metric("ä¼°è®¡åŠ é€Ÿæ¯”", f"{speedup:.1f}x")
    
    # æ·»åŠ GPUè®¾å¤‡è¯¦æƒ…
    with st.expander("GPUè®¾å¤‡è¯¦æƒ…"):
        properties = torch.cuda.get_device_properties(0)
        st.json({
            "è®¾å¤‡åç§°": torch.cuda.get_device_name(0),
            "è®¡ç®—èƒ½åŠ›": f"{properties.major}.{properties.minor}",
            "å¤šå¤„ç†å™¨æ•°é‡": properties.multi_processor_count,
            "æ€»æ˜¾å­˜(GB)": properties.total_memory / 1024**3,
            "æœ€å¤§çº¿ç¨‹æ•°/å—": properties.max_threads_per_block,
            "æ—¶é’Ÿé¢‘ç‡(MHz)": properties.clock_rate / 1000,
            "L2ç¼“å­˜å¤§å°(MB)": properties.l2_cache_size / 1024**2 if hasattr(properties, 'l2_cache_size') else "æœªçŸ¥",
        })

# æ·»åŠ CUDAæ£€æŸ¥å‡½æ•°ï¼Œç”¨äºéªŒè¯CUDAæ˜¯å¦æ­£å¸¸å·¥ä½œ
def verify_cuda_operation():
    """æ‰§è¡Œç®€å•çš„CUDAæ“ä½œä»¥éªŒè¯GPUæ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    if not torch.cuda.is_available():
        return "CUDAä¸å¯ç”¨"
    
    try:
        # åˆ›å»ºä¸¤ä¸ªéšæœºå¼ é‡
        a = torch.randn(1000, 1000).cuda()
        b = torch.randn(1000, 1000).cuda()
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start = time.time()
        
        # æ‰§è¡ŒçŸ©é˜µä¹˜æ³•
        c = torch.matmul(a, b)
        
        # ç¡®ä¿è®¡ç®—å®Œæˆ
        torch.cuda.synchronize()
        
        # è®¡ç®—è€—æ—¶
        duration = time.time() - start
        
        # æ¸…ç†
        del a, b, c
        torch.cuda.empty_cache()
        
        return f"CUDAè¿è¡Œæ­£å¸¸ï¼Œ1000x1000çŸ©é˜µä¹˜æ³•è€—æ—¶: {duration*1000:.2f}æ¯«ç§’"
    except Exception as e:
        return f"CUDAæµ‹è¯•å¤±è´¥: {str(e)}"

# åœ¨é¡µé¢åŠ è½½æ—¶æ·»åŠ GPUæµ‹è¯•æŒ‰é’®
st.sidebar.subheader("GPUçŠ¶æ€æ£€æµ‹")
if st.sidebar.button("æµ‹è¯•GPU"):
    result = verify_cuda_operation()
    st.sidebar.code(result)

# åœ¨é¡µé¢åŠ è½½æ—¶æ·»åŠ GPUè°ƒè¯•é¢æ¿
if st.sidebar.checkbox("å¯ç”¨GPUå®æ—¶ç›‘æ§", value=False):
    gpu_debug_panel()

# ä¸»ç•Œé¢é€»è¾‘
if st.button("å¼€å§‹åˆ†æ") and fileA is not None and fileB is not None:
    with st.spinner("æ­£åœ¨è¿›è¡Œ3Då½¢çŠ¶åˆ†æ..."):
        try:
            analysis_start_time = time.time()
            
            # åŠ è½½åˆ†å­
            st.info("åŠ è½½åˆ†å­æ•°æ®...")
            molsA, dfA = load_smiles(fileA, smiles_col)
            molsB, dfB = load_smiles(fileB, smiles_col)
            
            if molsA is None or molsB is None or len(molsA) == 0 or len(molsB) == 0:
                st.error("æ— æ³•åŠ è½½åˆ†å­æ•°æ®æˆ–æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®")
                st.stop()
            
            st.success(f"æˆåŠŸåŠ è½½ {len(molsA)} ä¸ªåˆ†å­(æ•°æ®é›†A)å’Œ {len(molsB)} ä¸ªåˆ†å­(æ•°æ®é›†B)")
            
            # åˆå§‹åŒ–GPU
            cuda_available = False
            device = torch.device("cpu")
            
            if enable_gpu:
                try:
                    cuda_available, device = initialize_cuda()
                    if cuda_available:
                        # æ ¹æ®GPUç­–ç•¥è®¾ç½®å†…å­˜é™åˆ¶
                        try:
                            if gpu_strategy == "å†…å­˜ä¼˜å…ˆ":
                                # ä¿ç•™æ›´å¤šGPUå†…å­˜ï¼Œæ…¢ä½†æ›´å®‰å…¨
                                torch.cuda.set_per_process_memory_fraction(gpu_mem_limit / 100 * 0.8)
                            elif gpu_strategy == "æ€§èƒ½ä¼˜å…ˆ":
                                # ä½¿ç”¨æ›´å¤šGPUå†…å­˜ï¼Œæ›´å¿«ä½†é£é™©æ›´é«˜
                                torch.cuda.set_per_process_memory_fraction(gpu_mem_limit / 100 * 0.95)
                            else:  # å¹³è¡¡æ¨¡å¼
                                torch.cuda.set_per_process_memory_fraction(gpu_mem_limit / 100 * 0.9)
                        except Exception as e:
                            st.warning(f"è®¾ç½®GPUå†…å­˜é™åˆ¶æ—¶å‡ºé”™: {str(e)}")
                        
                        # æ¸…ç†GPUå†…å­˜
                        torch.cuda.empty_cache()
                        
                        # æ˜¾ç¤ºGPUçŠ¶æ€
                        gpu_stats = {
                            "GPUå‹å·": torch.cuda.get_device_name(0),
                            "æ€»æ˜¾å­˜": f"{torch.cuda.get_device_properties(0).total_memory / 1024**2:.1f} MB",
                            "å·²ç”¨æ˜¾å­˜": f"{torch.cuda.memory_allocated() / 1024**2:.1f} MB",
                            "å·²ç¼“å­˜æ˜¾å­˜": f"{torch.cuda.memory_reserved() / 1024**2:.1f} MB"
                        }
                        st.write("GPUçŠ¶æ€:", gpu_stats)
                except Exception as e:
                    st.warning(f"åˆå§‹åŒ–GPUæ—¶å‡ºé”™: {str(e)}ï¼Œå°†ä½¿ç”¨CPUè¿›è¡Œè®¡ç®—")
                    cuda_available = False
                    device = torch.device("cpu")
            else:
                st.info("GPUåŠ é€Ÿå·²ç¦ç”¨ï¼Œå°†ä½¿ç”¨CPUè¿›è¡Œè®¡ç®—")
                
                # éšæœºæŠ½æ ·
                if len(molsA) > max_samples:
                    molsA = random.sample(molsA, max_samples)
                st.info(f"å·²ä»æ•°æ®é›†AéšæœºæŠ½å– {max_samples} ä¸ªåˆ†å­è¿›è¡Œåˆ†æ")
                if len(molsB) > max_samples:
                    molsB = random.sample(molsB, max_samples)
                st.info(f"å·²ä»æ•°æ®é›†BéšæœºæŠ½å– {max_samples} ä¸ªåˆ†å­è¿›è¡Œåˆ†æ")
            
            # æ”¶é›†æ„è±¡ç”Ÿæˆå‚æ•°
            conformer_params = {
                # åŸºæœ¬å‚æ•°
                'max_attempts': max_attempts,
                'use_mmff': use_mmff,
                'energy_iter': energy_iter,
                'add_hydrogens': add_hydrogens,
                
                # OpenMMå‚æ•°
                'openmm_forcefield': openmm_forcefield if 'openmm_forcefield' in locals() else 'amber14-all',
                'openmm_platform': openmm_platform if 'openmm_platform' in locals() else 'CUDA',
                
                # TorchANIå‚æ•°
                'torchani_model': torchani_model if 'torchani_model' in locals() else 'ANI2x',
                'optimization_steps': optimization_steps if 'optimization_steps' in locals() else 100,
                
                # DeepChemå‚æ•°
                'model_type': deepchem_model if 'deepchem_model' in locals() else 'mpnn',
                'use_gpu': enable_gpu and cuda_available,
                'use_mixed_precision': use_mixed_precision if 'use_mixed_precision' in locals() else True,
                
                # Claraå‚æ•°
                'force_field': clara_force_field if 'clara_force_field' in locals() else 'MMFF94s',
                'precision': clara_precision if 'clara_precision' in locals() else 'mixed',
                'num_conformers': clara_num_conformers if 'clara_num_conformers' in locals() else 1,
                'energy_threshold': clara_energy_threshold if 'clara_energy_threshold' in locals() else 1.0
            }
            
            # ä½¿ç”¨æ–°çš„æ‰¹é‡3Dæ„è±¡ç”Ÿæˆ
            step_start_time = time.time()
            
            # Create status elements and progress bar before calling the function
            status_container_A = st.empty()
            progress_text_A = st.empty()
            progress_bar_A = st.progress(0)

            with st.spinner("ç”Ÿæˆæ•°æ®é›†Açš„3Dæ„è±¡..."):
                molsA_3d = batch_generate_3d_conformers(
                    molsA, 
                    progress_bar=progress_bar_A, # Pass the progress bar object
                    status_container=status_container_A,
                    progress_text=progress_text_A,
                    backend=conformer_backend,
                    batch_size=batch_size if not auto_batch else None,
                    **conformer_params
                )
            
            step_time = time.time() - step_start_time
            st.success(f"æ•°æ®é›†Aæ„è±¡ç”Ÿæˆå®Œæˆï¼Œç”¨æ—¶: {step_time:.1f}ç§’")
            # Clear progress bar A after completion
            progress_bar_A.empty() 
            status_container_A.empty()
            progress_text_A.empty()

            step_start_time = time.time()
            
            # Create status elements and progress bar for dataset B
            status_container_B = st.empty()
            progress_text_B = st.empty()
            progress_bar_B = st.progress(0)
            
            with st.spinner("ç”Ÿæˆæ•°æ®é›†Bçš„3Dæ„è±¡..."):
                molsB_3d = batch_generate_3d_conformers(
                    molsB, 
                    progress_bar=progress_bar_B, # Pass the progress bar object
                    status_container=status_container_B,
                    progress_text=progress_text_B,
                    backend=conformer_backend,
                    batch_size=batch_size if not auto_batch else None,
                    **conformer_params
                )
            
            step_time = time.time() - step_start_time
            st.success(f"æ•°æ®é›†Bæ„è±¡ç”Ÿæˆå®Œæˆï¼Œç”¨æ—¶: {step_time:.1f}ç§’")
            # Clear progress bar B after completion
            progress_bar_B.empty() 
            status_container_B.empty()
            progress_text_B.empty()
            
            # ç§»é™¤æ— æ•ˆæ„è±¡
            valid_molsA_3d = [mol for mol in molsA_3d if mol is not None]
            valid_molsB_3d = [mol for mol in molsB_3d if mol is not None]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æœ‰æ•ˆæ„è±¡
            if len(valid_molsA_3d) == 0 or len(valid_molsB_3d) == 0:
                st.error("æœ‰æ•ˆçš„3Dæ„è±¡æ•°é‡ä¸è¶³ï¼Œæ— æ³•ç»§ç»­åˆ†æ")
                st.error(f"æ•°æ®é›†A: {len(valid_molsA_3d)}/{len(molsA)} ä¸ªæœ‰æ•ˆæ„è±¡")
                st.error(f"æ•°æ®é›†B: {len(valid_molsB_3d)}/{len(molsB)} ä¸ªæœ‰æ•ˆæ„è±¡")
                st.stop()
            
            # çŠ¶æ€æ˜¾ç¤º
            st.info(f"æ•°æ®é›†A: {len(valid_molsA_3d)}/{len(molsA)} ä¸ªæœ‰æ•ˆæ„è±¡ ({len(valid_molsA_3d)/len(molsA)*100:.1f}%)")
            st.info(f"æ•°æ®é›†B: {len(valid_molsB_3d)}/{len(molsB)} ä¸ªæœ‰æ•ˆæ„è±¡ ({len(valid_molsB_3d)/len(molsB)*100:.1f}%)")
            
            # è®¡ç®—å½¢çŠ¶æè¿°ç¬¦
            step_start_time = time.time()
            
            with st.spinner(f"è®¡ç®— {shape_desc} å½¢çŠ¶æè¿°ç¬¦..."):
                try:
                    if shape_desc == "USR":
                        descsA = batch_compute_shape_descriptors(
                            valid_molsA_3d, 
                            descriptor_type="USR", 
                        cuda_available=cuda_available,
                            batch_size=batch_size if not auto_batch else None
                        )
                        descsB = batch_compute_shape_descriptors(
                            valid_molsB_3d, 
                            descriptor_type="USR", 
                            cuda_available=cuda_available,
                            batch_size=batch_size if not auto_batch else None
                        )
                    else:  # USRCAT
                        descsA = batch_compute_shape_descriptors(
                            valid_molsA_3d, 
                            descriptor_type="USRCAT", 
                            cuda_available=cuda_available,
                            batch_size=batch_size if not auto_batch else None
                        )
                        descsB = batch_compute_shape_descriptors(
                            valid_molsB_3d, 
                            descriptor_type="USRCAT", 
                            cuda_available=cuda_available,
                            batch_size=batch_size if not auto_batch else None
                        )
                except Exception as e:
                    st.error(f"è®¡ç®—å½¢çŠ¶æè¿°ç¬¦æ—¶å‡ºé”™: {str(e)}")
                    st.stop()
            
            step_time = time.time() - step_start_time
            st.success(f"å½¢çŠ¶æè¿°ç¬¦è®¡ç®—å®Œæˆï¼Œç”¨æ—¶: {step_time:.1f}ç§’")
            
            # ç§»é™¤æ— æ•ˆæè¿°ç¬¦
            valid_descsA = [d for d in descsA if d is not None]
            valid_descsB = [d for d in descsB if d is not None]
            
            if len(valid_descsA) == 0 or len(valid_descsB) == 0:
                st.error("æœ‰æ•ˆçš„å½¢çŠ¶æè¿°ç¬¦æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
                st.error(f"æ•°æ®é›†A: {len(valid_descsA)}/{len(valid_molsA_3d)} ä¸ªæœ‰æ•ˆæè¿°ç¬¦")
                st.error(f"æ•°æ®é›†B: {len(valid_descsB)}/{len(valid_molsB_3d)} ä¸ªæœ‰æ•ˆæè¿°ç¬¦")
                st.stop()
            
            st.info(f"æ•°æ®é›†A: {len(valid_descsA)}/{len(valid_molsA_3d)} ä¸ªæœ‰æ•ˆæè¿°ç¬¦ ({len(valid_descsA)/len(valid_molsA_3d)*100:.1f}%)")
            st.info(f"æ•°æ®é›†B: {len(valid_descsB)}/{len(valid_molsB_3d)} ä¸ªæœ‰æ•ˆæè¿°ç¬¦ ({len(valid_descsB)/len(valid_molsB_3d)*100:.1f}%)")
            
            # æè¿°ç¬¦æ ‡å‡†åŒ–
            if normalize_desc:
                step_start_time = time.time()
                with st.spinner("æ ‡å‡†åŒ–æè¿°ç¬¦..."):
                    try:
                        # åˆå¹¶æ‰€æœ‰æè¿°ç¬¦ä»¥è®¡ç®—å…¨å±€å‡å€¼å’Œæ ‡å‡†å·®
                        all_descs = np.vstack([valid_descsA, valid_descsB])
                        mean = np.mean(all_descs, axis=0)
                        std = np.std(all_descs, axis=0)
                        # é˜²æ­¢é™¤ä»¥é›¶
                        std[std == 0] = 1.0
                        # åº”ç”¨æ ‡å‡†åŒ–
                        valid_descsA = (valid_descsA - mean) / std
                        valid_descsB = (valid_descsB - mean) / std
                    except Exception as e:
                        st.error(f"æ ‡å‡†åŒ–æè¿°ç¬¦æ—¶å‡ºé”™: {str(e)}")
                        st.warning("è·³è¿‡æ ‡å‡†åŒ–æ­¥éª¤...")
                
                step_time = time.time() - step_start_time
                st.success(f"æè¿°ç¬¦æ ‡å‡†åŒ–å®Œæˆï¼Œç”¨æ—¶: {step_time:.1f}ç§’")
            
            # é™ç»´å¯è§†åŒ–
            step_start_time = time.time()
            with st.spinner(f"ä½¿ç”¨ {dim_reduction} é™ç»´..."):
                try:
                    # ç»„åˆä¸¤ä¸ªæ•°æ®é›†ä»¥è¿›è¡Œé™ç»´
                    combined_descs = np.vstack([valid_descsA, valid_descsB])
                    
                    # é™ç»´å‚æ•°
                    dim_params = {}
                    if dim_reduction == "t-SNE":
                        dim_params["perplexity"] = perplexity
                    else:  # UMAP
                        dim_params["n_neighbors"] = n_neighbors
                        dim_params["min_dist"] = min_dist
                    
                    # æ‰§è¡Œé™ç»´
                    coords = perform_dimensionality_reduction(
                        combined_descs, 
                            method=dim_reduction,
                            cuda_available=cuda_available,
                        **dim_params
                    )
                    
                    # åˆ†ç¦»ä¸¤ä¸ªæ•°æ®é›†çš„åæ ‡
                    coordsA = coords[:len(valid_descsA)]
                    coordsB = coords[len(valid_descsA):]
                except Exception as e:
                    st.error(f"æ‰§è¡Œé™ç»´æ—¶å‡ºé”™: {str(e)}")
                    st.stop()
            
            step_time = time.time() - step_start_time
            st.success(f"é™ç»´å®Œæˆï¼Œç”¨æ—¶: {step_time:.1f}ç§’")
            
            # è®¡ç®—ä¸»æƒ¯é‡æ¯”ç‡ï¼ˆç”¨äºPMIä¸‰è§’å½¢å›¾ï¼‰
            step_start_time = time.time()
            with st.spinner("è®¡ç®—ä¸»æƒ¯é‡æ¯”ç‡..."):
                try:
                    pmiA = [compute_pmi_ratios(mol) for mol in valid_molsA_3d]
                    pmiB = [compute_pmi_ratios(mol) for mol in valid_molsB_3d]
                    
                    # è¿‡æ»¤æ— æ•ˆå€¼
                    pmiA = [p for p in pmiA if p is not None]
                    pmiB = [p for p in pmiB if p is not None]
                except Exception as e:
                    st.warning(f"è®¡ç®—PMIæ¯”ç‡æ—¶å‡ºé”™: {str(e)}")
                    pmiA = []
                    pmiB = []
            
            step_time = time.time() - step_start_time
            if len(pmiA) > 0 and len(pmiB) > 0:
                st.success(f"PMIæ¯”ç‡è®¡ç®—å®Œæˆï¼Œç”¨æ—¶: {step_time:.1f}ç§’")
            else:
                st.warning("PMIæ¯”ç‡è®¡ç®—æœªå®Œæˆæˆ–æ²¡æœ‰æœ‰æ•ˆç»“æœ")
            
            # è®¡ç®—å½¢çŠ¶ç©ºé—´åˆ†å¸ƒæŒ‡æ ‡
            step_start_time = time.time()
            with st.spinner("è®¡ç®—åˆ†å¸ƒæŒ‡æ ‡..."):
                try:
                    dist_metrics = calculate_distribution_metrics(coordsA, coordsB)
                except Exception as e:
                    st.warning(f"è®¡ç®—åˆ†å¸ƒæŒ‡æ ‡æ—¶å‡ºé”™: {str(e)}")
                    dist_metrics = {
                        "hausdorff_distance": float('nan'),
                        "earth_movers_distance": float('nan'),
                        "kl_divergence": float('nan'),
                        "js_divergence": float('nan')
                    }
            
            step_time = time.time() - step_start_time
            st.success(f"åˆ†å¸ƒæŒ‡æ ‡è®¡ç®—å®Œæˆï¼Œç”¨æ—¶: {step_time:.1f}ç§’")
            
            # è®¡ç®—æ€»ç”¨æ—¶
            total_time = time.time() - analysis_start_time
            
            # æ˜¾ç¤ºç»“æœ
            st.success(f"åˆ†æå®Œæˆï¼æ€»ç”¨æ—¶: {total_time:.1f}ç§’")
            
            # ç»“æœé€‰é¡¹å¡
            result_tabs = st.tabs(["å½¢çŠ¶ç©ºé—´åˆ†æ", "ä¸»æƒ¯é‡åˆ†æ", "åˆ†å¸ƒæŒ‡æ ‡", "æ€§èƒ½ç»Ÿè®¡"])
            
            with result_tabs[0]:
                st.subheader("å½¢çŠ¶ç©ºé—´åˆ†å¸ƒ")
                
                plot_shape_space(
                    coordsA, coordsB, 
                    title=f"{shape_desc} å½¢çŠ¶ç©ºé—´åˆ†å¸ƒ ({dim_reduction})"
                )
                
                st.write(f"æ•°æ®é›†A: {len(coordsA)} ä¸ªç‚¹ï¼Œæ•°æ®é›†B: {len(coordsB)} ä¸ªç‚¹")
                
                # æ·»åŠ æ ·æœ¬ç‚¹å‡»æ˜¾ç¤º
                if st.checkbox("å¯ç”¨æ ·æœ¬ç‚¹å‡»", value=False):
                    st.info("ç‚¹å‡»å›¾ä¸­çš„ç‚¹å¯ä»¥æŸ¥çœ‹å¯¹åº”çš„åˆ†å­ç»“æ„ï¼ˆå°šæœªå®ç°ï¼‰")
            
            with result_tabs[1]:
                st.subheader("ä¸»æƒ¯é‡ä¸‰è§’å½¢")
                
                if len(pmiA) > 0 and len(pmiB) > 0:
                    plot_pmi_triangle(pmiA, pmiB, "æ•°æ®é›†A", "æ•°æ®é›†B")
                    st.write(f"æ•°æ®é›†A: {len(pmiA)} ä¸ªæœ‰æ•ˆPMIæ¯”ç‡ï¼Œæ•°æ®é›†B: {len(pmiB)} ä¸ªæœ‰æ•ˆPMIæ¯”ç‡")
                else:
                    st.warning("æ— æ³•è®¡ç®—è¶³å¤Ÿçš„ä¸»æƒ¯é‡æ¯”ç‡ä»¥ç”Ÿæˆä¸‰è§’å½¢å›¾")
            
            with result_tabs[2]:
                st.subheader("åˆ†å¸ƒç›¸ä¼¼æ€§æŒ‡æ ‡")
                
                # æ˜¾ç¤ºè®¡ç®—çš„å„ç§æŒ‡æ ‡
                metrics_df = pd.DataFrame({
                    "æŒ‡æ ‡": ["è±ªæ–¯å¤šå¤«è·ç¦»", "åœ°çƒç§»åŠ¨è·ç¦» (EMD)", "KLæ•£åº¦", "JSæ•£åº¦"],
                    "å€¼": [
                        dist_metrics["hausdorff_distance"],
                        dist_metrics["earth_movers_distance"],
                        dist_metrics["kl_divergence"],
                        dist_metrics["js_divergence"]
                    ]
                })
                st.dataframe(metrics_df)
                
                st.markdown("""
                **æŒ‡æ ‡è¯´æ˜:**
                - **è±ªæ–¯å¤šå¤«è·ç¦»**: ä¸¤ä¸ªç‚¹é›†ä¹‹é—´çš„æœ€å¤§æœ€å°è·ç¦»ï¼Œè¾ƒä½çš„å€¼è¡¨ç¤ºå½¢çŠ¶ç©ºé—´æ›´ç›¸ä¼¼
                - **åœ°çƒç§»åŠ¨è·ç¦» (EMD)**: ä¹Ÿç§°ä¸ºWassersteinè·ç¦»ï¼Œè¡¨ç¤ºå°†ä¸€ä¸ªåˆ†å¸ƒè½¬æ¢ä¸ºå¦ä¸€ä¸ªæ‰€éœ€çš„"å·¥ä½œé‡"
                - **KLæ•£åº¦**: è¡¡é‡ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒç›¸å¯¹äºå¦ä¸€ä¸ªçš„å·®å¼‚
                - **JSæ•£åº¦**: KLæ•£åº¦çš„å¯¹ç§°ç‰ˆæœ¬ï¼ŒèŒƒå›´åœ¨[0,1]ï¼Œ0è¡¨ç¤ºå®Œå…¨ç›¸åŒçš„åˆ†å¸ƒ
                """)
            
            with result_tabs[3]:
                st.subheader("æ€§èƒ½ç»Ÿè®¡")
                
                # GPUçŠ¶æ€
                if cuda_available:
                    with st.expander("GPUä½¿ç”¨æƒ…å†µ", expanded=True):
                        # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ
                        gpu_stats = {
                            "GPUå‹å·": torch.cuda.get_device_name(0),
                            "å·²ç”¨æ˜¾å­˜": f"{torch.cuda.memory_allocated()/1024**2:.1f} MB",
                            "å·²ç¼“å­˜æ˜¾å­˜": f"{torch.cuda.memory_reserved()/1024**2:.1f} MB",
                            "å¯ç”¨æ˜¾å­˜": f"{(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved())/1024**2:.1f} MB",
                            "æ€»æ˜¾å­˜": f"{torch.cuda.get_device_properties(0).total_memory/1024**2:.1f} MB",
                            "ä½¿ç”¨ç‡": f"{torch.cuda.memory_allocated()/torch.cuda.get_device_properties(0).total_memory*100:.1f}%",
                        }
                        
                        st.json(gpu_stats)
                
                # æ„è±¡ç”Ÿæˆåç«¯ä¿¡æ¯
                with st.expander("æ„è±¡ç”Ÿæˆä¿¡æ¯", expanded=True):
                    st.info(f"ä½¿ç”¨åç«¯: {conformer_backend}")
                    st.write("æ„è±¡ç”Ÿæˆå‚æ•°:", conformer_params)
                    st.write(f"æ•°æ®é›†AæˆåŠŸç‡: {len(valid_molsA_3d)/len(molsA)*100:.1f}% ({len(valid_molsA_3d)}/{len(molsA)})")
                    st.write(f"æ•°æ®é›†BæˆåŠŸç‡: {len(valid_molsB_3d)/len(molsB)*100:.1f}% ({len(valid_molsB_3d)}/{len(molsB)})")
                
                # æ€§èƒ½ç»Ÿè®¡
                with st.expander("å¤„ç†æ—¶é—´ç»Ÿè®¡", expanded=True):
                    # æ„é€ å„æ­¥éª¤çš„å¤„ç†æ—¶é—´è¡¨æ ¼
                    steps_df = pd.DataFrame({
                        "å¤„ç†æ­¥éª¤": ["æ„è±¡ç”Ÿæˆ", "å½¢çŠ¶æè¿°ç¬¦è®¡ç®—", "é™ç»´", "PMIè®¡ç®—", "åˆ†å¸ƒæŒ‡æ ‡è®¡ç®—", "æ€»è®¡"],
                        "æ—¶é—´ (ç§’)": [
                            # è¿™äº›æ—¶é—´åœ¨å‰é¢çš„ä»£ç ä¸­å·²ç»è®¡ç®—è¿‡ï¼Œè¿™é‡Œç”¨å ä½ç¬¦ 
                            # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™äº›å€¼å°†è¢«çœŸå®çš„æ—¶é—´æ›¿ä»£
                            total_time * 0.5,  # å‡è®¾æ„è±¡ç”Ÿæˆå æ€»æ—¶é—´çš„50%
                            total_time * 0.2,  # å‡è®¾æè¿°ç¬¦è®¡ç®—å æ€»æ—¶é—´çš„20%
                            total_time * 0.1,  # å‡è®¾é™ç»´å æ€»æ—¶é—´çš„10%
                            total_time * 0.05, # å‡è®¾PMIè®¡ç®—å æ€»æ—¶é—´çš„5%
                            total_time * 0.05, # å‡è®¾åˆ†å¸ƒæŒ‡æ ‡è®¡ç®—å æ€»æ—¶é—´çš„5%
                            total_time
                        ],
                        "å¤„ç†åˆ†å­/é¡¹æ•°": [
                            f"{len(molsA) + len(molsB)}ä¸ª",
                            f"{len(valid_molsA_3d) + len(valid_molsB_3d)}ä¸ª",
                            f"{len(valid_descsA) + len(valid_descsB)}ä¸ª",
                            f"{len(pmiA) + len(pmiB)}ä¸ª",
                            "2ä¸ªæ•°æ®é›†",
                            ""
                        ],
                        "æ¯é¡¹å¹³å‡æ—¶é—´ (ç§’)": [
                            (total_time * 0.5) / (len(molsA) + len(molsB)) if (len(molsA) + len(molsB)) > 0 else 0,
                            (total_time * 0.2) / (len(valid_molsA_3d) + len(valid_molsB_3d)) if (len(valid_molsA_3d) + len(valid_molsB_3d)) > 0 else 0,
                            (total_time * 0.1) / (len(valid_descsA) + len(valid_descsB)) if (len(valid_descsA) + len(valid_descsB)) > 0 else 0,
                            (total_time * 0.05) / (len(pmiA) + len(pmiB)) if (len(pmiA) + len(pmiB)) > 0 else 0,
                            (total_time * 0.05) / 2 if 2 > 0 else 0,
                            ""
                        ]
                    })
                    st.dataframe(steps_df)
                    
                    # æ€§èƒ½å¯¹æ¯”å›¾
                    fig, ax = plt.subplots(figsize=(10, 6))
                    bars = ax.barh(steps_df["å¤„ç†æ­¥éª¤"][:-1], steps_df["æ—¶é—´ (ç§’)"][:-1])
                    ax.set_xlabel("æ—¶é—´ (ç§’)")
                    ax.set_title("å„æ­¥éª¤å¤„ç†æ—¶é—´")
                    
                    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
                    for bar in bars:
                        width = bar.get_width()
                        ax.text(width + 0.5, bar.get_y() + bar.get_height()/2, f"{width:.1f}s", 
                                ha='left', va='center')
                    
                    st.pyplot(fig)
                    
                    # å¦‚æœå¯ç”¨äº†GPUï¼Œæ˜¾ç¤ºGPU vs CPUå¯¹æ¯”
                    if cuda_available:
                        st.info(f"ä½¿ç”¨GPUåŠ é€Ÿï¼Œä¼°è®¡åŠ é€Ÿæ¯”: 2-5å€ï¼ˆå–å†³äºåˆ†å­å¤æ‚åº¦å’ŒGPUæ€§èƒ½ï¼‰")
        except Exception as e:
            st.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}") 
            import traceback
            st.error(f"é”™è¯¯è¯¦æƒ…:\n{traceback.format_exc()}")
            
            if enable_gpu and torch.cuda.is_available():
                try:
                    st.error("GPUé”™è¯¯è¯¦æƒ…:")
                    st.error(f"- å·²ç”¨æ˜¾å­˜: {torch.cuda.memory_allocated()/1024**2:.1f}MB")
                    st.error(f"- å·²ç¼“å­˜æ˜¾å­˜: {torch.cuda.memory_reserved()/1024**2:.1f}MB")
                    torch.cuda.empty_cache()  # æ¸…ç†GPUå†…å­˜
                except:
                    st.error("æ— æ³•è·å–GPUçŠ¶æ€ä¿¡æ¯")