"""
åˆ†å­åº“ä»£è¡¨æ€§å­é›†é€‰æ‹©ç³»ç»Ÿ - ç»“æ„å¤šæ ·æ€§è¯„ä¼°é¡µé¢
"""
import os
# å¿…é¡»åœ¨ **å¯¼å…¥ streamlit ä¹‹å‰** è®¾ç½®
# (æ­¤å¤„çœç•¥ STREAMLIT ç›¸å…³çš„ç¯å¢ƒå˜é‡ï¼Œå› ä¸ºå®ƒä»¬å·²åœ¨ config.toml ä¸­è®¾ç½®)

# æŠ‘åˆ¶ TensorFlow å’Œ CUDA è­¦å‘Š/å†²çª
os.environ.setdefault('TF_CPP_MIN_LOG_LEVEL', '2') # 1 = INFO, 2 = WARNING, 3 = ERROR
os.environ.setdefault('TF_FORCE_GPU_ALLOW_GROWTH', 'true')
# å°è¯•è§£å†³é‡å¤æ³¨å†Œé—®é¢˜
os.environ.setdefault('TF_CUDNN_DETERMINISTIC', '1')
os.environ.setdefault('TF_ENABLE_ONEDNN_OPTS', '0') # ç¦ç”¨ oneDNN ä¼˜åŒ–ï¼Œæœ‰æ—¶èƒ½é¿å…å†²çª

import sys
import math
import numpy as np
import pandas as pd
import streamlit as st
from rdkit import Chem
from rdkit.Chem import Descriptors, AllChem, rdMolDescriptors
from rdkit.Chem.Scaffolds import MurckoScaffold
from rdkit import DataStructs
from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator, GetRDKitFPGenerator
import matplotlib.pyplot as plt
import seaborn as sns
import time
from sklearn.cluster import KMeans, DBSCAN
from sklearn.manifold import TSNE
import psutil
import torch
import cupy as cp
from cuml.manifold import TSNE as cuTSNE
from cuml.cluster import KMeans as cuKMeans, DBSCAN as cuDBSCAN
import umap
import scipy.stats as stats
from sklearn.neighbors import KernelDensity
from scipy.stats import gaussian_kde
import openmm as mm
from sklearn.decomposition import PCA

# è®¾ç½®inotifyé™åˆ¶

os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(script_dir)

# è®¾ç½®é¡µé¢
st.set_page_config(
    page_title="åˆ†å­åº“ä»£è¡¨æ€§å­é›†é€‰æ‹©ç³»ç»Ÿ - ç»“æ„å¤šæ ·æ€§è¯„ä¼°",
    page_icon="ğŸ§ª",
    layout="wide"
)

def initialize_cuda():
    """åˆå§‹åŒ–CUDAè®¾å¤‡å¹¶è¿”å›è®¾å¤‡ä¿¡æ¯ (ç®€åŒ–ç‰ˆ)"""
    try:
        cuda_available = torch.cuda.is_available()
        device = torch.device("cuda" if cuda_available else "cpu")
        
        if cuda_available:
            # æ¸…ç†CUDAç¼“å­˜
            torch.cuda.empty_cache()
            
            # è·å–GPUä¿¡æ¯
            gpu_name = torch.cuda.get_device_name(0)
            gpu_mem_total = torch.cuda.get_device_properties(0).total_memory / 1024**2
            gpu_mem_alloc = torch.cuda.memory_allocated(0) / 1024**2
            gpu_mem_cached = torch.cuda.memory_reserved(0) / 1024**2
            
            st.sidebar.success("âœ… CUDAå¯ç”¨ï¼Œå°†ä½¿ç”¨GPUåŠ é€Ÿ")
            st.sidebar.info(
                f"GPUä¿¡æ¯:\n"
                f"- è®¾å¤‡: {gpu_name}\n"
                f"- æ€»æ˜¾å­˜: {gpu_mem_total:.1f}MB\n"
                f"- å·²åˆ†é…: {gpu_mem_alloc:.1f}MB\n"
                f"- å·²ç¼“å­˜: {gpu_mem_cached:.1f}MB"
            )
        else:
            st.sidebar.info("â„¹ï¸ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®¡ç®—")
        
        return cuda_available, device
    except Exception as e:
        st.sidebar.error(f"GPUåˆå§‹åŒ–é”™è¯¯: {str(e)}")
        return False, torch.device("cpu")

# å·¥å…·å‡½æ•°
def load_molecules_from_csv(file, smiles_col="SMILES"):
    """ä»CSVæ–‡ä»¶åŠ è½½åˆ†å­"""
    try:
        df = pd.read_csv(file)
        if smiles_col not in df.columns:
            st.error(f"æœªæ‰¾åˆ°SMILESåˆ— '{smiles_col}'")
            return None, None
        
        mols = []
        valid_indices = []
        for i, row in df.iterrows():
            smi = str(row[smiles_col]).strip()
            mol = Chem.MolFromSmiles(smi)
            if mol:
                mols.append(mol)
                valid_indices.append(i)
        
        return mols, df.iloc[valid_indices]
    except Exception as e:
        st.error(f"è¯»å–CSVæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return None, None

def compute_fingerprint(mol, fp_type="morgan", radius=2, nBits=2048, use_features=False, **kwargs):
    """è®¡ç®—åˆ†å­æŒ‡çº¹
    
    Args:
        mol: RDKitåˆ†å­å¯¹è±¡
        fp_type: æŒ‡çº¹ç±»å‹ï¼Œå¯é€‰:
            - "morgan": Morgan/ECFPæŒ‡çº¹
            - "fcfp": Morganç‰¹å¾æŒ‡çº¹
            - "maccs": MACCSç»“æ„é”®
            - "topological": æ‹“æ‰‘æŒ‡çº¹
            - "atom_pairs": åŸå­å¯¹æŒ‡çº¹
            - "torsion": æ‰­è½¬æŒ‡çº¹
            - "layered": åˆ†å±‚æŒ‡çº¹
        radius: MorganæŒ‡çº¹çš„åŠå¾„
        nBits: æŒ‡çº¹ä½æ•°
        use_features: æ˜¯å¦ä½¿ç”¨åŸå­ç‰¹å¾ï¼ˆç”¨äºFCFPï¼‰
        **kwargs: å…¶ä»–å‚æ•°
    """
    if fp_type == "morgan":
        # ä½¿ç”¨æ–°çš„MorganGenerator API
        morgan_gen = GetMorganGenerator(radius=radius, fpSize=nBits, useFeatures=use_features)
        return morgan_gen.GetFingerprint(mol)
    elif fp_type == "fcfp":
        # ä½¿ç”¨æ–°çš„MorganGenerator APIï¼Œè®¾ç½®useFeatures=True
        morgan_gen = GetMorganGenerator(radius=radius, fpSize=nBits, useFeatures=True)
        return morgan_gen.GetFingerprint(mol)
    elif fp_type == "maccs":
        return AllChem.GetMACCSKeysFingerprint(mol)
    elif fp_type == "topological":
        # ä½¿ç”¨æ–°çš„RDKitFPGenerator API
        rdk_gen = GetRDKitFPGenerator(fpSize=nBits, **kwargs)
        return rdk_gen.GetFingerprint(mol)
    elif fp_type == "atom_pairs":
        return AllChem.GetHashedAtomPairFingerprintAsBitVect(mol, nBits=nBits)
    elif fp_type == "torsion":
        return AllChem.GetHashedTopologicalTorsionFingerprintAsBitVect(mol, nBits=nBits)
    elif fp_type == "layered":
        return Chem.LayeredFingerprint(mol, layerFlags=kwargs.get('layerFlags', 0xFFFFFFFF))
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æŒ‡çº¹ç±»å‹: {fp_type}")

@st.cache_data
def compute_fingerprints_batch(smiles_list, fp_type="morgan", radius=2, nBits=2048, use_features=False, **kwargs):
    """æ‰¹é‡è®¡ç®—åˆ†å­æŒ‡çº¹"""
    import torch
    
    # åˆ›å»ºè¿›åº¦æ¡
    progress_bar = st.progress(0)
    status_text = st.empty()
    debug_info = st.empty()
    
    # åˆå§‹åŒ–CUDA
    cuda_available, device = initialize_cuda()
    start_time = time.time()
    
    # åˆ›å»ºæŒ‡çº¹ç”Ÿæˆå™¨
    if fp_type == "morgan":
        fp_gen = GetMorganGenerator(radius=radius, fpSize=nBits, useFeatures=use_features)
    elif fp_type == "fcfp":
        fp_gen = GetMorganGenerator(radius=radius, fpSize=nBits, useFeatures=True)
    elif fp_type == "topological":
        fp_gen = GetRDKitFPGenerator(fpSize=nBits, **kwargs)
    
    fps = []
    total_mols = len(smiles_list)
    
    # è®¡ç®—æŒ‡çº¹
    for i, smi in enumerate(smiles_list):
        try:
            mol = Chem.MolFromSmiles(smi)
            if mol is None:
                fps.append(np.zeros(nBits))
            else:
                if fp_type in ["morgan", "fcfp"]:
                    fp = np.array(fp_gen.GetFingerprint(mol).ToList())
                elif fp_type == "maccs":
                    fp = np.array(AllChem.GetMACCSKeysFingerprint(mol).ToList())
                elif fp_type == "topological":
                    fp = np.array(fp_gen.GetFingerprint(mol).ToList())
                elif fp_type == "atom_pairs":
                    fp = np.array(AllChem.GetHashedAtomPairFingerprintAsBitVect(mol, nBits=nBits).ToList())
                elif fp_type == "torsion":
                    fp = np.array(AllChem.GetHashedTopologicalTorsionFingerprintAsBitVect(mol, nBits=nBits).ToList())
                elif fp_type == "layered":
                    fp = np.array(Chem.LayeredFingerprint(mol, layerFlags=kwargs.get('layerFlags', 0xFFFFFFFF)).ToList())
                fps.append(fp)
        except Exception as e:
            st.warning(f"å¤„ç†åˆ†å­ {smi} æ—¶å‡ºé”™: {str(e)}")
            fps.append(np.zeros(nBits))
        
        # æ›´æ–°è¿›åº¦
        progress = (i + 1) / total_mols
        progress_bar.progress(progress)
        if (i + 1) % 100 == 0:
            status_text.text(f"å·²å¤„ç†: {i + 1}/{total_mols} ä¸ªåˆ†å­")
    
    fps_array = np.array(fps)
    
    # å¦‚æœæœ‰GPUï¼Œå°†æ•°æ®è½¬ç§»åˆ°GPU
    if cuda_available:
        fps_tensor = torch.tensor(fps_array, dtype=torch.float32).to(device)
        debug_info.info(f"æ•°æ®å·²è½¬ç§»åˆ°GPUï¼Œæ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated()/1024**2:.1f}MB")
    
    # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
    total_time = time.time() - start_time
    avg_speed = total_mols / total_time
    debug_info.success(
        f"âœ… æŒ‡çº¹è®¡ç®—å®Œæˆ:\n"
        f"- æ€»è®¡ç®—æ—¶é—´: {total_time:.1f} ç§’\n"
        f"- å¹³å‡é€Ÿåº¦: {avg_speed:.1f} åˆ†å­/ç§’\n"
        f"- è®¡ç®—è®¾å¤‡: {'GPU' if cuda_available else 'CPU'}"
    )
    
    # æ¸…é™¤è¿›åº¦æ˜¾ç¤º
    progress_bar.empty()
    status_text.empty()
    
    return fps_array

def compute_similarity_matrix(fps_list, progress_text="è®¡ç®—ç›¸ä¼¼æ€§çŸ©é˜µ"):
    """ä½¿ç”¨GPUåŠ é€Ÿè®¡ç®—ç›¸ä¼¼æ€§çŸ©é˜µ"""
    import torch
    debug_info = st.empty()
    start_time = time.time()
    
    # åˆå§‹åŒ–CUDA
    cuda_available, device = initialize_cuda()
    
    # æ˜¾ç¤ºè®¡ç®—è®¾å¤‡ä¿¡æ¯
    device_info = "GPU (CUDA)" if cuda_available else "CPU"
    debug_info.info(f"ğŸ’» ä½¿ç”¨ {device_info} {progress_text}")
    
    # å°†æŒ‡çº¹è½¬æ¢ä¸ºäºŒè¿›åˆ¶æ•°ç»„
    n_fps = len(fps_list)
    if n_fps == 0:
        return np.array([])
    
    # è·å–æŒ‡çº¹é•¿åº¦
    fp_length = len(fps_list[0].ToBitString())
    
    # åˆ›å»ºäºŒè¿›åˆ¶çŸ©é˜µ
    fp_array = np.zeros((n_fps, fp_length), dtype=np.float32)
    for i, fp in enumerate(fps_list):
        fp_array[i] = np.array(list(fp.ToBitString())).astype(np.float32)
    
    if cuda_available:
        # è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶ç§»è‡³GPU
        fp_tensor = torch.from_numpy(fp_array).to(device)
        
        # è®¡ç®—ç‚¹ç§¯
        dot_product = torch.mm(fp_tensor, fp_tensor.t())
        
        # è®¡ç®—æ¯ä¸ªæŒ‡çº¹çš„1çš„æ•°é‡
        fp_sums = torch.sum(fp_tensor, dim=1, keepdim=True)
        
        # è®¡ç®—å¹¶é›†
        union = fp_sums + fp_sums.t() - dot_product
        
        # è®¡ç®—Tanimotoç›¸ä¼¼åº¦
        similarity_matrix = (dot_product / union).cpu().numpy()
        
        # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        gpu_mem_alloc = torch.cuda.memory_allocated(device) / 1024**2
        gpu_mem_cached = torch.cuda.memory_reserved(device) / 1024**2
        debug_info.info(
            f"GPUå†…å­˜ä½¿ç”¨æƒ…å†µ:\n"
            f"- å·²åˆ†é…: {gpu_mem_alloc:.1f} MB\n"
            f"- ç¼“å­˜: {gpu_mem_cached:.1f} MB"
        )
    else:
        # CPUè®¡ç®—
        similarity_matrix = np.zeros((n_fps, n_fps))
        for i in range(n_fps):
            for j in range(i, n_fps):
                sim = DataStructs.TanimotoSimilarity(fps_list[i], fps_list[j])
                similarity_matrix[i,j] = sim
                similarity_matrix[j,i] = sim
    
    # è®¡ç®—æ€§èƒ½ç»Ÿè®¡
    total_time = time.time() - start_time
    comparisons = (n_fps * (n_fps - 1)) / 2
    speed = comparisons / total_time if total_time > 0 else 0
    
    debug_info.success(
        f"âœ… ç›¸ä¼¼æ€§çŸ©é˜µè®¡ç®—å®Œæˆ:\n"
        f"- çŸ©é˜µå¤§å°: {n_fps}x{n_fps}\n"
        f"- æ¯”è¾ƒæ¬¡æ•°: {comparisons:,.0f}\n"
        f"- è®¡ç®—æ—¶é—´: {total_time:.1f} ç§’\n"
        f"- è®¡ç®—é€Ÿåº¦: {speed:,.0f} æ¯”è¾ƒ/ç§’\n"
        f"- è®¡ç®—è®¾å¤‡: {device_info}"
    )
    
    return similarity_matrix

@st.cache_data(show_spinner=False)
def process_molecules_parallel(smiles_list, radius=2, nBits=2048):
    """å¤„ç†åˆ†å­å¹¶è®¡ç®—ç›¸ä¼¼æ€§çŸ©é˜µ"""
    # åˆå§‹åŒ–CUDA
    cuda_available, _ = initialize_cuda()
    
    st.info(
        f"ğŸš€ å¼€å§‹å¤„ç†:\n"
        f"- åˆ†å­æ•°é‡: {len(smiles_list)}\n"
        f"- æŒ‡çº¹åŠå¾„: {radius}\n"
        f"- æŒ‡çº¹ä½æ•°: {nBits}\n"
        f"- GPUåŠ é€Ÿ: {'å¯ç”¨' if cuda_available else 'ä¸å¯ç”¨'}"
    )
    
    # è®¡ç®—åˆ†å­æŒ‡çº¹
    fps = compute_fingerprints_batch(smiles_list, radius=radius, nBits=nBits)
    
    # è®¡ç®—ç›¸ä¼¼æ€§çŸ©é˜µ
    sim_matrix = compute_similarity_matrix(fps)
    
    return fps, sim_matrix

def calculate_diversity_metrics(sim_matrix):
    """è®¡ç®—å¤šæ ·æ€§æŒ‡æ ‡"""
    # è·å–ä¸Šä¸‰è§’çŸ©é˜µçš„å€¼ï¼ˆä¸åŒ…æ‹¬å¯¹è§’çº¿ï¼‰
    triu_indices = np.triu_indices_from(sim_matrix, k=1)
    similarities = sim_matrix[triu_indices]
    
    metrics = {
        "å¹³å‡ç›¸ä¼¼æ€§": np.mean(similarities),
        "ç›¸ä¼¼æ€§æ ‡å‡†å·®": np.std(similarities),
        "æœ€å¤§ç›¸ä¼¼æ€§": np.max(similarities),
        "æœ€å°ç›¸ä¼¼æ€§": np.min(similarities),
        "ä¸­ä½æ•°ç›¸ä¼¼æ€§": np.median(similarities)
    }
    
    return metrics

def get_bemis_murcko_scaffold(mol):
    """è·å–Bemis-Murckoéª¨æ¶"""
    core = MurckoScaffold.GetScaffoldForMol(mol)
    if core:
        return Chem.MolToSmiles(core)
    return None

def calc_scaffold_stats(mols):
    """è®¡ç®—éª¨æ¶ç»Ÿè®¡ä¿¡æ¯"""
    scaff_dict = {}
    for mol in mols:
        scaf_smi = get_bemis_murcko_scaffold(mol)
        if scaf_smi:
            scaff_dict[scaf_smi] = scaff_dict.get(scaf_smi, 0) + 1
    
    if not scaff_dict:
        return 0, 0.0, 0.0, pd.DataFrame([])
    
    items = sorted(scaff_dict.items(), key=lambda x: x[1], reverse=True)
    scaffolds_freq_df = pd.DataFrame(items, columns=["Scaffold_SMILES", "Count"])
    n_scaffolds = len(items)
    n_mols = sum([count for _, count in items])
    
    scaffold_entropy = 0.0
    for _, c in items:
        p = c / n_mols
        scaffold_entropy -= p * math.log2(p)
    
    singletons = sum([1 for _, c in items if c == 1])
    fraction_singletons = singletons / n_scaffolds
    
    return n_scaffolds, scaffold_entropy, fraction_singletons, scaffolds_freq_df

def calc_F50(scaffolds_freq_df):
    """è®¡ç®—F50å€¼"""
    if scaffolds_freq_df.empty:
        return 0.0
    total_mols = scaffolds_freq_df["Count"].sum()
    half_mols = total_mols * 0.5
    
    cover = 0
    used_scafs = 0
    for i, row in scaffolds_freq_df.iterrows():
        cover += row["Count"]
        used_scafs += 1
        if cover >= half_mols:
            break
    
    total_scaf = len(scaffolds_freq_df)
    f50 = used_scafs / total_scaf
    return f50

def plot_property_comparison(propsA, propsB, prop_name, prop_label):
    """Plot property distribution comparison"""
    fig, ax = plt.subplots(figsize=(8, 5))
    sns.kdeplot(data=propsA, x=prop_name, label="Library A", ax=ax, color='#2ecc71')
    sns.kdeplot(data=propsB, x=prop_name, label="Library B", ax=ax, color='#e74c3c')
    ax.set_title(f"Distribution of {prop_label}")
    ax.set_xlabel(prop_label)
    ax.set_ylabel("Density")
    ax.legend()
    plt.tight_layout()
    return fig

def calc_property_stats(mols):
    """Calculate molecular properties"""
    records = []
    for mol in mols:
        mw = Descriptors.MolWt(mol)
        logp = Descriptors.MolLogP(mol)
        tpsa = Descriptors.TPSA(mol)
        hbd = rdMolDescriptors.CalcNumHBD(mol)
        hba = rdMolDescriptors.CalcNumHBA(mol)
        rotatable = rdMolDescriptors.CalcNumRotatableBonds(mol)
        records.append((mw, logp, tpsa, hbd, hba, rotatable))
    
    columns = {
        "MW": "Molecular Weight",
        "LogP": "LogP",
        "TPSA": "TPSA",
        "HBD": "H-Bond Donors",
        "HBA": "H-Bond Acceptors",
        "RotBonds": "Rotatable Bonds"
    }
    
    df = pd.DataFrame(records, columns=list(columns.keys()))
    df.columns.name = "Property"
    return df, columns

def calculate_shannon_entropy(fp_array):
    """è®¡ç®—æŒ‡çº¹æ¯”ç‰¹çš„é¦™å†œç†µ"""
    # è®¡ç®—æ¯ä¸ªæ¯”ç‰¹ä½çš„å‡ºç°é¢‘ç‡
    bit_frequencies = np.mean(fp_array, axis=0)
    # è®¡ç®—é¦™å†œç†µ
    entropy = 0
    for freq in bit_frequencies:
        if freq > 0 and freq < 1:  # é¿å…log(0)
            entropy -= freq * np.log2(freq) + (1-freq) * np.log2(1-freq)
    return entropy

def calculate_mean_nearest_neighbor(sim_matrix):
    """è®¡ç®—å¹³å‡æœ€è¿‘é‚»Tanimotoç›¸ä¼¼åº¦"""
    # å¯¹æ¯ä¸ªåˆ†å­ï¼Œæ‰¾åˆ°ä¸å…¶æœ€ç›¸ä¼¼çš„å…¶ä»–åˆ†å­ï¼ˆä¸åŒ…æ‹¬è‡ªèº«ï¼‰
    n = sim_matrix.shape[0]
    nearest_neighbors = []
    for i in range(n):
        # åˆ›å»ºæ©ç æ’é™¤è‡ªèº«
        mask = np.ones(n, dtype=bool)
        mask[i] = False
        # æ‰¾åˆ°æœ€å¤§ç›¸ä¼¼åº¦
        max_sim = np.max(sim_matrix[i][mask])
        nearest_neighbors.append(max_sim)
    return np.mean(nearest_neighbors)

def plot_similarity_distribution(sim_matrix):
    """ç»˜åˆ¶ç›¸ä¼¼åº¦åˆ†å¸ƒç›´æ–¹å›¾"""
    # è·å–ä¸Šä¸‰è§’çŸ©é˜µçš„å€¼ï¼ˆä¸åŒ…æ‹¬å¯¹è§’çº¿ï¼‰
    triu_indices = np.triu_indices_from(sim_matrix, k=1)
    similarities = sim_matrix[triu_indices]
    
    fig, ax = plt.subplots(figsize=(8, 5))
    sns.histplot(data=similarities, bins=50, kde=True, ax=ax)
    ax.set_title("Distribution of Pairwise Tanimoto Similarities")
    ax.set_xlabel("Tanimoto Similarity")
    ax.set_ylabel("Count")
    plt.tight_layout()
    return fig

def plot_similarity_threshold_stats(sim_matrix):
    """ç»˜åˆ¶ç›¸ä¼¼åº¦é˜ˆå€¼ç»Ÿè®¡å›¾"""
    triu_indices = np.triu_indices_from(sim_matrix, k=1)
    similarities = sim_matrix[triu_indices]
    
    thresholds = np.arange(0, 1.1, 0.1)
    percentages = [np.mean(similarities >= t) * 100 for t in thresholds]
    
    fig, ax = plt.subplots(figsize=(8, 5))
    ax.plot(thresholds, percentages, marker='o')
    ax.set_title("Percentage of Pairs Above Similarity Threshold")
    ax.set_xlabel("Similarity Threshold")
    ax.set_ylabel("Percentage of Pairs (%)")
    ax.grid(True)
    plt.tight_layout()
    return fig

def calc_pairwise_similarity_stats(sim_matrix):
    """è®¡ç®—ä¸¤ä¸¤åˆ†å­é—´ç›¸ä¼¼åº¦çš„ç»Ÿè®¡ä¿¡æ¯"""
    # è·å–ä¸Šä¸‰è§’çŸ©é˜µçš„å€¼ï¼ˆä¸åŒ…æ‹¬å¯¹è§’çº¿ï¼‰
    triu_indices = np.triu_indices_from(sim_matrix, k=1)
    similarities = sim_matrix[triu_indices]
    
    stats = {
        "mean": np.mean(similarities),
        "std": np.std(similarities),
        "median": np.median(similarities),
        "min": np.min(similarities),
        "max": np.max(similarities),
        "q1": np.percentile(similarities, 25),
        "q3": np.percentile(similarities, 75)
    }
    return stats

def calc_similarity_threshold_stats(sim_matrix, thresholds=None):
    """è®¡ç®—ä¸åŒç›¸ä¼¼åº¦é˜ˆå€¼ä¸‹çš„åˆ†å­å¯¹æ¯”ä¾‹"""
    if thresholds is None:
        thresholds = np.arange(0, 1.1, 0.1)
    
    triu_indices = np.triu_indices_from(sim_matrix, k=1)
    similarities = sim_matrix[triu_indices]
    
    stats = {}
    for t in thresholds:
        fraction = np.mean(similarities >= t)
        stats[t] = fraction
    return stats

def plot_bit_frequency(fps_list, title="Fingerprint Bit Frequency"):
    """Plot fingerprint bit frequency distribution"""
    bit_freqs = np.mean([np.array(list(fp.ToBitString())) == '1' for fp in fps_list], axis=0)
    
    fig, ax = plt.subplots(figsize=(10, 4))
    ax.plot(range(len(bit_freqs)), bit_freqs, 'b-', alpha=0.5)
    ax.set_title(title)
    ax.set_xlabel("Bit Position")
    ax.set_ylabel("Frequency")
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    return fig

def plot_similarity_heatmap(sim_matrix, max_mols=100):
    """ç»˜åˆ¶ç›¸ä¼¼åº¦çŸ©é˜µçƒ­å›¾"""
    # å¦‚æœåˆ†å­æ•°é‡å¤ªå¤šï¼Œéšæœºé‡‡æ ·
    n_mols = len(sim_matrix)
    if n_mols > max_mols:
        indices = np.random.choice(n_mols, max_mols, replace=False)
        sim_matrix = sim_matrix[indices][:, indices]
    
    fig, ax = plt.subplots(figsize=(8, 8))
    sns.heatmap(sim_matrix, cmap='viridis', ax=ax)
    ax.set_title("Similarity Matrix Heatmap")
    plt.tight_layout()
    return fig

def plot_nearest_neighbor_distribution(sim_matrix):
    """ç»˜åˆ¶æœ€è¿‘é‚»ç›¸ä¼¼åº¦åˆ†å¸ƒ"""
    n = sim_matrix.shape[0]
    nn_sims = []
    for i in range(n):
        mask = np.ones(n, dtype=bool)
        mask[i] = False
        nn_sims.append(np.max(sim_matrix[i][mask]))
    
    fig, ax = plt.subplots(figsize=(8, 5))
    sns.histplot(data=nn_sims, bins=50, kde=True, ax=ax)
    ax.set_title("Nearest Neighbor Similarity Distribution")
    ax.set_xlabel("Nearest Neighbor Similarity")
    ax.set_ylabel("Count")
    plt.tight_layout()
    return fig

def perform_clustering_analysis(sim_matrix, n_clusters=5, eps=0.3, min_samples=5, perplexity=30.0):
    """è¿›è¡Œèšç±»åˆ†æ"""
    # ç§»é™¤äº†å¯¹ cuml.manifold.TSNE, cuml.cluster.KMeans, cuml.cluster.DBSCAN çš„å±€éƒ¨å¯¼å…¥
    # å°†ä½¿ç”¨å…¨å±€å¯¼å…¥çš„åˆ«å (cuTSNE, cuKMeans, cuDBSCAN) æˆ–å…¨å±€çš„ sklearn ç‰ˆæœ¬

    cuda_available, device = initialize_cuda()

    if not cuda_available:
        st.warning("âš ï¸ GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè¿›è¡Œè®¡ç®—")
        dist_matrix = 1 - sim_matrix  # Numpy distance matrix

        # ä½¿ç”¨ sklearn.manifold.TSNE
        # TSNE æ˜¯åœ¨æ–‡ä»¶é¡¶éƒ¨ä» sklearn.manifold å¯¼å…¥çš„
        tsne_cpu = TSNE(n_components=2, metric='precomputed', random_state=42, init='random', learning_rate='auto')
        coords = tsne_cpu.fit_transform(dist_matrix)

        # ä½¿ç”¨ sklearn.cluster.KMeans
        # KMeans æ˜¯åœ¨æ–‡ä»¶é¡¶éƒ¨ä» sklearn.cluster å¯¼å…¥çš„
        # å¯¹äº sklearn KMeans, n_init='auto' æ˜¯æœ‰æ•ˆçš„ (é»˜è®¤ä¸º10æ¬¡è¿è¡Œ)
        kmeans_cpu = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto', algorithm='lloyd')
        clusters = kmeans_cpu.fit_predict(coords)  # K-means é€šå¸¸åœ¨ t-SNE é™ç»´åçš„åæ ‡ä¸Šè¿è¡Œ

        # ä½¿ç”¨ sklearn.cluster.DBSCAN
        # DBSCAN æ˜¯åœ¨æ–‡ä»¶é¡¶éƒ¨ä» sklearn.cluster å¯¼å…¥çš„
        dbscan_cpu = DBSCAN(metric='precomputed', eps=eps, min_samples=min_samples)
        dbscan_clusters = dbscan_cpu.fit_predict(dist_matrix) # DBSCAN é€šå¸¸åœ¨åŸå§‹è·ç¦»çŸ©é˜µä¸Šè¿è¡Œ

        return {
            'coords': coords,
            'kmeans_clusters': clusters,
            'dbscan_clusters': dbscan_clusters
        }
    else: # GPU è·¯å¾„
        # å¯¼å…¥ torch å’Œ cupy (å¦‚æœå®ƒä»¬åªåœ¨æ­¤å—çš„GPUç‰¹å®šé€»è¾‘ä¸­ä½¿ç”¨ï¼Œåˆ™ä¿æŒå±€éƒ¨å¯¼å…¥æ˜¯å¯è¡Œçš„)
        import torch 
        import cupy as cp

        dist_matrix = 1 - sim_matrix # åŸå§‹ sim_matrix æ˜¯ numpy æ•°ç»„
        dist_matrix_gpu = cp.asarray(dist_matrix) # cupy è·ç¦»çŸ©é˜µ

        st.info("ğŸš€ ä½¿ç”¨GPUåŠ é€Ÿçš„t-SNEè¿›è¡Œé™ç»´...")
        # å°è¯•ä½¿ç”¨ cuML TSNEï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ° CPU ç‰ˆæœ¬
        try:
            # cuML TSNE å¯èƒ½ä¸æ”¯æŒ metric='precomputed'ï¼Œå…ˆå°è¯•ä¸ä½¿ç”¨å®ƒ
            tsne_gpu = cuTSNE(n_components=2, perplexity=perplexity, random_state=42)
            # ç”±äº cuML TSNE å¯èƒ½ä¸æ¥å—è·ç¦»çŸ©é˜µï¼Œæˆ‘ä»¬å°è¯•ç›´æ¥ä½¿ç”¨ç›¸ä¼¼æ€§çŸ©é˜µ
            coords_gpu = tsne_gpu.fit_transform(cp.asarray(sim_matrix))
            coords = cp.asnumpy(coords_gpu)
            st.success("âœ… æˆåŠŸä½¿ç”¨ cuML TSNE")
        except Exception as e:
            st.warning(f"âš ï¸ cuML TSNE å¤±è´¥ ({str(e)})ï¼Œå›é€€åˆ° CPU ç‰ˆæœ¬")
            # å›é€€åˆ° CPU ç‰ˆæœ¬çš„ TSNE
            tsne_cpu = TSNE(n_components=2, metric='precomputed', perplexity=perplexity, random_state=42, init='random', learning_rate='auto')
            coords = tsne_cpu.fit_transform(dist_matrix)

        st.info("ğŸš€ ä½¿ç”¨GPUåŠ é€Ÿçš„K-meansè¿›è¡Œèšç±»...")
        # ä½¿ç”¨å…¨å±€åˆ«å cuKMeans (cuml.cluster.KMeans)
        # ä¿®å¤: n_init å¿…é¡»æ˜¯æ•´æ•°, ä¾‹å¦‚ 10
        kmeans_gpu = cuKMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        clusters_gpu = kmeans_gpu.fit_predict(cp.asarray(coords)) # K-means åœ¨ t-SNE é™ç»´åçš„åæ ‡ä¸Šè¿è¡Œ
        clusters = cp.asnumpy(clusters_gpu)

        st.info("ğŸš€ ä½¿ç”¨GPUåŠ é€Ÿçš„DBSCANè¿›è¡Œèšç±»...")
        # ä½¿ç”¨å…¨å±€åˆ«å cuDBSCAN (cuml.cluster.DBSCAN)
        dbscan_gpu = cuDBSCAN(metric='precomputed', eps=eps, min_samples=min_samples)
        dbscan_clusters_gpu = dbscan_gpu.fit_predict(dist_matrix_gpu) # DBSCAN åœ¨åŸå§‹è·ç¦»çŸ©é˜µä¸Šè¿è¡Œ
        dbscan_clusters = cp.asnumpy(dbscan_clusters_gpu)

        # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        gpu_mem_alloc = torch.cuda.memory_allocated(device) / 1024**2
        gpu_mem_cached = torch.cuda.memory_reserved(device) / 1024**2
        st.success(
            f"âœ… GPUåŠ é€Ÿèšç±»åˆ†æå®Œæˆ:\\n"
            f"- GPUå†…å­˜ä½¿ç”¨: {gpu_mem_alloc:.1f}MB\\n"
            f"- GPUç¼“å­˜: {gpu_mem_cached:.1f}MB"
        )

        return {
            'coords': coords,
            'kmeans_clusters': clusters,
            'dbscan_clusters': dbscan_clusters
        }

def plot_clustering_results(clustering_results, title="Clustering Results"):
    """Plot clustering results"""
    coords = clustering_results['coords']
    kmeans_clusters = clustering_results['kmeans_clusters']
    dbscan_clusters = clustering_results['dbscan_clusters']
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # K-means results
    scatter1 = ax1.scatter(coords[:, 0], coords[:, 1], c=kmeans_clusters, cmap='tab10')
    ax1.set_title("K-means Clustering")
    ax1.set_xlabel("t-SNE Dimension 1")
    ax1.set_ylabel("t-SNE Dimension 2")
    plt.colorbar(scatter1, ax=ax1, label='Cluster ID')
    
    # DBSCAN results
    scatter2 = ax2.scatter(coords[:, 0], coords[:, 1], c=dbscan_clusters, cmap='tab10')
    ax2.set_title("DBSCAN Clustering")
    ax2.set_xlabel("t-SNE Dimension 1")
    ax2.set_ylabel("t-SNE Dimension 2")
    plt.colorbar(scatter2, ax=ax2, label='Cluster ID')
    
    plt.suptitle(title, y=1.02)
    plt.tight_layout()
    return fig

def monitor_memory_usage():
    """ç›‘æ§å†…å­˜ä½¿ç”¨"""
    process = psutil.Process()
    memory_info = process.memory_info()
    return {
        'rss': memory_info.rss / 1024**2,  # RSS in MB
        'vms': memory_info.vms / 1024**2,  # VMS in MB
        'percent': process.memory_percent()
    }

def calculate_distribution_metrics(coords_A, coords_B):
    """è®¡ç®—ä¸¤ç»„ç‚¹çš„åˆ†å¸ƒå·®å¼‚æŒ‡æ ‡"""
    from sklearn.neighbors import KernelDensity
    
    debug_info = st.empty()
    timing_info = st.empty()
    start_total = time.time()
    
    # æ£€æŸ¥è¾“å…¥æ•°ç»„æ˜¯å¦ä¸ºç©ºæˆ–å¤ªå°
    if len(coords_A) < 2 or len(coords_B) < 2:
        debug_info.warning("âš ï¸ è­¦å‘Šï¼šä¸€ä¸ªæˆ–ä¸¤ä¸ªæ•°æ®é›†æ ·æœ¬æ•°é‡ä¸è¶³ï¼ˆéœ€è¦è‡³å°‘2ä¸ªæ ·æœ¬ï¼‰")
        return {
            "ä¸­å¿ƒç‚¹è·ç¦»": np.nan,
            "Aç»„ç¦»æ•£åº¦": np.nan if len(coords_A) < 2 else 0,
            "Bç»„ç¦»æ•£åº¦": np.nan if len(coords_B) < 2 else 0,
            "Aç»„å¹³å‡è·ç¦»": np.nan if len(coords_A) < 2 else 0,
            "Bç»„å¹³å‡è·ç¦»": np.nan if len(coords_B) < 2 else 0,
            "Xè½´KSæ£€éªŒç»Ÿè®¡é‡": np.nan,
            "Xè½´KSæ£€éªŒpå€¼": np.nan,
            "Yè½´KSæ£€éªŒç»Ÿè®¡é‡": np.nan,
            "Yè½´KSæ£€éªŒpå€¼": np.nan,
            "åˆ†å¸ƒé‡å åº¦": 0.0
        }
    
    try:
        debug_info.info("ğŸ”„ å¼€å§‹è®¡ç®—åˆ†å¸ƒæŒ‡æ ‡...")
        
        # è®¡ç®—ä¸­å¿ƒç‚¹
        debug_info.info("1ï¸âƒ£ è®¡ç®—ä¸­å¿ƒç‚¹å’Œç¦»æ•£åº¦...")
        start_time = time.time()
        center_A = np.mean(coords_A, axis=0)
        center_B = np.mean(coords_B, axis=0)
        
        # è®¡ç®—ä¸­å¿ƒç‚¹ä¹‹é—´çš„æ¬§æ°è·ç¦»
        center_distance = np.linalg.norm(center_A - center_B)
        
        # è®¡ç®—æ¯ç»„ç‚¹çš„ç¦»æ•£åº¦ï¼ˆæ–¹å·®ï¼‰
        dispersion_A = np.mean(np.linalg.norm(coords_A - center_A, axis=1))
        dispersion_B = np.mean(np.linalg.norm(coords_B - center_B, axis=1))
        timing_info.info(f"âœ“ ä¸­å¿ƒç‚¹å’Œç¦»æ•£åº¦è®¡ç®—å®Œæˆ ({time.time() - start_time:.2f}ç§’)")
        
        # è®¡ç®—æ¯ç»„çš„å¹³å‡è·ç¦»ï¼ˆä½¿ç”¨GPUåŠ é€Ÿï¼‰
        debug_info.info("2ï¸âƒ£ è®¡ç®—ç»„å†…å¹³å‡è·ç¦»...")
        start_time = time.time()
        
        try:
            import cupy as cp
            debug_info.info("   â†ª ä½¿ç”¨GPUåŠ é€Ÿè·ç¦»è®¡ç®—...")
            
            # è®¡ç®—Aç»„å¹³å‡è·ç¦»
            coords_A_gpu = cp.asarray(coords_A)
            diff_A = coords_A_gpu[:, None, :] - coords_A_gpu[None, :, :]
            dist_matrix_A = cp.sqrt(cp.sum(diff_A ** 2, axis=2))
            # æ’é™¤è‡ªèº«è·ç¦»ï¼ˆå¯¹è§’çº¿ï¼‰
            mask_A = cp.ones_like(dist_matrix_A, dtype=bool)
            cp.fill_diagonal(mask_A, False)
            dist_matrix_A = dist_matrix_A[mask_A]
            dist_A = float(cp.mean(dist_matrix_A).get())
            
            # è®¡ç®—Bç»„å¹³å‡è·ç¦»
            coords_B_gpu = cp.asarray(coords_B)
            diff_B = coords_B_gpu[:, None, :] - coords_B_gpu[None, :, :]
            dist_matrix_B = cp.sqrt(cp.sum(diff_B ** 2, axis=2))
            # æ’é™¤è‡ªèº«è·ç¦»ï¼ˆå¯¹è§’çº¿ï¼‰
            mask_B = cp.ones_like(dist_matrix_B, dtype=bool)
            cp.fill_diagonal(mask_B, False)
            dist_matrix_B = dist_matrix_B[mask_B]
            dist_B = float(cp.mean(dist_matrix_B).get())
            
            # æ¸…ç†GPUå†…å­˜
            del coords_A_gpu, coords_B_gpu, diff_A, diff_B, dist_matrix_A, dist_matrix_B
            cp.get_default_memory_pool().free_all_blocks()
            
            debug_info.success("   âœ“ GPUåŠ é€Ÿè·ç¦»è®¡ç®—å®Œæˆ")
            
        except (ImportError, Exception) as e:
            debug_info.warning(f"   âš ï¸ GPUåŠ é€Ÿå¤±è´¥ ({str(e)})ï¼Œä½¿ç”¨CPUè®¡ç®—...")
            
            # ä½¿ç”¨numpyçš„çŸ¢é‡åŒ–æ“ä½œ
            # è®¡ç®—Aç»„å¹³å‡è·ç¦»
            diff_A = coords_A[:, None, :] - coords_A[None, :, :]
            dist_matrix_A = np.sqrt(np.sum(diff_A ** 2, axis=2))
            # æ’é™¤è‡ªèº«è·ç¦»ï¼ˆå¯¹è§’çº¿ï¼‰
            mask_A = np.ones_like(dist_matrix_A, dtype=bool)
            np.fill_diagonal(mask_A, False)
            dist_A = np.mean(dist_matrix_A[mask_A])
            
            # è®¡ç®—Bç»„å¹³å‡è·ç¦»
            diff_B = coords_B[:, None, :] - coords_B[None, :, :]
            dist_matrix_B = np.sqrt(np.sum(diff_B ** 2, axis=2))
            # æ’é™¤è‡ªèº«è·ç¦»ï¼ˆå¯¹è§’çº¿ï¼‰
            mask_B = np.ones_like(dist_matrix_B, dtype=bool)
            np.fill_diagonal(mask_B, False)
            dist_B = np.mean(dist_matrix_B[mask_B])
        
        timing_info.info(f"âœ“ å¹³å‡è·ç¦»è®¡ç®—å®Œæˆ ({time.time() - start_time:.2f}ç§’)")
        
        # è¿›è¡ŒåŒæ ·æœ¬KSæ£€éªŒ
        debug_info.info("3ï¸âƒ£ æ‰§è¡ŒKSæ£€éªŒ...")
        start_time = time.time()
        ks_statistic_x, p_value_x = stats.ks_2samp(coords_A[:, 0], coords_B[:, 0])
        ks_statistic_y, p_value_y = stats.ks_2samp(coords_A[:, 1], coords_B[:, 1])
        timing_info.info(f"âœ“ KSæ£€éªŒå®Œæˆ ({time.time() - start_time:.2f}ç§’)")
        
        # ä½¿ç”¨cuMLçš„KernelDensityè®¡ç®—åˆ†å¸ƒé‡å åº¦
        debug_info.info("4ï¸âƒ£ è®¡ç®—å¯†åº¦åˆ†å¸ƒ...")
        start_time = time.time()
        try:
            from cuml.neighbors import KernelDensity as cuKernelDensity
            import cupy as cp
            
            debug_info.info("ğŸš€ ä½¿ç”¨GPUåŠ é€Ÿå¯†åº¦ä¼°è®¡è®¡ç®—")
            
            # ç¡®ä¿æ•°æ®æ˜¯æµ®ç‚¹å‹
            coords_A = coords_A.astype(np.float64)
            coords_B = coords_B.astype(np.float64)
            
            # è½¬ç§»æ•°æ®åˆ°GPU
            debug_info.info("   â†ª è½¬ç§»æ•°æ®åˆ°GPU...")
            coords_A_gpu = cp.asarray(coords_A)
            coords_B_gpu = cp.asarray(coords_B)
            
            # ä½¿ç”¨GPUç‰ˆæœ¬çš„KDE
            debug_info.info("   â†ª è®­ç»ƒKDEæ¨¡å‹...")
            kde_A = cuKernelDensity(bandwidth=0.1).fit(coords_A_gpu)
            kde_B = cuKernelDensity(bandwidth=0.1).fit(coords_B_gpu)
            
            # ç”Ÿæˆç½‘æ ¼ç‚¹
            debug_info.info("   â†ª ç”Ÿæˆè¯„ä¼°ç½‘æ ¼...")
            x_min = min(coords_A[:, 0].min(), coords_B[:, 0].min())
            x_max = max(coords_A[:, 0].max(), coords_B[:, 0].max())
            y_min = min(coords_A[:, 1].min(), coords_B[:, 1].min())
            y_max = max(coords_A[:, 1].max(), coords_B[:, 1].max())
            
            # æ·»åŠ è¾¹è·
            margin_x = (x_max - x_min) * 0.1
            margin_y = (y_max - y_min) * 0.1
            x_min -= margin_x
            x_max += margin_x
            y_min -= margin_y
            y_max += margin_y
            
            # ç”Ÿæˆç½‘æ ¼ç‚¹
            xx, yy = np.mgrid[x_min:x_max:30j, y_min:y_max:30j]
            positions = cp.asarray(np.vstack([xx.ravel(), yy.ravel()]).T)
            
            # è®¡ç®—ä¸¤ä¸ªåˆ†å¸ƒçš„å¯†åº¦
            debug_info.info("   â†ª è®¡ç®—å¯†åº¦åˆ†å¸ƒ...")
            log_dens_A = kde_A.score_samples(positions)
            log_dens_B = kde_B.score_samples(positions)
            
            # è½¬å›CPUè¿›è¡Œåç»­è®¡ç®—
            debug_info.info("   â†ª è½¬ç§»ç»“æœå›CPU...")
            density_A = cp.exp(log_dens_A).reshape(xx.shape)
            density_B = cp.exp(log_dens_B).reshape(xx.shape)
            density_A = cp.asnumpy(density_A)
            density_B = cp.asnumpy(density_B)
            
            # è®¡ç®—åˆ†å¸ƒé‡å åº¦
            debug_info.info("   â†ª è®¡ç®—åˆ†å¸ƒé‡å åº¦...")
            min_sum = np.minimum(density_A, density_B).sum()
            max_sum = max(density_A.sum(), density_B.sum())
            overlap = min_sum / max_sum if max_sum > 0 else 0
            
            timing_info.info(f"âœ“ GPUå¯†åº¦ä¼°è®¡å®Œæˆ ({time.time() - start_time:.2f}ç§’)")
            
        except Exception as e:
            debug_info.warning(f"âš ï¸ GPUåŠ é€Ÿå¤±è´¥ï¼Œåˆ‡æ¢åˆ°CPUç‰ˆæœ¬ (åŸå› : {str(e)})")
            start_time = time.time()
            
            # å›é€€åˆ°CPUç‰ˆæœ¬çš„KDE
            kde_A = KernelDensity(kernel='gaussian', bandwidth=0.1).fit(coords_A)
            kde_B = KernelDensity(kernel='gaussian', bandwidth=0.1).fit(coords_B)
            
            # ç”Ÿæˆç½‘æ ¼ç‚¹
            xx, yy = np.mgrid[x_min:x_max:30j, y_min:y_max:30j]
            positions = np.vstack([xx.ravel(), yy.ravel()]).T
            
            # è®¡ç®—å¯†åº¦
            log_dens_A = kde_A.score_samples(positions)
            log_dens_B = kde_B.score_samples(positions)
            density_A = np.exp(log_dens_A).reshape(xx.shape)
            density_B = np.exp(log_dens_B).reshape(xx.shape)
            
            # è®¡ç®—é‡å åº¦
            min_sum = np.minimum(density_A, density_B).sum()
            max_sum = max(density_A.sum(), density_B.sum())
            overlap = min_sum / max_sum if max_sum > 0 else 0
            
            timing_info.info(f"âœ“ CPUå¯†åº¦ä¼°è®¡å®Œæˆ ({time.time() - start_time:.2f}ç§’)")
        
        total_time = time.time() - start_total
        debug_info.success(f"âœ… æ‰€æœ‰åˆ†å¸ƒæŒ‡æ ‡è®¡ç®—å®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        return {
            "ä¸­å¿ƒç‚¹è·ç¦»": center_distance,
            "Aç»„ç¦»æ•£åº¦": dispersion_A,
            "Bç»„ç¦»æ•£åº¦": dispersion_B,
            "Aç»„å¹³å‡è·ç¦»": dist_A,
            "Bç»„å¹³å‡è·ç¦»": dist_B,
            "Xè½´KSæ£€éªŒç»Ÿè®¡é‡": ks_statistic_x,
            "Xè½´KSæ£€éªŒpå€¼": p_value_x,
            "Yè½´KSæ£€éªŒç»Ÿè®¡é‡": ks_statistic_y,
            "Yè½´KSæ£€éªŒpå€¼": p_value_y,
            "åˆ†å¸ƒé‡å åº¦": overlap
        }
        
    except Exception as e:
        debug_info.error(f"âŒ è®¡ç®—åˆ†å¸ƒæŒ‡æ ‡æ—¶å‡ºé”™: {str(e)}")
        return {
            "ä¸­å¿ƒç‚¹è·ç¦»": np.nan,
            "Aç»„ç¦»æ•£åº¦": np.nan,
            "Bç»„ç¦»æ•£åº¦": np.nan,
            "Aç»„å¹³å‡è·ç¦»": np.nan,
            "Bç»„å¹³å‡è·ç¦»": np.nan,
            "Xè½´KSæ£€éªŒç»Ÿè®¡é‡": np.nan,
            "Xè½´KSæ£€éªŒpå€¼": np.nan,
            "Yè½´KSæ£€éªŒç»Ÿè®¡é‡": np.nan,
            "Yè½´KSæ£€éªŒpå€¼": np.nan,
            "åˆ†å¸ƒé‡å åº¦": 0.0
        }

def plot_distribution_comparison(coords_A, coords_B, metrics):
    """ç»˜åˆ¶åˆ†å¸ƒå¯¹æ¯”å›¾"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # æ•£ç‚¹å›¾
    scatter1 = ax1.scatter(coords_A[:, 0], coords_A[:, 1], c='blue', alpha=0.6, s=30, label='æ•°æ®é›†A')
    scatter2 = ax1.scatter(coords_B[:, 0], coords_B[:, 1], c='orange', alpha=0.6, s=30, label='æ•°æ®é›†B')
    ax1.set_title('åˆ†å¸ƒæ•£ç‚¹å›¾')
    ax1.legend()
    
    # å¯†åº¦ç­‰é«˜çº¿å›¾
    x = np.concatenate([coords_A[:, 0], coords_B[:, 0]])
    y = np.concatenate([coords_A[:, 1], coords_B[:, 1]])
    
    # åˆ›å»ºç½‘æ ¼
    xmin, xmax = x.min() - 1, x.max() + 1
    ymin, ymax = y.min() - 1, y.max() + 1
    xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]
    positions = np.vstack([xx.ravel(), yy.ravel()])
    
    # è®¡ç®—KDE
    values_A = np.vstack([coords_A[:, 0], coords_A[:, 1]])
    values_B = np.vstack([coords_B[:, 0], coords_B[:, 1]])
    
    kernel_A = gaussian_kde(values_A)
    kernel_B = gaussian_kde(values_B)
    
    z_A = np.reshape(kernel_A(positions), xx.shape)
    z_B = np.reshape(kernel_B(positions), xx.shape)
    
    # ç»˜åˆ¶ç­‰é«˜çº¿
    ax2.contour(xx, yy, z_A, levels=5, colors='blue', alpha=0.5, label='æ•°æ®é›†A')
    ax2.contour(xx, yy, z_B, levels=5, colors='orange', alpha=0.5, label='æ•°æ®é›†B')
    
    # æ·»åŠ å›¾ä¾‹
    ax2.legend()
    ax2.set_title('å¯†åº¦ç­‰é«˜çº¿å›¾')
    
    plt.tight_layout()
    return fig

def perform_dimensionality_reduction(similarity_matrix, method="t-SNE", perplexity=30, n_neighbors=15, min_dist=0.1):
    """æ‰§è¡Œé™ç»´æ“ä½œï¼Œæ”¯æŒGPUåŠ é€Ÿ"""
    import warnings
    warnings.filterwarnings('ignore', category=UserWarning)
    
    debug_info = st.empty()
    start_time = time.time()
    
    # åˆå§‹åŒ– cuda_available å˜é‡ï¼Œé¿å… UnboundLocalError
    cuda_available = False
    
    try:
        # ç§»é™¤å¯¹ asyncio çš„ä½¿ç”¨ï¼Œå› ä¸ºåœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­ä¸éœ€è¦
        # if not asyncio.get_event_loop().is_running():
        #     asyncio.set_event_loop(asyncio.new_event_loop())
        
        # åˆå§‹åŒ–CUDA
        cuda_available, device = initialize_cuda()
        
        # å°†ç›¸ä¼¼æ€§çŸ©é˜µè½¬æ¢ä¸ºè·ç¦»çŸ©é˜µ
        distance_matrix = 1 - similarity_matrix
        
        if method == "t-SNE":
            if cuda_available:
                debug_info.info("ä½¿ç”¨cuML t-SNEè¿›è¡Œé™ç»´...")
                # æ¸…ç†GPUå†…å­˜
                torch.cuda.empty_cache()
                
                # ä¸º cuML TSNE æ·»åŠ  try-exceptï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ° CPU
                try:
                    tsne = cuTSNE(
                        n_components=2,
                        perplexity=perplexity,
                        random_state=42
                    )
                    coords = tsne.fit_transform(cp.asarray(similarity_matrix))  # ä½¿ç”¨ç›¸ä¼¼æ€§çŸ©é˜µè€Œä¸æ˜¯è·ç¦»çŸ©é˜µ
                    coords = cp.asnumpy(coords)
                    debug_info.success("âœ… æˆåŠŸä½¿ç”¨ cuML TSNE")
                except Exception as e:
                    debug_info.warning(f"âš ï¸ cuML TSNE å¤±è´¥ ({str(e)})ï¼Œå›é€€åˆ° CPU ç‰ˆæœ¬")
                    # å›é€€åˆ° CPU ç‰ˆæœ¬çš„ TSNE
                    tsne = TSNE(
                        n_components=2,
                        perplexity=perplexity,
                        random_state=42,
                        metric='precomputed',
                        init='random',
                        learning_rate='auto'
                    )
                    coords = tsne.fit_transform(distance_matrix)
                
                # æ¸…ç†GPUå†…å­˜
                torch.cuda.empty_cache()
            else:
                debug_info.info("ä½¿ç”¨scikit-learn t-SNEè¿›è¡Œé™ç»´...")
                tsne = TSNE(
                    n_components=2,
                    perplexity=perplexity,
                    random_state=42,
                    metric='precomputed',
                    init='random',
                    learning_rate='auto'
                )
                coords = tsne.fit_transform(distance_matrix)
        
        elif method == "UMAP":
            if cuda_available:
                try:
                    from cuml.manifold import UMAP as cuUMAP
                    debug_info.info("ä½¿ç”¨cuML UMAPè¿›è¡Œé™ç»´...")
                    # æ¸…ç†GPUå†…å­˜
                    torch.cuda.empty_cache()
                    
                    reducer = cuUMAP(
                        n_components=2,
                        n_neighbors=n_neighbors,
                        min_dist=min_dist,
                        random_state=42
                    )
                    coords = reducer.fit_transform(cp.asarray(similarity_matrix))  # ä½¿ç”¨ç›¸ä¼¼æ€§çŸ©é˜µ
                    coords = cp.asnumpy(coords)
                    
                    # æ¸…ç†GPUå†…å­˜
                    torch.cuda.empty_cache()
                except ImportError:
                    debug_info.warning("cuML UMAPä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUç‰ˆæœ¬...")
                    reducer = umap.UMAP(
                        n_components=2,
                        n_neighbors=n_neighbors,
                        min_dist=min_dist,
                        metric='precomputed',
                        random_state=42
                    )
                    coords = reducer.fit_transform(distance_matrix)
                except Exception as e:
                    debug_info.warning(f"âš ï¸ cuML UMAP å¤±è´¥ ({str(e)})ï¼Œå›é€€åˆ° CPU ç‰ˆæœ¬")
                    reducer = umap.UMAP(
                        n_components=2,
                        n_neighbors=n_neighbors,
                        min_dist=min_dist,
                        metric='precomputed',
                        random_state=42
                    )
                    coords = reducer.fit_transform(distance_matrix)
            else:
                debug_info.info("ä½¿ç”¨CPU UMAPè¿›è¡Œé™ç»´...")
                reducer = umap.UMAP(
                    n_components=2,
                    n_neighbors=n_neighbors,
                    min_dist=min_dist,
                    metric='precomputed',
                    random_state=42
                )
                coords = reducer.fit_transform(distance_matrix)
        
        elif method == "PCA":
            # æ·»åŠ  PCA æ”¯æŒ
            from sklearn.decomposition import PCA
            debug_info.info("ä½¿ç”¨PCAè¿›è¡Œé™ç»´...")
            # PCA éœ€è¦ç‰¹å¾çŸ©é˜µï¼Œæˆ‘ä»¬ä½¿ç”¨ç›¸ä¼¼æ€§çŸ©é˜µ
            pca = PCA(n_components=2, random_state=42)
            coords = pca.fit_transform(similarity_matrix)
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„é™ç»´æ–¹æ³•: {method}")
        
        # éªŒè¯è¾“å‡ºç»´åº¦
        if coords.shape[1] != 2:
            raise ValueError(f"é™ç»´ç»“æœç»´åº¦ä¸æ­£ç¡®: {coords.shape}")
        
        debug_info.success(f"âœ… é™ç»´å®Œæˆ ({time.time() - start_time:.2f}ç§’)")
        return coords
    
    except Exception as e:
        debug_info.error(f"é™ç»´è¿‡ç¨‹å‡ºé”™: {str(e)}")
        st.error(f"é™ç»´å¤±è´¥: {str(e)}")
        return None
    
    finally:
        # æ¸…ç†GPUå†…å­˜ - ç°åœ¨ cuda_available æ€»æ˜¯è¢«åˆå§‹åŒ–
        if cuda_available:
            torch.cuda.empty_cache()

# ä¸»ç•Œé¢
st.title("ç»“æ„å¤šæ ·æ€§è¯„ä¼°")

# æ·»åŠ å¸®åŠ©ä¿¡æ¯åœ¨ç•Œé¢æœ€ä¸Šæ–¹
with st.expander("ğŸ’¡ æŒ‡çº¹ç±»å‹å’Œå‚æ•°è®¾ç½®è¯´æ˜", expanded=True):
    st.markdown("""
    ### åˆ†å­æŒ‡çº¹ç±»å‹è¯´æ˜
    
    #### 1. MACCS Keys (166ä½)
    - ç‰¹ç‚¹ï¼šå›ºå®šé•¿åº¦166ä½çš„ç»“æ„é”®æŒ‡çº¹
    - é€‚ç”¨ï¼šå¿«é€Ÿç›¸ä¼¼æ€§æœç´¢ï¼ŒåŸºç¡€ç»“æ„ç‰¹å¾è¯†åˆ«
    - ä¼˜åŠ¿ï¼šè®¡ç®—é€Ÿåº¦å¿«ï¼Œç»“æœæ˜“è§£é‡Š
    - å±€é™ï¼šä»…åŒ…å«é¢„å®šä¹‰çš„166ä¸ªç»“æ„ç‰¹å¾
    
    #### 2. MorganæŒ‡çº¹ï¼ˆECFPï¼‰
    - ç‰¹ç‚¹ï¼šç¯å¢ƒæ•æ„Ÿçš„å¾ªç¯æŒ‡çº¹ï¼ŒåŸºäºåˆ†å­ä¸­åŸå­ç¯å¢ƒ
    - å‚æ•°è¯´æ˜ï¼š
        - åŠå¾„(Radius)ï¼š2-3æœ€å¸¸ç”¨
            - 2: ECFP4ï¼Œæ•è·ç›´å¾„4çš„ç¯å¢ƒ
            - 3: ECFP6ï¼Œæ•è·æ›´å¤§èŒƒå›´çš„ç»“æ„ç‰¹å¾
        - ä½æ•°(nBits)ï¼š512-2048å¸¸ç”¨
            - è¾ƒå°(512)ï¼šæ›´å¿«çš„è®¡ç®—ï¼Œå¯èƒ½æœ‰ä¿¡æ¯æŸå¤±
            - è¾ƒå¤§(2048)ï¼šæ›´è¯¦ç»†çš„ç»“æ„ä¿¡æ¯ï¼Œå ç”¨æ›´å¤šå†…å­˜
    - é€‚ç”¨ï¼šè¯ç‰©å‘ç°ï¼Œç²¾ç¡®ç»“æ„åŒ¹é…
    
    #### 3. RDKitæŒ‡çº¹
    - ç‰¹ç‚¹ï¼šè·¯å¾„å‹æŒ‡çº¹ï¼ŒåŸºäºåˆ†å­ä¸­çš„åŸå­è·¯å¾„
    - å‚æ•°è¯´æ˜ï¼š
        - æœ€å°è·¯å¾„(minPath)ï¼š1-2å¸¸ç”¨
            - 1ï¼šåŒ…å«å•é”®ä¿¡æ¯
            - 2ï¼šä»åŒé”®å¼€å§‹
        - æœ€å¤§è·¯å¾„(maxPath)ï¼š5-7å¸¸ç”¨
            - è¾ƒå°ï¼šå…³æ³¨å±€éƒ¨ç»“æ„
            - è¾ƒå¤§ï¼šåŒ…å«æ›´å¤šå¤§èŒƒå›´ç»“æ„ä¿¡æ¯
        - ä½æ•°(nBits)ï¼š1024-4096å¸¸ç”¨
    - é€‚ç”¨ï¼šé€šç”¨ç›¸ä¼¼æ€§æœç´¢ï¼Œç»“æ„éª¨æ¶åˆ†æ
    
    #### 4. Atom PairsæŒ‡çº¹
    - ç‰¹ç‚¹ï¼šåŸºäºåŸå­å¯¹ä¹‹é—´çš„æ‹“æ‰‘è·ç¦»
    - å‚æ•°è¯´æ˜ï¼š
        - æœ€å¤§è·ç¦»(maxDistance)ï¼š10-30å¸¸ç”¨
            - è¾ƒå°ï¼šå…³æ³¨è¿‘è·ç¦»åŸå­å…³ç³»
            - è¾ƒå¤§ï¼šåŒ…å«åˆ†å­æ•´ä½“ç»“æ„ä¿¡æ¯
    - é€‚ç”¨ï¼šæ„è±¡æ— å…³çš„ç»“æ„æ¯”è¾ƒ
    
    ### èšç±»å‚æ•°è®¾ç½®æŒ‡å—
    
    #### 1. K-Meansèšç±»
    - ç°‡æ•°(n_clusters)å»ºè®®ï¼š
        - å°æ•°æ®é›†(<1000): 3-10
        - ä¸­ç­‰æ•°æ®é›†(1000-10000): 10-50
        - å¤§æ•°æ®é›†(>10000): 50-200
    - ç‰¹ç‚¹ï¼š
        - ä¼˜åŠ¿ï¼šå¿«é€Ÿï¼Œç»“æœæ˜“ç†è§£
        - å±€é™ï¼šéœ€è¦é¢„å…ˆæŒ‡å®šç°‡æ•°ï¼Œå¯¹å¼‚å¸¸å€¼æ•æ„Ÿ
    
    #### 2. DBSCANèšç±»
    - eps(é‚»åŸŸåŠå¾„)å»ºè®®ï¼š
        - Morgan/RDKitæŒ‡çº¹: 0.2-0.4
        - MACCS: 0.3-0.5
        - Atom Pairs: 0.25-0.45
    - min_sampleså»ºè®®ï¼š
        - å°æ•°æ®é›†: 3-5
        - å¤§æ•°æ®é›†: 5-10
    - ç‰¹ç‚¹ï¼š
        - ä¼˜åŠ¿ï¼šå¯å‘ç°ä»»æ„å½¢çŠ¶çš„ç°‡ï¼Œè‡ªåŠ¨å¤„ç†å¼‚å¸¸ç‚¹
        - å±€é™ï¼šå‚æ•°æ•æ„Ÿï¼Œè®¡ç®—è¾ƒæ…¢
    
    ### å¯è§†åŒ–é™ç»´æ–¹æ³•é€‰æ‹©
    
    #### 1. PCA
    - ä¼˜åŠ¿ï¼šå¿«é€Ÿï¼Œä¿æŒå…¨å±€ç»“æ„
    - é€‚ç”¨ï¼šåˆæ­¥æ•°æ®æ¢ç´¢ï¼Œçº¿æ€§å…³ç³»æ˜¾è‘—çš„æ•°æ®
    
    #### 2. t-SNE
    - å‚æ•°ï¼šperplexity (5-50)
        - å°æ•°æ®é›†ï¼š5-15
        - å¤§æ•°æ®é›†ï¼š30-50
    - ä¼˜åŠ¿ï¼šä¿æŒå±€éƒ¨ç»“æ„ï¼Œèšç±»å¯è§†åŒ–æ•ˆæœå¥½
    - é€‚ç”¨ï¼šéçº¿æ€§æ•°æ®ï¼Œéœ€è¦è¯¦ç»†æŸ¥çœ‹å±€éƒ¨ç»“æ„
    
    #### 3. UMAP
    - å‚æ•°ï¼š
        - n_neighborsï¼š10-50
        - min_distï¼š0.1-0.5
    - ä¼˜åŠ¿ï¼šä¿æŒå…¨å±€å’Œå±€éƒ¨ç»“æ„ï¼Œé€Ÿåº¦å¿«
    - é€‚ç”¨ï¼šå¤§è§„æ¨¡æ•°æ®é›†ï¼Œéœ€è¦å¹³è¡¡å…¨å±€å’Œå±€éƒ¨ç»“æ„
    """)

# ä¾§è¾¹æ è®¾ç½®
st.sidebar.title("å‚æ•°è®¾ç½®")

# æŒ‡çº¹è®¾ç½®
with st.sidebar.expander("æŒ‡çº¹è®¾ç½®", expanded=True):
    fp_type = st.selectbox(
        "æŒ‡çº¹ç±»å‹",
        ["morgan", "maccs", "topological", "atom_pairs"],
        help="""
        - morgan: Morgan/ECFPæŒ‡çº¹ï¼Œé€‚åˆè¯ç‰©å‘ç°
        - maccs: MACCS Keysï¼Œ166ä½ç»“æ„é”®æŒ‡çº¹
        - topological: æ‹“æ‰‘æŒ‡çº¹ï¼ŒåŸºäºåˆ†å­å›¾
        - atom_pairs: åŸå­å¯¹æŒ‡çº¹ï¼ŒåŸºäºåŸå­å¯¹è·ç¦»
        """
    )
    
    if fp_type == "morgan":
        radius = st.slider("MorganåŠå¾„", 1, 4, 2)
        nBits = st.slider("æŒ‡çº¹ä½æ•°", 512, 4096, 2048)
    elif fp_type == "topological":
        nBits = st.slider("æŒ‡çº¹ä½æ•°", 512, 4096, 2048)
    elif fp_type == "atom_pairs":
        max_distance = st.slider("æœ€å¤§è·ç¦»", 5, 30, 10)
        nBits = st.slider("æŒ‡çº¹ä½æ•°", 512, 4096, 2048)

# èšç±»è®¾ç½®
with st.sidebar.expander("èšç±»è®¾ç½®", expanded=True):
    n_clusters = st.slider(
        "èšç±»æ•°é‡(K-means)",
        min_value=2,
        max_value=20,
        value=5,
        help="K-meansèšç±»çš„ç°‡æ•°"
    )
    
    eps = st.slider(
        "DBSCANé‚»åŸŸå¤§å°",
        min_value=0.1,
        max_value=1.0,
        value=0.3,
        step=0.1,
        help="DBSCANçš„é‚»åŸŸåŠå¾„å‚æ•°"
    )
    
    min_samples = st.slider(
        "DBSCANæœ€å°æ ·æœ¬æ•°",
        min_value=2,
        max_value=20,
        value=5,
        help="DBSCANåˆ¤å®šæ ¸å¿ƒç‚¹æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°"
    )

# å¯è§†åŒ–è®¾ç½®
with st.sidebar.expander("å¯è§†åŒ–è®¾ç½®", expanded=True):
    dim_reduction_method = st.selectbox(
        "é™ç»´æ–¹æ³•",
        ["t-SNE", "UMAP", "PCA"],
        help="""
        - t-SNE: ä¿æŒå±€éƒ¨ç»“æ„ï¼Œé€‚åˆèšç±»å¯è§†åŒ–
        - UMAP: å¹³è¡¡å…¨å±€å’Œå±€éƒ¨ç»“æ„ï¼Œé€Ÿåº¦å¿«
        - PCA: ä¿æŒå…¨å±€ç»“æ„ï¼Œé€‚åˆåˆæ­¥æ¢ç´¢
        """
    )
    
    if dim_reduction_method == "t-SNE":
        perplexity = st.slider(
            "Perplexity",
            min_value=5,
            max_value=50,
            value=30,
            help="t-SNEçš„å›°æƒ‘åº¦å‚æ•°ï¼Œå½±å“å±€éƒ¨ç»“æ„çš„ä¿æŒ"
        )
    elif dim_reduction_method == "UMAP":
        n_neighbors = st.slider(
            "é‚»å±…æ•°é‡",
            min_value=5,
            max_value=100,
            value=15,
            help="UMAPçš„é‚»å±…æ•°é‡å‚æ•°"
        )
        min_dist = st.slider(
            "æœ€å°è·ç¦»",
            min_value=0.0,
            max_value=0.99,
            value=0.1,
            help="UMAPçš„æœ€å°è·ç¦»å‚æ•°"
        )

# æ–‡ä»¶ä¸Šä¼ ç•Œé¢
col1, col2 = st.columns(2)

with col1:
    st.subheader("æ•°æ®é›†A")
    fileA = st.file_uploader("ä¸Šä¼ ç¬¬ä¸€ä¸ªCSVæ–‡ä»¶", type="csv")
    
with col2:
    st.subheader("æ•°æ®é›†B")
    fileB = st.file_uploader("ä¸Šä¼ ç¬¬äºŒä¸ªCSVæ–‡ä»¶", type="csv")

# SMILESåˆ—åè¾“å…¥
smiles_col = st.text_input("SMILESåˆ—å", value="SMILES")

if st.button("å¼€å§‹è¯„ä¼°") and fileA is not None and fileB is not None:
    with st.spinner("æ­£åœ¨è¿›è¡Œå¤šæ ·æ€§è¯„ä¼°..."):
        # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ
        mem_usage = monitor_memory_usage()
        st.sidebar.info(
            f"å†…å­˜ä½¿ç”¨æƒ…å†µ:\n"
            f"- RSS: {mem_usage['rss']:.1f} MB\n"
            f"- å†…å­˜å ç”¨: {mem_usage['percent']:.1f}%"
        )
        
        # åŠ è½½åˆ†å­
        molsA, dfA = load_molecules_from_csv(fileA, smiles_col)
        molsB, dfB = load_molecules_from_csv(fileB, smiles_col)
        
        if molsA and molsB:
            st.success(f"æˆåŠŸåŠ è½½: æ•°æ®é›†A {len(molsA)}ä¸ªåˆ†å­, æ•°æ®é›†B {len(molsB)}ä¸ªåˆ†å­")
            
            # 1. æŒ‡çº¹å¤šæ ·æ€§åˆ†æ
            st.header("1. æŒ‡çº¹å¤šæ ·æ€§åˆ†æ")
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # è®¡ç®—æŒ‡çº¹
            with st.spinner(f"æ­£åœ¨è®¡ç®—{fp_type}æŒ‡çº¹..."):
                total_mols = len(molsA) + len(molsB)
                
                # åˆ›å»ºæŒ‡çº¹ç”Ÿæˆå™¨
                if fp_type == "morgan":
                    fp_gen = GetMorganGenerator(radius=int(radius), fpSize=int(nBits))
                elif fp_type == "topological":
                    fp_gen = GetRDKitFPGenerator(fpSize=int(nBits), minPath=1, maxPath=7)
                
                fpsA = []
                fpsB = []
                
                # å¤„ç†æ•°æ®é›†A
                for i, mol in enumerate(molsA):
                    try:
                        if fp_type == "morgan":
                            fp = fp_gen.GetFingerprint(mol)
                        elif fp_type == "maccs":
                            fp = AllChem.GetMACCSKeysFingerprint(mol)
                        elif fp_type == "topological":
                            fp = fp_gen.GetFingerprint(mol)
                        elif fp_type == "atom_pairs":
                            fp = AllChem.GetHashedAtomPairFingerprintAsBitVect(mol, nBits=int(nBits))
                        fpsA.append(fp)
                    except Exception as e:
                        st.warning(f"å¤„ç†åˆ†å­æ—¶å‡ºé”™: {str(e)}")
                        continue
                    
                    # æ›´æ–°è¿›åº¦
                    progress = min(1.0, (i + 1) / total_mols * 0.5)
                    progress_bar.progress(progress)
                    status_text.text(f"å¤„ç†æ•°æ®é›†A: {i+1}/{len(molsA)}")
                
                # å¤„ç†æ•°æ®é›†B
                for i, mol in enumerate(molsB):
                    try:
                        if fp_type == "morgan":
                            fp = fp_gen.GetFingerprint(mol)
                        elif fp_type == "maccs":
                            fp = AllChem.GetMACCSKeysFingerprint(mol)
                        elif fp_type == "topological":
                            fp = fp_gen.GetFingerprint(mol)
                        elif fp_type == "atom_pairs":
                            fp = AllChem.GetHashedAtomPairFingerprintAsBitVect(mol, nBits=int(nBits))
                        fpsB.append(fp)
                    except Exception as e:
                        st.warning(f"å¤„ç†åˆ†å­æ—¶å‡ºé”™: {str(e)}")
                        continue
                    
                    # æ›´æ–°è¿›åº¦
                    progress = min(1.0, 0.5 + (i + 1) / total_mols * 0.5)
                    progress_bar.progress(progress)
                    status_text.text(f"å¤„ç†æ•°æ®é›†B: {i+1}/{len(molsB)}")
                
                # è®¡ç®—ç›¸ä¼¼æ€§çŸ©é˜µï¼ˆåªè®¡ç®—ä¸€æ¬¡ï¼‰
                sim_matrixA = compute_similarity_matrix(fpsA, "è®¡ç®—æ•°æ®é›†Aç›¸ä¼¼æ€§çŸ©é˜µ")
                sim_matrixB = compute_similarity_matrix(fpsB, "è®¡ç®—æ•°æ®é›†Bç›¸ä¼¼æ€§çŸ©é˜µ")
            
            progress_bar.empty()
            status_text.empty()
            
            # æ·»åŠ æ–°çš„å¯è§†åŒ–
            st.subheader("Fingerprint Bit Frequency Distribution")
            col1, col2 = st.columns(2)
            with col1:
                fig = plot_bit_frequency(fpsA, "Library A Bit Frequency")
                st.pyplot(fig)
                plt.close(fig)
            
            with col2:
                fig = plot_bit_frequency(fpsB, "Library B Bit Frequency")
                st.pyplot(fig)
                plt.close(fig)
            
            st.subheader("Similarity Matrix Heatmap")
            col1, col2 = st.columns(2)
            with col1:
                fig = plot_similarity_heatmap(sim_matrixA)
                st.pyplot(fig)
                plt.close(fig)
            
            with col2:
                fig = plot_similarity_heatmap(sim_matrixB)
                st.pyplot(fig)
                plt.close(fig)
            
            st.subheader("Nearest Neighbor Distribution")
            col1, col2 = st.columns(2)
            with col1:
                fig = plot_nearest_neighbor_distribution(sim_matrixA)
                st.pyplot(fig)
                plt.close(fig)
            
            with col2:
                fig = plot_nearest_neighbor_distribution(sim_matrixB)
                st.pyplot(fig)
                plt.close(fig)
            
            # èšç±»åˆ†æ
            st.subheader("Clustering Analysis")
            with st.spinner("Performing clustering analysis..."):
                # ç¡®ä¿ n_clusters æ˜¯æ•´æ•°ç±»å‹, eps æ˜¯æµ®ç‚¹ç±»å‹
                # å‡è®¾ n_clusters å’Œ eps å˜é‡åœ¨æ­¤å¤„æ˜¯å¯ç”¨çš„
                # æ‚¨éœ€è¦ç¡®ä¿å®ƒä»¬åœ¨ streamlit UI ä¸­è¢«æ­£ç¡®å®šä¹‰å’Œè·å–
                try:
                    current_n_clusters = int(n_clusters) 
                except NameError:
                    st.error("å˜é‡ 'n_clusters' æœªå®šä¹‰ã€‚è¯·æ£€æŸ¥æ‚¨çš„ Streamlit UI è¾“å…¥éƒ¨åˆ†ã€‚")
                    st.stop()
                except ValueError:
                    st.error(f"å˜é‡ 'n_clusters' çš„å€¼ '{n_clusters}' æ— æ³•è½¬æ¢ä¸ºæ•´æ•°ã€‚è¯·æ£€æŸ¥è¾“å…¥ã€‚")
                    st.stop()
                
                try:
                    current_eps = float(eps)
                except NameError:
                    st.error("å˜é‡ 'eps' æœªå®šä¹‰ã€‚è¯·æ£€æŸ¥æ‚¨çš„ Streamlit UI è¾“å…¥éƒ¨åˆ†ã€‚")
                    st.stop()
                except ValueError:
                    st.error(f"å˜é‡ 'eps' çš„å€¼ '{eps}' æ— æ³•è½¬æ¢ä¸ºæµ®ç‚¹æ•°ã€‚è¯·æ£€æŸ¥è¾“å…¥ã€‚")
                    st.stop()

                st.write(f"Debug: Using n_clusters: {current_n_clusters} (type: {type(current_n_clusters)}) for KMeans")
                st.write(f"Debug: Using eps: {current_eps} (type: {type(current_eps)}) for DBSCAN")
                
                # è·å–å¹¶è½¬æ¢ min_samples
                try:
                    # min_samples æ˜¯ä» st.slider è·å–çš„ï¼Œåº”è¯¥å·²ç»æ˜¯ int
                    current_min_samples = int(min_samples) 
                except NameError:
                    st.error("å˜é‡ 'min_samples' æœªå®šä¹‰ã€‚è¯·æ£€æŸ¥æ‚¨çš„ Streamlit UI è¾“å…¥éƒ¨åˆ†ã€‚")
                    st.stop()
                except ValueError:
                    st.error(f"å˜é‡ 'min_samples' çš„å€¼ '{min_samples}' æ— æ³•è½¬æ¢ä¸ºæ•´æ•°ã€‚è¯·æ£€æŸ¥è¾“å…¥ã€‚")
                    st.stop()
                st.write(f"Debug: Using min_samples: {current_min_samples} (type: {type(current_min_samples)}) for DBSCAN")

                clustering_resultsA = perform_clustering_analysis(
                    sim_matrixA, 
                    n_clusters=current_n_clusters,
                    eps=current_eps,
                    min_samples=current_min_samples, # ä¼ é€’ min_samples
                    perplexity=30.0 # Pass fixed perplexity for this visualization
                )
                clustering_resultsB = perform_clustering_analysis(
                    sim_matrixB, 
                    n_clusters=current_n_clusters,
                    eps=current_eps,
                    min_samples=current_min_samples, # ä¼ é€’ min_samples
                    perplexity=30.0 # Pass fixed perplexity for this visualization
                )
                
                col1, col2 = st.columns(2)
                with col1:
                    fig = plot_clustering_results(clustering_resultsA, "Library A Clustering Results")
                    st.pyplot(fig)
                    plt.close(fig)
                
                with col2:
                    fig = plot_clustering_results(clustering_resultsB, "Library B Clustering Results")
                    st.pyplot(fig)
                    plt.close(fig)
            
            # 2. éª¨æ¶å¤šæ ·æ€§
            st.header("2. éª¨æ¶å¤šæ ·æ€§")
            col1, col2 = st.columns(2)
            
            num_scafA, scaf_entropyA, frac_singletonsA, scaff_freq_A = calc_scaffold_stats(molsA)
            num_scafB, scaf_entropyB, frac_singletonsB, scaff_freq_B = calc_scaffold_stats(molsB)
            
            f50_A = calc_F50(scaff_freq_A)
            f50_B = calc_F50(scaff_freq_B)
            
            with col1:
                st.subheader("æ•°æ®é›†A")
                st.write(f"éª¨æ¶æ€»æ•°: {num_scafA}")
                st.write(f"éª¨æ¶ç†µ: {scaf_entropyA:.3f}")
                st.write(f"å•ä¾‹éª¨æ¶æ¯”ä¾‹: {frac_singletonsA:.3f}")
                st.write(f"F50å€¼: {f50_A:.3f}")
                
                if not scaff_freq_A.empty:
                    st.write("å‰10ä¸ªæœ€å¸¸è§éª¨æ¶:")
                    st.dataframe(scaff_freq_A.head(10))
            
            with col2:
                st.subheader("æ•°æ®é›†B")
                st.write(f"éª¨æ¶æ€»æ•°: {num_scafB}")
                st.write(f"éª¨æ¶ç†µ: {scaf_entropyB:.3f}")
                st.write(f"å•ä¾‹éª¨æ¶æ¯”ä¾‹: {frac_singletonsB:.3f}")
                st.write(f"F50å€¼: {f50_B:.3f}")
                
                if not scaff_freq_B.empty:
                    st.write("å‰10ä¸ªæœ€å¸¸è§éª¨æ¶:")
                    st.dataframe(scaff_freq_B.head(10))
            
            # 3. ç†åŒ–æ€§è´¨åˆ†å¸ƒ
            st.header("3. Property Distribution Analysis")
            
            propsA, prop_labels = calc_property_stats(molsA)
            propsB, prop_labels = calc_property_stats(molsB)
            
            # Display statistical summary
            col1, col2 = st.columns(2)
            with col1:
                st.subheader("Library A Statistics")
                summary_A = propsA.describe()
                summary_A.index = ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']
                st.dataframe(summary_A.round(2))
            
            with col2:
                st.subheader("Library B Statistics")
                summary_B = propsB.describe()
                summary_B.index = ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']
                st.dataframe(summary_B.round(2))
            
            # Plot distribution comparisons in two columns
            st.subheader("Property Distribution Comparison")
            properties = list(prop_labels.keys())
            for i in range(0, len(properties), 2):
                col1, col2 = st.columns(2)
                
                # First plot in the row
                with col1:
                    prop = properties[i]
                    fig = plot_property_comparison(propsA, propsB, prop, prop_labels[prop])
                    st.pyplot(fig)
                    plt.close(fig)
                
                # Second plot in the row (if available)
                with col2:
                    if i + 1 < len(properties):
                        prop = properties[i + 1]
                        fig = plot_property_comparison(propsA, propsB, prop, prop_labels[prop])
                        st.pyplot(fig)
                        plt.close(fig)
            
            # Export assessment report
            st.header("Export Assessment Report")
            
            # Update the report data with English labels
            report_data = {
                "Metric": [
                    "Number of Molecules",
                    "Mean Similarity",
                    "Similarity Std",
                    "Max Similarity",
                    "Min Similarity",
                    "Median Similarity"
                ],
                "Library A": [
                    len(molsA),
                    calculate_diversity_metrics(sim_matrixA)['å¹³å‡ç›¸ä¼¼æ€§'],
                    calculate_diversity_metrics(sim_matrixA)['ç›¸ä¼¼æ€§æ ‡å‡†å·®'],
                    calculate_diversity_metrics(sim_matrixA)['æœ€å¤§ç›¸ä¼¼æ€§'],
                    calculate_diversity_metrics(sim_matrixA)['æœ€å°ç›¸ä¼¼æ€§'],
                    calculate_diversity_metrics(sim_matrixA)['ä¸­ä½æ•°ç›¸ä¼¼æ€§']
                ],
                "Library B": [
                    len(molsB),
                    calculate_diversity_metrics(sim_matrixB)['å¹³å‡ç›¸ä¼¼æ€§'],
                    calculate_diversity_metrics(sim_matrixB)['ç›¸ä¼¼æ€§æ ‡å‡†å·®'],
                    calculate_diversity_metrics(sim_matrixB)['æœ€å¤§ç›¸ä¼¼æ€§'],
                    calculate_diversity_metrics(sim_matrixB)['æœ€å°ç›¸ä¼¼æ€§'],
                    calculate_diversity_metrics(sim_matrixB)['ä¸­ä½æ•°ç›¸ä¼¼æ€§']
                ]
            }
            
            # Export assessment report
            report_df = pd.DataFrame(report_data)
            csv = report_df.to_csv(index=False)
            st.download_button(
                "Download Assessment Report",
                csv,
                "diversity_assessment_report.csv",
                "text/csv",
                key='download-report'
            )
            
            # æ·»åŠ ç»“æ„åˆ†å¸ƒåˆ†æéƒ¨åˆ†
            st.header("4. ç»“æ„åˆ†å¸ƒåˆ†æ")
            
            debug_info = st.empty()
            timing_info = st.empty()

            # è·å–æ•°æ®é›†Aå’ŒBçš„SMILESé›†åˆ
            debug_info.info("å¼€å§‹å¤„ç†æ•°æ®é›†...")
            smiles_A = [Chem.MolToSmiles(mol) for mol in molsA]
            smiles_B = set([Chem.MolToSmiles(mol) for mol in molsB])
            debug_info.info(f"åŸå§‹æ•°æ®é›†AåŒ…å« {len(smiles_A)} ä¸ªåˆ†å­")
            debug_info.info(f"é€‰ä¸­çš„æ•°æ®é›†BåŒ…å« {len(smiles_B)} ä¸ªåˆ†å­")

            # åˆ›å»ºä¸¤ä¸ªç‹¬ç«‹çš„æ•°æ®é›†ç”¨äºæ¯”è¾ƒ
            dataset_A = []  # åŸå§‹å®Œæ•´æ•°æ®é›†
            dataset_B = []  # è¢«é€‰ä¸­çš„å­é›†
            sources_A = []  # åŸå§‹æ•°æ®é›†çš„æ ‡è®°
            sources_B = []  # å­é›†çš„æ ‡è®°

            # å¤„ç†æ•°æ®é›†Aï¼ˆå®Œæ•´æ•°æ®é›†ï¼‰
            for mol in molsA:
                dataset_A.append(mol)
                sources_A.append('A')

            # å¤„ç†æ•°æ®é›†Bï¼ˆé€‰ä¸­çš„å­é›†ï¼‰
            for mol in molsB:
                dataset_B.append(mol)
                sources_B.append('B')

            # ç»Ÿè®¡ä¿¡æ¯
            debug_info.info(f"å¤„ç†å®Œæˆ:")
            debug_info.info(f"- åŸå§‹æ•°æ®é›†å¤§å°: {len(dataset_A)}ä¸ªåˆ†å­")
            debug_info.info(f"- é€‰ä¸­å­é›†å¤§å°: {len(dataset_B)}ä¸ªåˆ†å­")

            # è®¡ç®—ä¸¤ä¸ªæ•°æ®é›†çš„æŒ‡çº¹
            debug_info.info("å¼€å§‹è®¡ç®—æŒ‡çº¹...")
            start_time = time.time()

            # è®¡ç®—æ•°æ®é›†Açš„æŒ‡çº¹
            fps_A = []
            for i, mol in enumerate(dataset_A):
                try:
                    if fp_type == "morgan":
                        fp = fp_gen.GetFingerprint(mol)
                    elif fp_type == "maccs":
                        fp = AllChem.GetMACCSKeysFingerprint(mol)
                    elif fp_type == "topological":
                        fp = fp_gen.GetFingerprint(mol)
                    elif fp_type == "atom_pairs":
                        fp = AllChem.GetHashedAtomPairFingerprintAsBitVect(mol, nBits=int(nBits))
                    fps_A.append(fp)
                    
                    if (i + 1) % 100 == 0:
                        debug_info.info(f"å·²å¤„ç†æ•°æ®é›†A: {i+1}/{len(dataset_A)} ä¸ªåˆ†å­çš„æŒ‡çº¹")
                except Exception as e:
                    st.warning(f"å¤„ç†æ•°æ®é›†Aç¬¬{i+1}ä¸ªåˆ†å­æŒ‡çº¹æ—¶å‡ºé”™: {str(e)}")
                    continue

            # è®¡ç®—æ•°æ®é›†Bçš„æŒ‡çº¹
            fps_B = []
            for i, mol in enumerate(dataset_B):
                try:
                    if fp_type == "morgan":
                        fp = fp_gen.GetFingerprint(mol)
                    elif fp_type == "maccs":
                        fp = AllChem.GetMACCSKeysFingerprint(mol)
                    elif fp_type == "topological":
                        fp = fp_gen.GetFingerprint(mol)
                    elif fp_type == "atom_pairs":
                        fp = AllChem.GetHashedAtomPairFingerprintAsBitVect(mol, nBits=int(nBits))
                    fps_B.append(fp)
                    
                    if (i + 1) % 100 == 0:
                        debug_info.info(f"å·²å¤„ç†æ•°æ®é›†B: {i+1}/{len(dataset_B)} ä¸ªåˆ†å­çš„æŒ‡çº¹")
                except Exception as e:
                    st.warning(f"å¤„ç†æ•°æ®é›†Bç¬¬{i+1}ä¸ªåˆ†å­æŒ‡çº¹æ—¶å‡ºé”™: {str(e)}")
                    continue

            timing_info.text(f"æŒ‡çº¹è®¡ç®—è€—æ—¶: {time.time() - start_time:.2f}ç§’")
            debug_info.info(f"æˆåŠŸè®¡ç®—çš„æŒ‡çº¹æ•°é‡ - æ•°æ®é›†A: {len(fps_A)}, æ•°æ®é›†B: {len(fps_B)}")

            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            debug_info.info("å¼€å§‹è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ...")
            start_time = time.time()

            # åˆå¹¶æ‰€æœ‰æŒ‡çº¹å¹¶è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            fps_combined = fps_A + fps_B
            sources_combined = sources_A + sources_B
            sim_matrix_combined = compute_similarity_matrix(fps_combined, "è®¡ç®—åˆå¹¶æ•°æ®é›†ç›¸ä¼¼æ€§çŸ©é˜µ")
            timing_info.text(f"ç›¸ä¼¼åº¦çŸ©é˜µè®¡ç®—è€—æ—¶: {time.time() - start_time:.2f}ç§’")

            # é™ç»´
            debug_info.info(f"å¼€å§‹ä½¿ç”¨{dim_reduction_method}è¿›è¡Œé™ç»´...")
            start_time = time.time()

            # æ·»åŠ è¾“å…¥çŸ©é˜µçš„å½¢çŠ¶ä¿¡æ¯
            debug_info.info(f"è¾“å…¥ç›¸ä¼¼æ€§çŸ©é˜µå½¢çŠ¶: {sim_matrix_combined.shape}")

            coords = perform_dimensionality_reduction(
                sim_matrix_combined,
                method=dim_reduction_method,
                perplexity=perplexity if dim_reduction_method == "t-SNE" else None,
                n_neighbors=n_neighbors if dim_reduction_method == "UMAP" else None,
                min_dist=min_dist if dim_reduction_method == "UMAP" else None
            )

            # æ·»åŠ é™ç»´ç»“æœçš„å½¢çŠ¶ä¿¡æ¯
            debug_info.info(f"é™ç»´åçš„åæ ‡å½¢çŠ¶: {coords.shape}")
            if coords.shape[1] != 2:
                debug_info.error(f"é”™è¯¯ï¼šé™ç»´ç»“æœç»´åº¦ä¸æ­£ç¡®ï¼ŒæœŸæœ›2ç»´ä½†å¾—åˆ°{coords.shape[1]}ç»´")
                st.error("é™ç»´è¿‡ç¨‹å‡ºé”™ï¼šç»“æœç»´åº¦ä¸æ­£ç¡®") 
                st.stop()

            timing_info.text(f"é™ç»´è®¡ç®—è€—æ—¶: {time.time() - start_time:.2f}ç§’")

            # åˆ†ç¦»åæ ‡
            debug_info.info("åˆ†ç¦»æ•°æ®é›†åæ ‡...")
            coords_A = coords[:len(dataset_A)]  # åŸå§‹æ•°æ®é›†çš„åæ ‡
            coords_B = coords[len(dataset_A):]  # é€‰ä¸­å­é›†çš„åæ ‡

            debug_info.info(f"åˆ†ç¦»åçš„åæ ‡å½¢çŠ¶:")
            debug_info.info(f"- åŸå§‹æ•°æ®é›†(A): {coords_A.shape}")
            debug_info.info(f"- é€‰ä¸­å­é›†(B): {coords_B.shape}")

            # éªŒè¯åæ ‡æ•°æ®çš„æœ‰æ•ˆæ€§
            if len(coords_A) < 2 or len(coords_B) < 2:
                debug_info.error("é”™è¯¯ï¼šä¸€ä¸ªæˆ–ä¸¤ä¸ªæ•°æ®é›†æ ·æœ¬æ•°é‡ä¸è¶³ï¼ˆéœ€è¦è‡³å°‘2ä¸ªæ ·æœ¬ï¼‰")
                st.error("æ— æ³•è¿›è¡Œåˆ†å¸ƒåˆ†æï¼šæ ·æœ¬æ•°é‡ä¸è¶³")
            elif np.isnan(coords_A).any() or np.isnan(coords_B).any():
                debug_info.error("é”™è¯¯ï¼šåæ ‡ä¸­åŒ…å«NaNå€¼")
                st.error("æ— æ³•è¿›è¡Œåˆ†å¸ƒåˆ†æï¼šåæ ‡åŒ…å«æ— æ•ˆå€¼")
            else:
                # è®¡ç®—åˆ†å¸ƒæŒ‡æ ‡
                debug_info.info("å¼€å§‹è®¡ç®—åˆ†å¸ƒæŒ‡æ ‡...")
                start_time = time.time()
                distribution_metrics = calculate_distribution_metrics(coords_A, coords_B)
                timing_info.text(f"åˆ†å¸ƒæŒ‡æ ‡è®¡ç®—è€—æ—¶: {time.time() - start_time:.2f}ç§’")

                # å¯è§†åŒ–ç»“æœ
                debug_info.info("å¼€å§‹ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
                start_time = time.time()
                st.subheader(f"ç»“æ„åˆ†å¸ƒå¯¹æ¯” ({dim_reduction_method})")
                st.markdown("""
                - è“è‰²ç‚¹ï¼šåŸå§‹å®Œæ•´æ•°æ®é›†ï¼ˆæ•°æ®é›†Aï¼‰
                - æ©™è‰²ç‚¹ï¼šè¢«é€‰ä¸­çš„å­é›†ï¼ˆæ•°æ®é›†Bï¼‰
                """)
                fig = plot_distribution_comparison(coords_A, coords_B, distribution_metrics)
                st.pyplot(fig)
                plt.close(fig)
                timing_info.text(f"å¯è§†åŒ–ç”Ÿæˆè€—æ—¶: {time.time() - start_time:.2f}ç§’")

                # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if torch.cuda.is_available():
                    gpu_mem_alloc = torch.cuda.memory_allocated() / 1024**2
                    gpu_mem_cached = torch.cuda.memory_reserved() / 1024**2
                    debug_info.info(
                        f"GPUå†…å­˜ä½¿ç”¨æƒ…å†µ:\n"
                        f"- å·²åˆ†é…: {gpu_mem_alloc:.1f} MB\n"
                        f"- å·²ç¼“å­˜: {gpu_mem_cached:.1f} MB"
                    )

                # æ˜¾ç¤ºè¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯
                st.subheader("åˆ†å¸ƒç»Ÿè®¡æŒ‡æ ‡")
                metrics_df = pd.DataFrame({
                    "æŒ‡æ ‡": list(distribution_metrics.keys()),
                    "æ•°å€¼": list(distribution_metrics.values())
                })
                st.dataframe(metrics_df.style.format({
                    "æ•°å€¼": lambda x: f"{x:.3f}" if isinstance(x, float) else str(x)
                })) 